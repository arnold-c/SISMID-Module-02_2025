[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SISMID Module 1 Materials (2025)",
    "section": "",
    "text": "Welcome\nHello!\nWelcome to this website that accompanies the 2025 Summer Institute in Modeling for Infectious Diseases (SISMID) Module 1: Mathematical Models of Infectious Diseases. This website contains both the lecture notes, so you can refer back to them at a later date, and the exercises that we will be completing throughout the module.",
    "crumbs": [
      "**Welcome**"
    ]
  },
  {
    "objectID": "index.html#workshop-pre-requisites",
    "href": "index.html#workshop-pre-requisites",
    "title": "SISMID Module 1 Materials (2025)",
    "section": "Workshop Pre-Requisites",
    "text": "Workshop Pre-Requisites\nAs there is a reasonable amount of material to cover, and a relatively short amount of time to cover it in, there are some pre-requisite tasks to get set up ahead of the workshop.\n\n\n\n\n\n\nImportant\n\n\n\nEveryone should refer to the Exercise Requirements page, particularly the R Packages and Data Files sections to ensure you have the neccessary packages and data files installed for the exercises.\n\n\nFirstly, this workshop requires some prior understanding of the R programming languages, as well as a working installation of R. If you do not have any experience, please refer to the Installing R and Just Enough RStudio & Just Enought R pages of the Pre-Requisites section.\nIt is also highly recommended that you read the Organizing A Project page for how to structure your code as you go through the exercises: we provide some guidelines to make your code easier to understand and navigate, both for yourself, and for others.\nPlease set aside about an hour to go through these materials and get set up. You should be able to do this in about 15 minutes if you are already experienced with R, but as with all things computational, it’s worth including some buffer time in case you run into issues. If you are completely new to R, this could take an hour or so, but is certainly worth the time investment: it will be hard to follow some of the exercises if you do not understand what the code is doing.",
    "crumbs": [
      "**Welcome**"
    ]
  },
  {
    "objectID": "index.html#tips-about-the-website",
    "href": "index.html#tips-about-the-website",
    "title": "SISMID Module 1 Materials (2025)",
    "section": "Tips About the Website",
    "text": "Tips About the Website\nThere are a number of useful features throughout this website to help you.\nFirstly, in sections where there is R code showing, clicking on the text ▶ Code above the code block will hide the code if open (the default), or show the code if hidden.\nSecondly, in the top right corner of each code block, there is a button that looks like a clipboard. Clicking this button will copy the code to your clipboard, so you can paste it into your own R session.\nFinally, within the code blocks (and, in fact, in the regular text like this section), functions (e.g. list(), pivot_longer()) that come from a package (i.e., ones we didn’t write) show up in a different color. In most cases, you can hover your cursor over them (on the part next the the parentheses e.g., ode() from the line deSolve::ode()), and if they become underlined, you can click on them to go to the documentation for that function. This is like searching for the documentation from your R console using ?ode.",
    "crumbs": [
      "**Welcome**"
    ]
  },
  {
    "objectID": "index.html#keywords-code-and-other-formatting",
    "href": "index.html#keywords-code-and-other-formatting",
    "title": "SISMID Module 1 Materials (2025)",
    "section": "Keywords, Code, and Other Formatting",
    "text": "Keywords, Code, and Other Formatting\nThroughout the book, you’ll see some keywords, code, and other points that I’ll try to delineate with the following formatting:\n\n\n\n\n\n\nNote\n\n\n\nThis will be a note, and will be used to highlight important points, or to provide additional information.\n\n\n\n\n\n\n\n\nSET\n\n\n\nThis will be an instruction to set certain parameters or values in the R code, or for the interactive plots.\n\n\n\n\n\n\n\n\nInstruction\n\n\n\nThis will be a general instruction.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis will be used to highlight a useful tip.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis will provide a warning that you may get an unexpected result if you’re not careful.\n\n\nIt is worth noting that some of these callouts may be collapsible. You can tell a callout is collapsible if there is a little &gt; or ⋁ in the top right corner of the callout i.e.,\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis is an example of a collapsible callout that defaults to being collapsed.\n\n\n\n\n\nExercise:\n\nExercises that require you to complete missing sections of code, or answer a question, will be highlighted like this in R Session 2 and R Session 3\n\n\ncode will be used to highlight code.\n{package::function()} will be used to denote a specific package and function, e.g., {dplyr::mutate()} denotes the mutate() function from the {dplyr} package.\n\nOften the function() will be listed without the package, as there are library() calls at the top of each page indicating the packages used in that page, and the majority of functions will not cause conflicts, i.e., there are not packages with functions of the same name.\n\nBold will be used to highlight keywords and phrases, e.g., Git or GitHub.\n\nBold will also be highlighted in this way, e.g., commits or pushed being the result of the code git commit or git push\n\nBold-italics will be used to highlight file names, e.g., README.md or LICENSE.\nItalics will be used for emphasis in certain circumstances, e.g., signifying a question from an interactive terminal command.",
    "crumbs": [
      "**Welcome**"
    ]
  },
  {
    "objectID": "index.html#about-the-instructors",
    "href": "index.html#about-the-instructors",
    "title": "SISMID Module 1 Materials (2025)",
    "section": "About the Instructors",
    "text": "About the Instructors\n\n\n\nDr. Amy Winter\n\n\nDr. Winter is an Assistant Professor of Epidemiology & Biostatistics at the University of Georgia College of Public Health.\nDr. Winter’s research group adopts an interdisciplinary approach to tackle policy-relevant challenges in the transmission and control of infectious diseases within human populations. Her work focuses on understanding the spatiotemporal spread of infectious diseases, particularly through the analysis of serological data, developing predictive models to assess outbreak risks, and using scenario modeling to design effective strategies for disease control.\n\n\n\nDr. Matt Ferrari\n\n\nDr. Ferrari is the Director of the Center for Infectious Disease Dynamics at The Pennsylvania State University.\nDr Ferrari’s lab does research on both the application of quantitative modeling and analysis to inform public health policy and the basic ecology of parasites and infectious diseases at the Center for Infectious Disease Dynamics at The Pennsylvania State University.\n\n\n\n\n\n\n\n\n\nDr. Alpha Forna\n\n Dr. Forna is a Post-doctoral Researcher Associate in the Drake and Rohani Labs at the University of Georgia.\nDr Forna’s research focuses on the adaptation and application of machine learning methods to biological and epidemiological data, with an aim to positively inform efficient and cost-effective infectious disease prevention and intervention efforts (vaccines and antimicrobials, especially).",
    "crumbs": [
      "**Welcome**"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Workshop Schedule",
    "section": "",
    "text": "Before class\nGeneral workshop times:\nJuly 14, Monday: 9:00 a.m. - 5 p.m. EDT\nJuly 15, Tuesday: 9:00 a.m. - 5 p.m. EDT\nJuly 16, Wednesday: 9:00 a.m. - 12 p.m. EDT\nPlease see pre-requisites on the course website",
    "crumbs": [
      "Workshop Schedule"
    ]
  },
  {
    "objectID": "schedule.html#day-01-monday",
    "href": "schedule.html#day-01-monday",
    "title": "Workshop Schedule",
    "section": "Day 01 – Monday",
    "text": "Day 01 – Monday\n\n\n\n\n\n\n\nTime\nSection\n\n\n\n\n9:00 - 9:30am\nWelcome and Introductions\n\n\n9:30 - 10am\nLecture 1: Introduction to ID Models by Amy Winter\n\n\n10 - 10:30pm\ncoffee break\n\n\n10:30am - 11:30 am\nLecture 2: Basics of SIR Models by Matt Ferrari\n\n\n11:30 - 12pm\nR Session 1: Intro to modeling by Matt Ferrari\n\n\n12 - 1:30pm\nlunch on your own\n\n\n1:30pm - 2:30pm\nR Session 1: Intro to modeling (continued) by Matt Ferrari\n\n\n2:30pm - 3pm\nLecture 3: Vaccination & Interventions by Matt Ferrari\n\n\n3pm - 3:30pm\nCoffee break\n\n\n3:30pm - 4:00pm\nLecture 3: Vaccination & Interventions by Matt Ferrari\n\n\n4:00pm - 5pm\nInteractive Session\n\n\n5pm - 7pm\nSISMID-wide networking mixer",
    "crumbs": [
      "Workshop Schedule"
    ]
  },
  {
    "objectID": "schedule.html#day-02-tuesday",
    "href": "schedule.html#day-02-tuesday",
    "title": "Workshop Schedule",
    "section": "Day 02 – Tuesday",
    "text": "Day 02 – Tuesday\n\n\n\n\n\n\n\nTime\nSection\n\n\n\n\n9:00am - 10:00am\nLecture 4: Heterogeneity and Age Structure by Amy Winter\n\n\n10 - 10:30am\nCoffee Break\n\n\n10:30 - 12am\nR Session 2: Heterogeneity and Age structure by Matt Ferrari\n\n\n12 - 1:30pm\nlunch on your own / SISMID-wide optional professional development module\n\n\n1:30 - 2:30pm\nLecture 5: Parameter Estimation (estimating \\(R_0\\)) by Matt Ferrari\n\n\n2:30 - 3:00pm\nR Session 3: Parameter estimation by Matt Ferrari\n\n\n3pm - 3:30pm\nCoffee break\n\n\n3:30pm - 4:00pm\nR Session 3: Parameter estimation by Matt Ferrari\n\n\n4:00pm - 5pm\nGroup Discussion\n\n\n5pm - 7pm\nSISMID-wide social activity",
    "crumbs": [
      "Workshop Schedule"
    ]
  },
  {
    "objectID": "schedule.html#day-03-wednesday",
    "href": "schedule.html#day-03-wednesday",
    "title": "Workshop Schedule",
    "section": "Day 03 – Wednesday",
    "text": "Day 03 – Wednesday\n\n\n\n\n\n\n\nTime\nSection\n\n\n\n\n9:00am - 10am\nLecture 6: Stochastic Models (Lecture and R session) by Matt Ferrari\n\n\n10am - 10:30am\nCoffee break\n\n\n10:30am - 11am\nLecture 6: Stochastic Models (Lecture and R session) by Matt Ferrari\n\n\n11am - 12am\nGroup Discussion and additional topics\n\n\n\nClass Ends Noon on Wednesday",
    "crumbs": [
      "Workshop Schedule"
    ]
  },
  {
    "objectID": "schedule.html#group-assignments",
    "href": "schedule.html#group-assignments",
    "title": "Workshop Schedule",
    "section": "Group assignments",
    "text": "Group assignments\n\nEach individual should create a slide deck to address each of the following questions for each of the day’s R exercises.\nEach group should develop consensus slides to present when we come together as a class.\nGroups will be asked to share their intuition gained.\n\nQuestions:\n\nWhat were the top two take-away messages you gained from this R exercise?\nWhat was the most unexpected/non-intuitive thing you learned from this R exercise?",
    "crumbs": [
      "Workshop Schedule"
    ]
  },
  {
    "objectID": "exercise-requirements.html",
    "href": "exercise-requirements.html",
    "title": "Exercise Requirements",
    "section": "",
    "text": "R Packages\nIf you use renv to manage package dependencies in your projects, you can visit the GitHub repository for this project and download the renv.lock, .Rprofile, and renv/activate.R files, before running the command renv::restore(). Alternatively, if you already use GitHub, you could clone the project and just run renv::restore().\nIf you would prefer to just install the packages manually to avoid the complications associated with using renv, you can install the packages printed below.\nCodeinstall.packages(c(\n  \"tidyverse\",\n  \"deSolve\",\n  \"diagram\",\n  \"gt\",\n  \"ggtext\",\n  \"here\",\n  \"rio\"\n))",
    "crumbs": [
      "Pre-Requisites",
      "Exercise Requirements"
    ]
  },
  {
    "objectID": "exercise-requirements.html#data-files",
    "href": "exercise-requirements.html#data-files",
    "title": "Exercise Requirements",
    "section": "Data Files",
    "text": "Data Files\nTo complete the exercises, some data files are required. The exercises should load the datafiles via urls, but if you would prefer to download them to your own machine, your are welcome to do so. To download the files, go to the GitHub repository and download the files in the data/ folder of your repository. You should then uncomment the lines of code that run ...  &lt;- rio::import(here::here(\"data\", ...)), and comment out the lines that include the URLs.",
    "crumbs": [
      "Pre-Requisites",
      "Exercise Requirements"
    ]
  },
  {
    "objectID": "install-r.html",
    "href": "install-r.html",
    "title": "Install and Setup R & RStudio",
    "section": "",
    "text": "R\nTo start using R, you first need to install it, as it does not come bundled with your computer. The easiest way to do this is to visit CRAN and click on the link for your operating system (there are versions for Windows, Mac, and Linux).\nCRAN (Comprehensive R Archive Network) is a network of servers around the world that store identical, up-to-date, versions of code and documentation for R. This is where we will download R from, but also all the packages that we will use in this course. When you first try to install a package, you will be prompted to select your CRAN mirror. You can select any mirror, but it is best to choose one that is close to your location, as this will make the download faster.",
    "crumbs": [
      "Pre-Requisites",
      "Install and Setup R & RStudio"
    ]
  },
  {
    "objectID": "install-r.html#r",
    "href": "install-r.html#r",
    "title": "Install and Setup R & RStudio",
    "section": "",
    "text": "Note\n\n\n\nYou could also use a small application called {rig} to install R. {rig} is a small cross-platform application (i.e. works on Windows, Mac, and Linux) that downloads and installs R for you. While this may seem pointless to install an application to install R, it is actually quite useful as it makes it far easier to download and install multiple versions or R. As R is updated, bugs are fixed and new exciting packages do not support older versions of R, you will eventually need to update your installation. This is normally a massive pain due to the way R and the associated packages are installed on your computer. {rig} makes this process much easier (although you still shouldn’t upgrade R versions mid-project unless you are OK losing a couple of hours getting set up again).",
    "crumbs": [
      "Pre-Requisites",
      "Install and Setup R & RStudio"
    ]
  },
  {
    "objectID": "install-r.html#rstudio",
    "href": "install-r.html#rstudio",
    "title": "Install and Setup R & RStudio",
    "section": "RStudio",
    "text": "RStudio\n\nInstallation\nR is the programming language, but we need a way of interacting with R. We can do it directly by typing R into the terminal/command prompt, but this will give us a very pared down experience that is missing many of the essential features that make our development experience much more productive (as well as more enjoyable). For that, we want to install a Graphical User Interface (GUI), or more specifically, an Interactive Development Environment. The most suitable one for most R users is RStudio. RStudio is an easy-to-use IDE that allows us to write scripts (so we can save our analysis and rerun it easily, without needing to re-type it all), use the R console to check things quickly, provides a plotting window to easily manipulate and visualize the data, as well as an environment viewer to quickly understand what packages we have loaded and objects we have created.\nTo download RStudio, simply visit this link, which should provide you with a button to download the appropriate version for your operating system (there is also the full list of versions below the download button, in case it doesn’t recognize your OS correctly).\n\n\nSetup\nOnce you’ve installed RStudio, you can get going straight away - that’s the beauty of it. However, spending a few minutes getting accustomed and adjusting the layout will make your development a little smoother.\n\nTheme\nThe first thing that’s worth doing is adding a theme - the default white background can be a little harsh when you spend a long time staring at code. Open up the global preferences (ctrl/cmd + ,), go to “Appearance &gt; Editor theme”, and select a theme that works for you. The “Cobalt” theme is usually a nice default that work for many. Here, Callum is using the “Catppuccin” theme, that can be downloaded from here, and installed by clicking on “Add” at the “Appearance” screen.\n\n\n\nRStudio editor theme\n\n\n\n\nPane layout\nThe next RStudio thing that you may want to customize is the default layout. In the “Pane Layout” section of the global preferences, you can determine what you want to show in each quadrant of RStudio. The default layout will show you all the necessary parts, but most of the time you will be using the “Source” section, as this is where you write your scripts, and the “Environment” panel, which is where you can see what objects have been created, as well as exploring their properties e.g., columns names in a dataframe. For this reason, you may like to place the “Environment” panel under the “Source” panel, so they take up the majority of your screen, and the “Files/Plots/Help” and “Console/Terminal” panels are off to the side as you will interact with them less.\n\n\n\nRStudio pane layout settings\n\n\n\n\n\nRStudio pane layout\n\n\n\n\nRData\nThe final thing to do, that’s actually quite important, is to turn off the “Restore .RData into workspace at startup”. The reason being, if you do not, objects from previous sessions will be loaded into your new working environment, putting things where they shouldn’t be, making it very difficult to catch bugs as your code may reference something that doesn’t exist by that point in your script as it is created later on, but you wouldn’t catch that mistake as it was loaded into your environment on startup.\n\n\n\nRStudio .RData settings",
    "crumbs": [
      "Pre-Requisites",
      "Install and Setup R & RStudio"
    ]
  },
  {
    "objectID": "just-enough-rstudio.html",
    "href": "just-enough-rstudio.html",
    "title": "Just Enough RStudio",
    "section": "",
    "text": "RStudio projects\nThere are a number of features in RStudio that will make your life easier. This section aims to highlight a few of them.\nRStudio gives you the ability to turn a directory (that you are hopefully using to contain all your project, as mentioned in the next section), into an RStudio project. One of the benefits of using RStudio projects is that you can easily switch between different projects and RStudio will start a new R session, meaning that objects you created in your first project won’t stick around, causing issues by existing in a place where they shouldn’t. The other key benefit is that you can use the {here} package to create relative file paths, for easier code sharing and increased reproducibility. See this section for more details about the {here} package.\nTo create a new RStudio projet, simply open up RStudio, and click on the “Project: (None)” button in the top right corner.\nYou will then be given the option to create the project in a “New Directory”, and “Existing Directory”, or from “Version Control”. If you’ve already created a project just for this workshop, then select “Existing Directory”, otherwise, create a new one. If you know about Git, then please feel free to use the last option, but options 1 & 2 are most relevant for new users. Either way, we’d recommend reading through our project organization tips about what this directory should look like/include. If you select “New Directory”, you probably want to select “New Project” on the next option, unless you have something specific in mind (like a “Quarto Book”, which is used for this website!).\nFinally, choose the directory name and location, and you’re in business (you should click the button in the bottom-left corner to “Open in new session” to make sure you’re starting in a fresh environment). From here on out, when you open up a project, all the files you’ve created will be easily accessibly, both from the “Files” pane, as well as using relative paths.",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough RStudio"
    ]
  },
  {
    "objectID": "just-enough-rstudio.html#rstudio-projects",
    "href": "just-enough-rstudio.html#rstudio-projects",
    "title": "Just Enough RStudio",
    "section": "",
    "text": "Creating a new RStudio project - 1\n\n\n\n\n\n\nCreating a new RStudio project - 2\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo reiterate our comments from the project organization tips page, it’s useful to have all your project directories in a single location on your computer, and make sure you do not have any spaces in the file or folder names.",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough RStudio"
    ]
  },
  {
    "objectID": "just-enough-rstudio.html#rstudio-keyboard-shortcuts",
    "href": "just-enough-rstudio.html#rstudio-keyboard-shortcuts",
    "title": "Just Enough RStudio",
    "section": "RStudio keyboard shortcuts",
    "text": "RStudio keyboard shortcuts\nThere are many shortcuts available to RStudio users, but here are the key ones:\n\nRStudio keyboard shortcuts\n\n\n\n\n\n\nShortcut\nCommand\n\n\n\n\ncmd/ctrl + enter\nSend the section of code to the console to be run\n\n\ncmd/ctrl + opt/alt + r\nRun all code\n\n\ncmd/ctrl + opt/alt + b\nRun all code from beginning to selected line\n\n\ncmd/ctrl + shift + enter\nRun current chunk (when within a Rmd or Quarto notebook)\n\n\ncmd/ctrl + shift + p\nOpen the command palette (a place where you can search for different commands)\n\n\ncmd/ctrl + shift + a\nReformat selected code (useful to help keep things readable)\n\n\ncmd/ctrl + shift + c\nComment the selected lines\n\n\ncmd/ctrl + shift + m\nInsert the pipe (%&gt;%) operator (or |&gt; if you have set up RStudio to use the base pipe by default)\n\n\nopt/alt + -\nInsert the assignment operator (&lt;-)\n\n\n\nThe common shortcuts for saving and opening files/selecting all etc. also exist in RStudio with the standard keybindings.\nTo see the full list of keyboard shortcuts, you can go to “Tools &gt; Keyboard Shortcuts Help”.\n\n\n\nRStudio keyboard shortcuts",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough RStudio"
    ]
  },
  {
    "objectID": "just-enough-r.html",
    "href": "just-enough-r.html",
    "title": "Just Enough R",
    "section": "",
    "text": "Objects & types introduction\nThe purpose of this section is to get you up-to-speed with R. If you’re completely unfamiliar with R and RStudio, this should provide you with enough to get started and understand what’s going on in the code (and you can always refer back to this page if you understandably get a little lost), and if you have some experience, then it should provide a sufficient description of the packages and functions that we use in this workshop.\nNow you have R set installed and you can access it and are familiar with RStudio, it’s time to learn some of the core features of the language.\nAn object is anything you can create in R using code, whether that is a table you import from a csv file (that will get converted to a dataframe), or a vector you create within a script. Each object you create has a type. We’ve already mentioned two (dataframes and vectors), but there are plenty more. But before we get into object types, let’s take a step back and look at types in general, thinking about individual elements and the fundamentals.",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#element-types",
    "href": "just-enough-r.html#element-types",
    "title": "Just Enough R",
    "section": "Element types",
    "text": "Element types\nGenerally in programming, we have two broad types of numbers: floating point and integer numbers, i.e., numbers with decimals, and whole numbers, respectively. In R, we have these number types, but a floating point number is called a double. The floating point number is the default type R assigns to number: look at the types assigned when we leave off a decimal place vs. specify type integer by ending a number with an L.\n\ntypeof(1)\n\n[1] \"double\"\n\ntypeof(1L)\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nTechnically type double is a subset of type numeric, so you will often see people convert numbers to floating points using as.numeric(), rather than as.double(), but the different is semantics. You can confirm this using the command typeof(as.numeric(10)) == typeof(as.double(10))semantics. You can confirm this using the commandtypeof(as.numeric(10)) == typeof(as.double(10))`.\n\n\n\nInteger types are not commonly used in R, but there are occasions when you will want to use them e.g., when you need whole numbers of people in a simulation you may want to use integers to enforce this. Integers are also slightly more precise (unless very big or small), so when exactness in whole number is required, you may want to use integers.\n\n\n\n\n\n\nNote\n\n\n\n\n\nR has some idiosyncrasies when it comes to numbers. For the most part, doubles are produced, but occasionally an integer will be produced when you are expecting a double.\nFor example:\n\ntypeof(1)\n\n[1] \"double\"\n\ntypeof(1:10)\n\n[1] \"integer\"\n\ntypeof(seq(1, 10))\n\n[1] \"integer\"\n\ntypeof(seq(1, 10, by = 1))\n\n[1] \"double\"\n\n\n\n\n\nOutside of numbers, we have characters (strings) and boolean types.\nA boolean (also known as a logical in R) is a TRUE/FALSE statement. In R, as in many programming languages, TRUE is equal to a value of 1, and FALSE equals 0. There are times when this comes in handy e.g. you need to calculate the number of people that responded to a question, and their responses is coded as TRUE/FALSE, you can just sum the vector of responses (more on vectors shortly).\n\nTRUE == 1\n\n[1] TRUE\n\nFALSE == 0\n\n[1] TRUE\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you figure out what value will be returned for the command (TRUE == 0) == FALSE?\n\n\nA character is anything in quotation marks. This would typically by letter, but is occasionally a number, or other symbol. Other languages make a distinction between characters and strings, but not R.\n\ntypeof(\"a\")\n\n[1] \"character\"\n\ntypeof(\"1\")\n\n[1] \"character\"\n\n\nIt is important to note that characters are not parsed i.e., they are not interpreted by R as anything other than a character. This means that despite \"1\" looking like the number 1, it behaves like a character in R, not a double, so we can’t do addition etc. with it.\n\n\"1\" + 1\n\nError in \"1\" + 1: non-numeric argument to binary operator",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#object-types",
    "href": "just-enough-r.html#object-types",
    "title": "Just Enough R",
    "section": "Object types",
    "text": "Object types\nVectors\nAs mentioned, anything you can create in R is an object. For example, we can create an character object with the assignment operator (&lt;-).\n\nmy_char_obj &lt;- \"a\"\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn other languages, = is used for assignment. In R, this is generally avoided to distinguish between creating objects (assignment), and specifying argument values (see the section on functions). However, despite what some purists may say, it really doesn’t matter which one you use, from a practical standpoint.\n\n\n\nYou will note that when we created our object, it did not return a value (unlike the previous examples, a value was not printed). To retrieve the value of the object (in this case, just print it), we just type out the object name.\n\nmy_char_obj\n\n[1] \"a\"\n\n\nIn this case, we just create an object with only one element. We can check this using the length() function.\n\nlength(my_char_obj)\n\n[1] 1\n\n\nWe could also create an atomic vector (commonly just called a vector, which we’ll use from here-on in). In fact, my_char_obj is actually an vector, i.e., it is a vector of length 1, as we’ve just seen. Generally, a vector is an object that contains multiple elements that each have the same type.\n\nmy_char_vec &lt;- c(\"a\", \"b\", \"c\")\n\nAs we’ll see in the example below, we can give each element in a vector a name, and to highlight that vectors must contain elements of the same type, watch what happens here.\n\nmy_named_char_vec &lt;- c(a = \"a\", b = \"b\", c = \"c\", d = 1)\nnames(my_named_char_vec)\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\nmy_named_char_vec\n\n  a   b   c   d \n\"a\" \"b\" \"c\" \"1\" \n\n\nBecause R saw the majority of the first elements in the vector were of type character it coerced the number to a character. This is super important to be aware of, as it can cause errors, particularly when coercion goes in the other direction i.e. trying to create a numeric vector.\nFactors\nAll the vector types we’ve mentioned so far map nicely to their corresponding element types. But there is an extension of the character vector used frequently: the factor (and, correspondingly, the ordered vector).\nA factor is a vector where there are distinct groups that exist within a vector i.e., they are nominal categorical data. For example, we often include gender as a covariate in epidemiological analysis. There is no intrinsic order, but we would want to account for the groups in the analysis.\nAn ordered vector is when there is an intrinsic order to the grouping i.e., we have ordinal categorical data. If, for example, we were interested in how the frequency of cigarette smoking is related to an outcome, and we wanted to use binned groups, rather than treating it as a continuous value, we would want to create an ordered vector as the ordering of the different groupings is important.\nLet’s use the mtcars dataset (that comes installed with R), and turn the number of cylinders (cyl) into an ordered vector, as there are discrete numbers of cylinders a car engine can have, and the ordering matters. Don’t worry about what $ is doing; we’ll come to that later\n\nmy_mtcars &lt;- mtcars\nmy_mtcars$cyl\n\n [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n\nmy_mtcars$cyl &lt;- ordered(my_mtcars$cyl)\nmy_mtcars$cyl\n\n [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\nLevels: 4 &lt; 6 &lt; 8\n\n\nIf we wanted to directly specify the ordering of the groups, we can do this using the levels argument i.e.\n\nmy_mtcars$cyl &lt;- ordered(my_mtcars$cyl, levels = c(8, 6, 4))\nmy_mtcars$cyl\n\n [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\nLevels: 8 &lt; 6 &lt; 4\n\n\nTo create a factor, just replace the ordered() call with factor()\nLists\nThere is another type of vector: the list. Most people do not refer to lists as type of vectors, so we will only refer to them as lists, and atomic vectors will just be referred to as vectors.\nUnlike vectors there are no requirements about the form of lists i.e., each element of the list can be completely different. One element could store a vector of numbers, another a model object, another a dataframe, and another a list (i.e. a nested list).\n\nmy_list &lt;- list(\n    c(1, 2, 3, 4, 5),\n    glm(mpg ~ ordered(cyl) + disp + hp, data = mtcars),\n    data.frame(column_1 = 1:5, column_2 = 6:10)\n)\nmy_named_list &lt;- list(\n    my_vec = c(1, 2, 3, 4, 5),\n    my_model = glm(mpg ~ ordered(cyl) + disp + hp, data = my_mtcars),\n    my_dataframe = data.frame(column_1 = 1:5, column_2 = 6:10)\n)\nmy_list\n\n[[1]]\n[1] 1 2 3 4 5\n\n[[2]]\n\nCall:  glm(formula = mpg ~ ordered(cyl) + disp + hp, data = mtcars)\n\nCoefficients:\n   (Intercept)  ordered(cyl).L  ordered(cyl).Q            disp              hp  \n      28.98802        -1.71963         2.31169        -0.02604        -0.02114  \n\nDegrees of Freedom: 31 Total (i.e. Null);  27 Residual\nNull Deviance:      1126 \nResidual Deviance: 225.1    AIC: 165.2\n\n[[3]]\n  column_1 column_2\n1        1        6\n2        2        7\n3        3        8\n4        4        9\n5        5       10\n\nmy_named_list\n\n$my_vec\n[1] 1 2 3 4 5\n\n$my_model\n\nCall:  glm(formula = mpg ~ ordered(cyl) + disp + hp, data = my_mtcars)\n\nCoefficients:\n   (Intercept)  ordered(cyl).L  ordered(cyl).Q            disp              hp  \n      28.98802         1.71963         2.31169        -0.02604        -0.02114  \n\nDegrees of Freedom: 31 Total (i.e. Null);  27 Residual\nNull Deviance:      1126 \nResidual Deviance: 225.1    AIC: 165.2\n\n$my_dataframe\n  column_1 column_2\n1        1        6\n2        2        7\n3        3        8\n4        4        9\n5        5       10\n\n\nSimilar to vectors, lists can be named, or unnamed, and also that we they display in slightly different ways: when unnamed, we get the notation [[1]] ... [[3]] to denote the different list elements, and with the named list we get $my_vec ... $my_dataframe. It is often useful to name them, though, as it gives you some useful options when it comes to indexing and extracting values later.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf you’re wondering why we are creating our list elements with the = operator, that’s because we can think of this as an argument in the list() function, where the argument name is the name we want the element to have, and the argument value is the element itself.\n\n\n\nDataframes\nDataframes are the last key object type to learn about. A dataframe is technically a special type of list. Effectively, it is a 2-D table where every column has to have elements of the same type (i.e., is a vector), but the columns can be different types to each other. The other important restriction is that all columns must be the same length, i.e. we have a rectangular dataframe.\nAs we’ve seen before, we can create a dataframe using this code, where 1:5 is shorthand for a vector that contains the sequence of numbers from 1 to 5, inclusive (i.e., c(1, 2, 3, 4, 5)). We could also write this sequence as seq(1, 5, by = 1), allowing us more control over the steps in the sequence.\n\nmy_dataframe &lt;- data.frame(\n    column_int = 1:5,\n    column_dbl = seq(6, 10, 1),\n    column_3 = letters[1:5]\n)\n\nLike with every other object type, we can just type in the dataframe’s name to return it’s value, but this tim, let’ explore the structure of the dataframe using the str() function. This function can be used on any of the objects we’ve seen so far, and is particularly helpful when exploring lists. One nice feature of dataframes is that it will explicitly print the columns types.\n\nstr(my_dataframe)\n\n'data.frame':   5 obs. of  3 variables:\n $ column_int: int  1 2 3 4 5\n $ column_dbl: num  6 7 8 9 10\n $ column_3  : chr  \"a\" \"b\" \"c\" \"d\" ...\n\n\nMatrices\nMatrices are crucial to many scientific fields, including epidemiology, as they are the basis of linear algebra. This course will use matrix multiplication extensively (notably R Session 2), so it is worth knowing how to create matrices.\nMuch like vectors, all elements in a matrix should be the same type (or they will be coerced if possible, resulting in NA if not). It is unusual to have a non-numeric matrix e.g., a character matrix, but it is possible. When we create our matrix, notice that it fills column-first, much like how we think of matrices in math (i.e., i then j).\n\nmy_matrix &lt;- matrix(1:8, nrow = 2)\nmy_matrix\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#indexing-objects",
    "href": "just-enough-r.html#indexing-objects",
    "title": "Just Enough R",
    "section": "Indexing objects",
    "text": "Indexing objects\nIndexing operators\nWe’ve got our objects, but now we want to do stuff with them. Without getting into too much detail about Object-Oriented Programming (e.g., the S3 class system in R), there are three mains ways of indexing in R:\n\nThe single bracket []\n\nThe double bracket [[]]\n\nThe dollar sign $\n\n\nWhich method we use depends on the type of object we have. Handily, [] will work for pretty much everything, and we typically only use use [[]] for lists.\nIndexing vectors\nWith both [] and [[]], we can use the indices i.e., the numbered position of the specific values/elements we want to extract, but if we have named objects, we can pass the names to the [] in a vector.\n\n# Extract elements 1 through 3 inclusively\nmy_char_vec[1:3]\n\n[1] \"a\" \"b\" \"c\"\n\n# Extract the same elements but using their names in a vector\nmy_named_char_vec[c(\"a\", \"b\", \"c\")]\n\n  a   b   c \n\"a\" \"b\" \"c\" \n\n\nNotice that when we index the named vector we get both the name and the value returned. Many times this is OK, but if we only wanted the value, then you’d index with [[]], but it is important to note that you can only pass one value to the brackets.\n\nmy_named_char_vec[[c(\"a\", \"b\")]]\n\nError in my_named_char_vec[[c(\"a\", \"b\")]]: attempt to select more than one element in vectorIndex\n\nmy_named_char_vec[[\"a\"]]\n\n[1] \"a\"\n\n\nIf you’re wondering why go through the hassle, it’s because values can change position in the list when we update inputs, such as csv datafiles, or needing to restructure code to make something else work. If we only index with the numeric indices, we run the risk of a silent error being returned i.e., a value is provided to us, but we don’t know that it’s referring to the wrong thing. Indexing with names mean that the element’s position in the vector doesn’t matter, and if it’s accidentally been removed when we updated code, and error will be explicitly thrown as it won’t be able to find the index.\nLists and Dataframes\nWhen it comes to indexing lists and dataframes (remember, dataframes are just special lists, so the same methods are available to us), it is more common to use [[]] and $, though there are obviously occasions when [] is useful. Let’s look at my_named_list first.\n\nmy_named_list[1]\n\n$my_vec\n[1] 1 2 3 4 5\n\nmy_named_list[\"my_vec\"]\n\n$my_vec\n[1] 1 2 3 4 5\n\nmy_named_list[[1]]\n\n[1] 1 2 3 4 5\n\nmy_named_list[[\"my_vec\"]]\n\n[1] 1 2 3 4 5\n\nmy_named_list$my_vec\n\n[1] 1 2 3 4 5\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the examples above, notice how both [] methods returned the name of the element as well as the values (as it did before with the named vector). This is important as it means we need to extract the values from what is returned before we can do any further indexing i.e., to get the value 3 from the list element my_vec.\n\n\nWe can do the same with the unnamed list, except the last two methods are not available as we do not have a name to use.\n\nmy_list[1]\n\n[[1]]\n[1] 1 2 3 4 5\n\nmy_list[[1]]\n\n[1] 1 2 3 4 5\n\n\nBecause a dataframe is a type of list where the column headers are the element names, we can use [[]] and $ as with the named list.\n\nmy_dataframe[1]\n\n  column_int\n1          1\n2          2\n3          3\n4          4\n5          5\n\nmy_dataframe[[1]]\n\n[1] 1 2 3 4 5\n\nmy_dataframe[\"column_int\"]\n\n  column_int\n1          1\n2          2\n3          3\n4          4\n5          5\n\nmy_dataframe$column_int\n\n[1] 1 2 3 4 5\n\n\nIf we wanted to extract a particular value from a column, we can use the following methods.\n\n# indexes i then j, just like in math\nmy_dataframe[2, 1]\n\n[1] 2\n\n# Extract the second element from the first column\nmy_dataframe[[1]][2]\n\n[1] 2\n\n# Extract the second element from column_int, using the i, j procedure as before\nmy_dataframe[2, \"column_int\"]\n\n[1] 2\n\n# Extract the second element from column_int\nmy_dataframe$column_int[2]\n\n[1] 2",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#packages",
    "href": "just-enough-r.html#packages",
    "title": "Just Enough R",
    "section": "Packages",
    "text": "Packages\nUp until now, we’ve been getting to grips with the core concepts of objects, and indexing them. But when you’re writing code, you’ll want to do things that are relatively complicated to implement, such as solve a set of differential equations. Fortunately, for many areas of computing (and, indeed, epidemiology and statistics), many others have also struggled with the same issues and some have gone one to document their solutions in a way others can re-use them. This is the basis for packages. Someone has packaged up a set of functions for others to re-use.\nWe’ve mentioned the word function a number of time so far, and we haven’t defined it, but that’s coming soon. For the moment, let’s just look at how we can find, install, and load packages.\nFinding packages\nAs mentioned previously CRAN is a place where many pieces of R code is documents and stored for others to download and use. Not only are the R programming language executables stored in CRAN, but so are user-defined functions that have been turned into packages.\nTo find packages, you can go to the CRAN website and search by name, but there are far too many for that to be worthwhile - just Google what you want to do and add “r” to the end of your search query, and you’ll likely find what you’re looking for. Once you’ve found a package you want to download, next you need to install it.\nInstalling packages\nBarring any super-niche packages, you should be able to use the following command(s):\n\ninstall.packages(\"package to download\")\n# Download multiple by passing a vector of package names\ninstall.packages(c(\"package 1\", \"package 2\"))\n\nIf for some reason you get an error message saying the package isn’t available on CRAN, first, check for typos, and if you still get an error, you may need to download it directly from GitHub. Read here for more information about using the pak package to download packages from other sources.\nLoading packages\nNow you have your packages installed, you just need to load them to get any of their functionality. The easiest way is to place this code at the top of your script.\n\n# Quotations are not required, but can be used\nlibrary(package to download)\n\n\nMost of the time, this is fine, but occasionally you will run in to an issue where a function doesn’t work as expected. Sometimes this is because of what’s called a namespace conflict i.e., you have two functions with the same name loaded, and potentially you’re using the wrong verion.\nFor example, in base R (i.e, these functions come pre-installed when you set up R), there is a filter() function from the {stats} package (as mentioned, we’ll denote this as stats::filter()). Throughout this workshop, you will see library(tidyverse) at the top of the pages to indicate the tidyverse set of packages are being loaded (this is actually a package that installs a bunch of related and useful packages for us). In dplyr (one of the packages loaded by tidyverse) there is also a function called filter(). Because dplyr was loaded after {stats} was loaded (because {stats} is automatically loaded when R is started), the dplyr::filter() function will take precedence. If we wanted to specifically use the {stats} version, we could write this:\n\n# Set the seed for the document so we get the same random numbers sampled\n# each time we run the script (assuming it's run in its entirety from start\n# to finish)\nset.seed(1234)\n\n# Create a cosine wave with random noise\nraw_timeseries &lt;- cos(pi * seq(-2, 2, length.out = 1000)) + rnorm(1000, sd = 0.5)\n\n# Calculate 20 day moving average using stats::filter()\nsmooth_timeseries &lt;- stats::filter(raw_timeseries, filter = rep(1/20, 20), sides = 1)\n\n# Plot raw data\nplot(raw_timeseries, col = \"grey80\")\n\n# Overlay smoothed data\nlines(smooth_timeseries, col = \"red\", lwd = 2)",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#functions",
    "href": "just-enough-r.html#functions",
    "title": "Just Enough R",
    "section": "Functions",
    "text": "Functions\nAs we’ve alluded to, functions are core to gaining functionality in R. We can always hand-write the code to complete a task, but if we have to repeat a task more than once, it can be tiresome to repeat the same code, particularly if it is a particularly complex task that requires many lines of code. This is where functions come in: they provide us with a mechanism to wrap up code into something that can be re-used. Not only does this reduce the amount of code we need to write, but by minimize code duplication, debugging becomes a lot easier as we only need to remember to make changes and correct one section of our codebase. Say, for example, you want to take a vector of numbers and calculate the cumulative sum e.g.;\n\nmy_dbl_vec &lt;- 1:10\n\ncumulative_sum &lt;- 0\n\nfor(i in seq_along(my_dbl_vec)) {\n    cumulative_sum &lt;- cumulative_sum + i\n}\n\ncumulative_sum\n\n[1] 55\n\n\nThis is OK if we only do this calculation once, but it’s easy to imagine us wanting to repeat this calculation; for example, we might use calculate the cumulative sum of daily cases to get a weekly incidence over every week of a year. In this situation, we would want to create a function.\n\nmy_cumsum &lt;- function(vector) {\n    cumulative_sum &lt;- 0\n\n    for(i in seq_along(my_dbl_vec)) {\n        cumulative_sum &lt;- cumulative_sum + i\n    }\n\n    cumulative_sum\n}\n\nmy_cumsum(my_dbl_vec)\n\n[1] 55\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis is obviously a contrived example because, as with many basic operations in R, there is already a function written to perform this calculation that does it in a much more performant and safer manner: cumsum()\n\n\n\nFor many of the manipulations we will want to perform, a function has already been written by someone else and put into a package that we can download, as we’ve already seen.\nAnonymous functions\nThere is a special class of functions called anonymous functions that are worth being aware of, as we will use them quite extensively throughout this workshop. As the name might suggest, anonymous functions are functions that are not named, and therefore, not saved for re-use. You may, understandably, be wondering why we would want to use them, given we just make the case for functions replacing repeatable blocks of code. In some instances, we want to be able to perform multiple computations that require creating intermediate objects, but because we only need to use them once, we don’t save them save to our environment, potentially causing issues with conflicts (e.g., accidentally using an object we didn’t mean to, or overwriting existing ones by re-using the same object name). This gets into the broader concept of local vs global scopes, but that is too far beyond the scope of this workshop: see Hands-On Programming with R and Advanced R for more information. Let’s look at an example to see when we might want to use an anonymous function.\nThroughout this workshop, we will make use of the map_*() series of functions from the purrr package. We’ll go into more detail about purr::map() shortly, but for now, imagine we have a vector of numbers, and we want to add 5 to each value before and multiplying by 10. The map_dbl() function takes a vector and a function, and outputs a double vector. We could write a function to perform this multiplication, but if we’re only going to do this operation once, it seems unnecessary.\n\npurrr::map_dbl(\n    .x = my_dbl_vec,\n    .f = function(.x) {\n        add_five_val &lt;- .x + 5\n\n        add_five_val * 10\n    }\n)\n\n [1]  60  70  80  90 100 110 120 130 140 150\n\n# only exists within the function\nadd_five_val\n\nError: object 'add_five_val' not found\n\n\nHere, we’ve specified the anonymous function to take the input .x and multiple each value by 10, and we did it without saving the function. This would be equivalent to writing this:\n\nadd_five_multiply_ten &lt;- function(x) {\n    add_five_val &lt;- x + 5\n    add_five_val * 10\n}\n\npurrr::map_dbl(\n    .x = my_dbl_vec,\n    .f = ~add_five_multiply_ten(.x)\n)\n\n [1]  60  70  80  90 100 110 120 130 140 150\n\n# only exists within the function\nadd_five_val\n\nError: object 'add_five_val' not found\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNotice the ~ used: this specifies that we want to pass arguments into our named function. Without it, we will get an error about .x not being found.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn this example, because we are doing standard arithmetic, R will vectorize our function so that it can automatically be applied to each element of the object, so this example was merely to illustrate the point.\n\nadd_five_multiply_ten(my_dbl_vec)\n\n [1]  60  70  80  90 100 110 120 130 140 150",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#tidy-data",
    "href": "just-enough-r.html#tidy-data",
    "title": "Just Enough R",
    "section": "Tidy data",
    "text": "Tidy data\nBefore we look at the common packages and functions we use throughout this workshop, let’s take a second to talk about how our data is structured. For much of what we do, it is convenient to work with dataframes, and many functions we will use are designed to work with long dataframes. What this means is that each column represents a variable, and each row is a unique observation.\nLet’s first look at a wide dataframe to see how data may be represented. Here, we have one column representing a number for each of the states in the US, and then we have two columns representing some random incidence: one for July and one for August.\n\nwide_df &lt;- data.frame(\n    state_id = 1:52,\n    july_inc = rbinom(52, 1000, 0.4),\n    aug_inc = rbinom(52, 1000, 0.6)\n)\n\nwide_df\n\n   state_id july_inc aug_inc\n1         1      399     613\n2         2      409     578\n3         3      381     604\n4         4      381     607\n5         5      387     603\n6         6      372     614\n7         7      403     597\n8         8      407     605\n9         9      388     604\n10       10      422     595\n11       11      343     597\n12       12      377     590\n13       13      406     618\n14       14      421     598\n15       15      407     603\n16       16      400     614\n17       17      387     585\n18       18      407     598\n19       19      387     604\n20       20      405     618\n21       21      378     599\n22       22      390     601\n23       23      399     587\n24       24      398     609\n25       25      398     591\n26       26      401     607\n27       27      387     591\n28       28      410     603\n29       29      396     585\n30       30      375     601\n31       31      398     596\n32       32      406     579\n33       33      405     633\n34       34      422     607\n35       35      395     578\n36       36      391     597\n37       37      384     568\n38       38      426     590\n39       39      390     587\n40       40      399     586\n41       41      373     589\n42       42      441     602\n43       43      365     600\n44       44      397     591\n45       45      417     615\n46       46      374     606\n47       47      398     617\n48       48      390     594\n49       49      404     579\n50       50      403     603\n51       51      414     609\n52       52      417     606\n\n\nInstead, we reshape this into a long dataframe so that there is a column for the state ID, a column for the month, and a column for the incidence (that is associated with both the state and the month). Using the tidyr package, we could reshape this wide dataframe to be a long dataframe (see this section for more information about the pivot_*() functions)\n\nlong_df &lt;- tidyr::pivot_longer(\n    wide_df,\n    cols = c(july_inc, aug_inc),\n    names_to = \"month\",\n    values_to = \"incidence\",\n    # Extract only the month using regex\n    names_pattern = \"(.*)_inc\"\n)\n\nlong_df\n\n# A tibble: 104 × 3\n   state_id month incidence\n      &lt;int&gt; &lt;chr&gt;     &lt;int&gt;\n 1        1 july        399\n 2        1 aug         613\n 3        2 july        409\n 4        2 aug         578\n 5        3 july        381\n 6        3 aug         604\n 7        4 july        381\n 8        4 aug         607\n 9        5 july        387\n10        5 aug         603\n# ℹ 94 more rows\n\n\nYou will notice that our new dataframe contains three columns still, but is longer than previously; two time as long, in fact.\n\n\n\n\n\n\nNote\n\n\n\n\n\nParticularly keen-eyed reader may also notice that long_df is also has class tibble, not a data.frame. A tibble effectively is a data.frame, but is an object commonly used and output by tidyverse functions, as it has a few extra safety features over the base data.frame.",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#core-code-used",
    "href": "just-enough-r.html#core-code-used",
    "title": "Just Enough R",
    "section": "Core code used",
    "text": "Core code used\nWe’re finally ready to talk about the functions that are used throughout this workshop. The first package to mention is the tidyverse package, which actually a collection of packages: the core packages can be found here. The reason why are using the tidyverse packages throughout this workshop is that they are relatively easily to learn, compared to base R and data.table (not that they are mutually exclusive), and what most people are familiar with. They also are well designed and powerful, so you should be able to do most things you need using their packages.\nYou can find a list of cheatsheets for all of these packages (and more) here.\nLet’s load the tidyverse packages and then go through the key functions used. Unless stated explicitly, these packages will be available to you after loading the tidyverse with the following command.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\ntibble()\nThe tibble is a modern reincarnation of the dataframes that is slightly safer i.e., is more restricted in what you can do with it, and will throw errrors more frequently, but very rarely for anything other than a bug. We will use the terms interchangeably, as most people will just talk about dataframes, as for the most part, they can be treated identically. Use the same syntax as the data.frame() function to create the tibble.\ndplyr::filter()\nIf we wanted to take a subset of rows of a dataframe, we would use the dplyr::filter() function. Here, we’re listing the package it’s coming from, as there are some other packages that also export their own version of the filter() function. However, for all the code in this workshop, there aren’t any concerns about namespace conflicts, so we won’t use it from here on in.\nThe filter() function is relatively simple to work with: you specify the dataframe variable you want to subset by, the filtering criteria, and that’s it. If we include multiple arguments, they get treated as AND statements (&), so all conditions need to be met.\n\nfilter(\n    long_df,\n    month == \"july\",\n    incidence &gt; 410\n    # equivalent to: month == \"july\" & incidence &gt; 410\n)\n\n# A tibble: 8 × 3\n  state_id month incidence\n     &lt;int&gt; &lt;chr&gt;     &lt;int&gt;\n1       10 july        422\n2       14 july        421\n3       34 july        422\n4       38 july        426\n5       42 july        441\n6       45 july        417\n7       51 july        414\n8       52 july        417\n\n\nWe can filter using OR statements (|), so if either condition returns TRUE, then it will be included in the subset.\n\nfilter(\n    long_df,\n    month == \"july\" | incidence &gt; 600\n)\n\n# A tibble: 78 × 3\n   state_id month incidence\n      &lt;int&gt; &lt;chr&gt;     &lt;int&gt;\n 1        1 july        399\n 2        1 aug         613\n 3        2 july        409\n 4        3 july        381\n 5        3 aug         604\n 6        4 july        381\n 7        4 aug         607\n 8        5 july        387\n 9        5 aug         603\n10        6 july        372\n# ℹ 68 more rows\n\n\nselect()\nIf, instead, we wanted to subset of columns of a dataframe, we would use the dplyr::select() function.\nLet’s say, from our wide incidence data, we only want the state’s ID and their August incidence. We can directly select the columns this way.\n\nselect(\n    wide_df,\n    state_id, aug_inc\n)\n\n   state_id aug_inc\n1         1     613\n2         2     578\n3         3     604\n4         4     607\n5         5     603\n6         6     614\n7         7     597\n8         8     605\n9         9     604\n10       10     595\n11       11     597\n12       12     590\n13       13     618\n14       14     598\n15       15     603\n16       16     614\n17       17     585\n18       18     598\n19       19     604\n20       20     618\n21       21     599\n22       22     601\n23       23     587\n24       24     609\n25       25     591\n26       26     607\n27       27     591\n28       28     603\n29       29     585\n30       30     601\n31       31     596\n32       32     579\n33       33     633\n34       34     607\n35       35     578\n36       36     597\n37       37     568\n38       38     590\n39       39     587\n40       40     586\n41       41     589\n42       42     602\n43       43     600\n44       44     591\n45       45     615\n46       46     606\n47       47     617\n48       48     594\n49       49     579\n50       50     603\n51       51     609\n52       52     606\n\n\nBut in this case, it would be more efficient (for us) to tell R the columns we don’t want. We can do that using the - sign.\n\nselect(\n    wide_df,\n    -july_inc\n)\n\n   state_id aug_inc\n1         1     613\n2         2     578\n3         3     604\n4         4     607\n5         5     603\n6         6     614\n7         7     597\n8         8     605\n9         9     604\n10       10     595\n11       11     597\n12       12     590\n13       13     618\n14       14     598\n15       15     603\n16       16     614\n17       17     585\n18       18     598\n19       19     604\n20       20     618\n21       21     599\n22       22     601\n23       23     587\n24       24     609\n25       25     591\n26       26     607\n27       27     591\n28       28     603\n29       29     585\n30       30     601\n31       31     596\n32       32     579\n33       33     633\n34       34     607\n35       35     578\n36       36     597\n37       37     568\n38       38     590\n39       39     587\n40       40     586\n41       41     589\n42       42     602\n43       43     600\n44       44     591\n45       45     615\n46       46     606\n47       47     617\n48       48     594\n49       49     579\n50       50     603\n51       51     609\n52       52     606\n\n\nIf there were multiple columns we didn’t want, we would pass them in a vector.\n\nselect(\n    wide_df,\n    -c(july_inc, aug_inc)\n)\n\n   state_id\n1         1\n2         2\n3         3\n4         4\n5         5\n6         6\n7         7\n8         8\n9         9\n10       10\n11       11\n12       12\n13       13\n14       14\n15       15\n16       16\n17       17\n18       18\n19       19\n20       20\n21       21\n22       22\n23       23\n24       24\n25       25\n26       26\n27       27\n28       28\n29       29\n30       30\n31       31\n32       32\n33       33\n34       34\n35       35\n36       36\n37       37\n38       38\n39       39\n40       40\n41       41\n42       42\n43       43\n44       44\n45       45\n46       46\n47       47\n48       48\n49       49\n50       50\n51       51\n52       52\n\n\nWhen it comes to selecting columns, the tidyselect package has a few very handy functions for us. To understand when they are most useful, let’s first look at the mutate() function, and then we’ll highlight how to use the different column selection functions available to use through tidyselect.\nmutate()\nIf we have a dataframe and want to add or edit a column, we use the mutate() function. Usually the mutate() function is used to add a column that is related to the existing data, but it is not necessary. Below are examples of both.\n\n# add September incidence that is based on August incidence\nmutate(\n    wide_df,\n    sep_inc = round(aug_inc * 1.2 + rnorm(52, 0, 10), digits = 0)\n)\n\n   state_id july_inc aug_inc sep_inc\n1         1      399     613     735\n2         2      409     578     692\n3         3      381     604     725\n4         4      381     607     733\n5         5      387     603     733\n6         6      372     614     740\n7         7      403     597     726\n8         8      407     605     737\n9         9      388     604     695\n10       10      422     595     710\n11       11      343     597     722\n12       12      377     590     697\n13       13      406     618     732\n14       14      421     598     720\n15       15      407     603     719\n16       16      400     614     719\n17       17      387     585     711\n18       18      407     598     709\n19       19      387     604     729\n20       20      405     618     746\n21       21      378     599     734\n22       22      390     601     723\n23       23      399     587     711\n24       24      398     609     742\n25       25      398     591     705\n26       26      401     607     712\n27       27      387     591     711\n28       28      410     603     737\n29       29      396     585     688\n30       30      375     601     731\n31       31      398     596     715\n32       32      406     579     696\n33       33      405     633     756\n34       34      422     607     739\n35       35      395     578     708\n36       36      391     597     732\n37       37      384     568     672\n38       38      426     590     716\n39       39      390     587     701\n40       40      399     586     693\n41       41      373     589     711\n42       42      441     602     722\n43       43      365     600     728\n44       44      397     591     727\n45       45      417     615     748\n46       46      374     606     720\n47       47      398     617     742\n48       48      390     594     727\n49       49      404     579     707\n50       50      403     603     708\n51       51      414     609     740\n52       52      417     606     724\n\n# add random September incidence\nmutate(\n    wide_df,\n    sep_inc = rbinom(52, 1000, 0.7)\n)\n\n   state_id july_inc aug_inc sep_inc\n1         1      399     613     702\n2         2      409     578     722\n3         3      381     604     711\n4         4      381     607     709\n5         5      387     603     684\n6         6      372     614     682\n7         7      403     597     689\n8         8      407     605     706\n9         9      388     604     688\n10       10      422     595     690\n11       11      343     597     688\n12       12      377     590     674\n13       13      406     618     708\n14       14      421     598     711\n15       15      407     603     718\n16       16      400     614     700\n17       17      387     585     706\n18       18      407     598     680\n19       19      387     604     702\n20       20      405     618     705\n21       21      378     599     701\n22       22      390     601     691\n23       23      399     587     704\n24       24      398     609     689\n25       25      398     591     694\n26       26      401     607     708\n27       27      387     591     703\n28       28      410     603     650\n29       29      396     585     706\n30       30      375     601     713\n31       31      398     596     725\n32       32      406     579     704\n33       33      405     633     690\n34       34      422     607     713\n35       35      395     578     721\n36       36      391     597     716\n37       37      384     568     677\n38       38      426     590     703\n39       39      390     587     708\n40       40      399     586     704\n41       41      373     589     719\n42       42      441     602     697\n43       43      365     600     703\n44       44      397     591     716\n45       45      417     615     721\n46       46      374     606     716\n47       47      398     617     679\n48       48      390     594     706\n49       49      404     579     705\n50       50      403     603     688\n51       51      414     609     717\n52       52      417     606     691\n\n\nIf we wanted to update a column, we can do that by specifying the column on both sides of the equals sign.\n\n# Update the August incidence to add random noise\nmutate(\n    wide_df,\n    aug_inc = aug_inc + round(rnorm(52, 0, 10), digits = 0)\n)\n\n   state_id july_inc aug_inc\n1         1      399     609\n2         2      409     587\n3         3      381     614\n4         4      381     616\n5         5      387     577\n6         6      372     605\n7         7      403     606\n8         8      407     598\n9         9      388     608\n10       10      422     597\n11       11      343     608\n12       12      377     583\n13       13      406     617\n14       14      421     601\n15       15      407     616\n16       16      400     612\n17       17      387     588\n18       18      407     585\n19       19      387     610\n20       20      405     620\n21       21      378     605\n22       22      390     597\n23       23      399     604\n24       24      398     608\n25       25      398     592\n26       26      401     606\n27       27      387     599\n28       28      410     595\n29       29      396     587\n30       30      375     607\n31       31      398     584\n32       32      406     581\n33       33      405     627\n34       34      422     621\n35       35      395     585\n36       36      391     580\n37       37      384     577\n38       38      426     592\n39       39      390     581\n40       40      399     582\n41       41      373     576\n42       42      441     606\n43       43      365     584\n44       44      397     590\n45       45      417     620\n46       46      374     607\n47       47      398     626\n48       48      390     595\n49       49      404     602\n50       50      403     594\n51       51      414     624\n52       52      417     590\n\n\nOne crucial thing to note is that mutate() applies our function/operation to each row simultaneously, so the new column’s value only depends on the row’s original values (or the vector in the case of the second example that didn’t use the values from the data).\npaste0()\nThe paste0() function is useful for manipulating objects and coercing them into string, allowing us to do string interpolation. It comes installed with base R, so there’s nothing to install, and because of the way mutate() works, apply functions to each row simultaneously, we can modify whole columns at once, depending on the row’s original values. It works to squish all the values together, without any separators by default. If you wanted spaces between your words, for example, you can use the paste(..., sep = \" \") function, which takes the sep argument.\n\nchar_df &lt;- mutate(\n    long_df,\n    # Notice that text is in commas, and object values being passed to paste0()\n    # are unquoted.\n    state_id = paste0(\"state_\", state_id)\n)\n\nchar_df\n\n# A tibble: 104 × 3\n   state_id month incidence\n   &lt;chr&gt;    &lt;chr&gt;     &lt;int&gt;\n 1 state_1  july        399\n 2 state_1  aug         613\n 3 state_2  july        409\n 4 state_2  aug         578\n 5 state_3  july        381\n 6 state_3  aug         604\n 7 state_4  july        381\n 8 state_4  aug         607\n 9 state_5  july        387\n10 state_5  aug         603\n# ℹ 94 more rows\n\n\nglue::glue()\nglue() is a function that comes installed with tidyverse, but is not loaded automatically, so you have to reference it explicitly by either using library(glue) or the :: notation shown below. It serves the same purpose as the base paste0(), but in a slightly different syntax. Instead of using a mix of quotations and unquoted object names, glue() requires everything to be in quotation marks, with any value being passed to the string interpolation being enclosed in { }. It is worth learning glue() as it is used throughout the tidyverse packages, such as in the pivot_wider() function.\n\nchar_df &lt;- mutate(\n    long_df,\n    state_id = glue::glue(\"state_{state_id}\")\n)\n\nchar_df\n\n# A tibble: 104 × 3\n   state_id month incidence\n   &lt;glue&gt;   &lt;chr&gt;     &lt;int&gt;\n 1 state_1  july        399\n 2 state_1  aug         613\n 3 state_2  july        409\n 4 state_2  aug         578\n 5 state_3  july        381\n 6 state_3  aug         604\n 7 state_4  july        381\n 8 state_4  aug         607\n 9 state_5  july        387\n10 state_5  aug         603\n# ℹ 94 more rows\n\n\nstr_replace_all()\nIf we want to replace characters throughout the whole of a string vector, we can do that with the str_replace_all() function. And because dataframes are made up of individual vectors, we can use this to modify vectors.\n\nmutate(\n    char_df,\n    # pass in the vector (a column, here), the pattern to remove, and the replacement\n    clean_state_id = str_replace_all(state_id, \"state_\", \"\")\n)\n\n# A tibble: 104 × 4\n   state_id month incidence clean_state_id\n   &lt;glue&gt;   &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;         \n 1 state_1  july        399 1             \n 2 state_1  aug         613 1             \n 3 state_2  july        409 2             \n 4 state_2  aug         578 2             \n 5 state_3  july        381 3             \n 6 state_3  aug         604 3             \n 7 state_4  july        381 4             \n 8 state_4  aug         607 4             \n 9 state_5  july        387 5             \n10 state_5  aug         603 5             \n# ℹ 94 more rows\n\n\nacross()\nAbove, we were only mutating a single column at a time, which is what we often do. But, sometimes we want to apply the exact same transformation to multiple columns. For example, say we wanted to turn our monthly incidence data into the average weekly incidence. We could write out each transformation by hand, but when there are more than two columns, this gets rather tedious and introduces the opportunity for mistakes when copying code (one of our motivations for using functions). The tidyselect::across() function allows us to specify the columns we want to apply the transformation, and the function (can be named or anonymous), and that’s it.\nThere are a couple of points to understand about the code below:\n\nNote the . preceding the cols, fns, and x\n\nEach column is passed to the .x value in the function argument\n\n~ is required to pass arguments into the function. In this case it is an anonymous function using the map_*() syntax.\n\n\nmutate(\n    wide_df,\n    across(\n        .cols = c(july_inc, aug_inc),\n        .fns = ~.x * 7 / 30\n    )\n)\n\n   state_id  july_inc  aug_inc\n1         1  93.10000 143.0333\n2         2  95.43333 134.8667\n3         3  88.90000 140.9333\n4         4  88.90000 141.6333\n5         5  90.30000 140.7000\n6         6  86.80000 143.2667\n7         7  94.03333 139.3000\n8         8  94.96667 141.1667\n9         9  90.53333 140.9333\n10       10  98.46667 138.8333\n11       11  80.03333 139.3000\n12       12  87.96667 137.6667\n13       13  94.73333 144.2000\n14       14  98.23333 139.5333\n15       15  94.96667 140.7000\n16       16  93.33333 143.2667\n17       17  90.30000 136.5000\n18       18  94.96667 139.5333\n19       19  90.30000 140.9333\n20       20  94.50000 144.2000\n21       21  88.20000 139.7667\n22       22  91.00000 140.2333\n23       23  93.10000 136.9667\n24       24  92.86667 142.1000\n25       25  92.86667 137.9000\n26       26  93.56667 141.6333\n27       27  90.30000 137.9000\n28       28  95.66667 140.7000\n29       29  92.40000 136.5000\n30       30  87.50000 140.2333\n31       31  92.86667 139.0667\n32       32  94.73333 135.1000\n33       33  94.50000 147.7000\n34       34  98.46667 141.6333\n35       35  92.16667 134.8667\n36       36  91.23333 139.3000\n37       37  89.60000 132.5333\n38       38  99.40000 137.6667\n39       39  91.00000 136.9667\n40       40  93.10000 136.7333\n41       41  87.03333 137.4333\n42       42 102.90000 140.4667\n43       43  85.16667 140.0000\n44       44  92.63333 137.9000\n45       45  97.30000 143.5000\n46       46  87.26667 141.4000\n47       47  92.86667 143.9667\n48       48  91.00000 138.6000\n49       49  94.26667 135.1000\n50       50  94.03333 140.7000\n51       51  96.60000 142.1000\n52       52  97.30000 141.4000\n\n\neverything()\nIf we wanted to select every column in a dataframe, we would use the everything() function. This may not seem helpful initially, but there are occasions when it’s very useful. For instance, in the previous example we still specified the exact columns we wanted to transform. However, if there were five times as many, we wouldn’t want to do that. Do note that if we replaced this with everything(), we would also mutate() our state_id column, which we probably don’t want to do, so we could combine it with the - selection seen previously.\ncontains()\nAnother very handy function is the tidyselect::contains() function. This allows us to specify a string that the column names must contain for them to be selected. We could change the above example to look like this:\n\nmutate(\n    wide_df,\n    across(\n        .cols = contains(\"_inc\"),\n        .fns = ~.x * 7 / 30\n    )\n)\n\n   state_id  july_inc  aug_inc\n1         1  93.10000 143.0333\n2         2  95.43333 134.8667\n3         3  88.90000 140.9333\n4         4  88.90000 141.6333\n5         5  90.30000 140.7000\n6         6  86.80000 143.2667\n7         7  94.03333 139.3000\n8         8  94.96667 141.1667\n9         9  90.53333 140.9333\n10       10  98.46667 138.8333\n11       11  80.03333 139.3000\n12       12  87.96667 137.6667\n13       13  94.73333 144.2000\n14       14  98.23333 139.5333\n15       15  94.96667 140.7000\n16       16  93.33333 143.2667\n17       17  90.30000 136.5000\n18       18  94.96667 139.5333\n19       19  90.30000 140.9333\n20       20  94.50000 144.2000\n21       21  88.20000 139.7667\n22       22  91.00000 140.2333\n23       23  93.10000 136.9667\n24       24  92.86667 142.1000\n25       25  92.86667 137.9000\n26       26  93.56667 141.6333\n27       27  90.30000 137.9000\n28       28  95.66667 140.7000\n29       29  92.40000 136.5000\n30       30  87.50000 140.2333\n31       31  92.86667 139.0667\n32       32  94.73333 135.1000\n33       33  94.50000 147.7000\n34       34  98.46667 141.6333\n35       35  92.16667 134.8667\n36       36  91.23333 139.3000\n37       37  89.60000 132.5333\n38       38  99.40000 137.6667\n39       39  91.00000 136.9667\n40       40  93.10000 136.7333\n41       41  87.03333 137.4333\n42       42 102.90000 140.4667\n43       43  85.16667 140.0000\n44       44  92.63333 137.9000\n45       45  97.30000 143.5000\n46       46  87.26667 141.4000\n47       47  92.86667 143.9667\n48       48  91.00000 138.6000\n49       49  94.26667 135.1000\n50       50  94.03333 140.7000\n51       51  96.60000 142.1000\n52       52  97.30000 141.4000\n\n\nrename_with()\nIf we wanted to rename columns of a dataframe, we can use the rename() function. However, like the previous tidyselect examples, sometimes we want to apply the same renaming scheme (function) to the columns. rename_with() allows us to pass a function to multiple columns at once, achieving what we want with minimal effort, and without needing to use across().\n\nrename_with(\n    wide_df,\n    .cols = contains(\"_inc\"),\n    .fn = ~str_replace_all(.x, \"_inc\", \"_incidence\")\n)\n\n   state_id july_incidence aug_incidence\n1         1            399           613\n2         2            409           578\n3         3            381           604\n4         4            381           607\n5         5            387           603\n6         6            372           614\n7         7            403           597\n8         8            407           605\n9         9            388           604\n10       10            422           595\n11       11            343           597\n12       12            377           590\n13       13            406           618\n14       14            421           598\n15       15            407           603\n16       16            400           614\n17       17            387           585\n18       18            407           598\n19       19            387           604\n20       20            405           618\n21       21            378           599\n22       22            390           601\n23       23            399           587\n24       24            398           609\n25       25            398           591\n26       26            401           607\n27       27            387           591\n28       28            410           603\n29       29            396           585\n30       30            375           601\n31       31            398           596\n32       32            406           579\n33       33            405           633\n34       34            422           607\n35       35            395           578\n36       36            391           597\n37       37            384           568\n38       38            426           590\n39       39            390           587\n40       40            399           586\n41       41            373           589\n42       42            441           602\n43       43            365           600\n44       44            397           591\n45       45            417           615\n46       46            374           606\n47       47            398           617\n48       48            390           594\n49       49            404           579\n50       50            403           603\n51       51            414           609\n52       52            417           606\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHopefully you are noticing a pattern between the tidyselect-type functions. When you need to apply a function to multiple columns in a dataframe, you will select the columns with the .cols argument, and pass the function to the .fn(s) argument with the ~ symbol indicating you are using the .x to represent the column in the function (yes, there is a touch of ambiguity between .fns and .fn, but the general pattern holds). This will be useful when we look at the map_*() family of functions.\n\n\nmagrittr::%&gt;%\nThe %&gt;% operator is an interesting and very useful function that comes installed (and loaded) with the tidyverse package (technically from the magrittr package from within the tidyverse). It allows us to chain together operations without needing to create intermediate objects. Say for example we have our wide incidence data and want to add data for September before turning it into a long dataframe, we could create and intermediate object before using the pivot_longer() function from before, but we might not want to create another object that we don’t really care about. This is when we would want to use a pipe, as it takes the output of one operation and pipes it into the next one.\n\nmutate(\n    wide_df,\n    sep_inc = round(aug_inc * 1.2 + rnorm(52, 0, 10), digits = 0)\n    ) %&gt;%\n    pivot_longer(\n        cols = c(july_inc, aug_inc, sep_inc),\n        names_to = \"month\",\n        values_to = \"incidence\",\n        names_pattern = \"(.*)_inc\",\n        data = .\n    )\n\n# A tibble: 156 × 3\n   state_id month incidence\n      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1        1 july        399\n 2        1 aug         613\n 3        1 sep         725\n 4        2 july        409\n 5        2 aug         578\n 6        2 sep         685\n 7        3 july        381\n 8        3 aug         604\n 9        3 sep         710\n10        4 july        381\n# ℹ 146 more rows\n\n\nBy default, the previous object gets input into the first argument of the next function, but here we’ve shown that you can manipulate the position the object is piped into by specify the argument using the . syntax.\n|&gt;\nIn R version 4.1.0, the |&gt; was added as the base pipe operator. It works slightly differently to %&gt;%, and frankly, is less powerful and less common (at the moment), so we won’t use it in this workshop.\ngroup_by()\nIf we have groups in our dataframe and want to apply some function to each group’s data, we can use the group_by() function. For example, if we wanted to calculate the mean and median incidence in our fake data from earlier, but group it by the month.\n\ngroup_by(long_df, month) %&gt;%\n    summarize(mean = mean(incidence), median = median(incidence))\n\n# A tibble: 2 × 3\n  month  mean median\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 aug    599.   600.\n2 july   397.   398 \n\n\npivot_*()\nWe’ve already seen the purpose of the pivot_longer() function: taking wide data and reshaping it to be long. There is an equivalent to go from long to wide: pivot_wider(). Occassionally this is useful (though it is less common than creating long data).\n\npivot_wider(\n    long_df,\n    names_from = month,\n    values_from = incidence,\n    names_glue = \"{month}_inc\"\n)\n\n# A tibble: 52 × 3\n   state_id july_inc aug_inc\n      &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n 1        1      399     613\n 2        2      409     578\n 3        3      381     604\n 4        4      381     607\n 5        5      387     603\n 6        6      372     614\n 7        7      403     597\n 8        8      407     605\n 9        9      388     604\n10       10      422     595\n# ℹ 42 more rows\n\n\nHere, the names_glue argument is making use of the glue::glue() function (see above) that is installed with tidyverse, but not loaded automatically for use by the users.\nmap_*()\nThe map_*() functions come from the purrr package (a core part of the tidyverse), and are incredibly useful. They are relatively complicated, so there isn’t enough space to go into full detail, but here we’ll just outline enough so you can read more and understand what’s going on.\nWe’ve already seen we can apply functions to each element of a vector (atomic or list vectors). The key points to note are the . preceding the x and f arguments. If we use map() we get a list returned, map_dbl() a double vector, map_char() a character vector, map_dfr() a dataframe etc.\nIn the example below, we’ll walk through map_dfr() as it’s one of the more confusing variants due to the return requirements.\n\nmap_dfr_example &lt;- map_dfr(\n    .x = my_dbl_vec,\n    .f = function(.x) {\n        # Note we don't use , at the end of each line - it's as if we were\n        # running the code in the console\n        times_ten &lt;- .x * 10\n        divide_ten &lt;- .x / 10\n\n        # construct a tibble as normal (requires , between arguments)\n        tibble(\n            original_val = .x,\n            times_ten = times_ten,\n            divide_ten = divide_ten\n        )\n    }\n)\n\nmap_dfr_example\n\n# A tibble: 10 × 3\n   original_val times_ten divide_ten\n          &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1            1        10        0.1\n 2            2        20        0.2\n 3            3        30        0.3\n 4            4        40        0.4\n 5            5        50        0.5\n 6            6        60        0.6\n 7            7        70        0.7\n 8            8        80        0.8\n 9            9        90        0.9\n10           10       100        1  \n\n\nWhat’s happening under the hood is that map_dfr() is applying the anonymous function we defined to each element in our vector and returning a list of dataframes that contains one row and three columns, i.e. for the first element, we would get this:\n\nlist(map_dfr_example[1, ])\n\n[[1]]\n# A tibble: 1 × 3\n  original_val times_ten divide_ten\n         &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1            1        10        0.1\n\n\nIt then calls the bind_rows() function to squash all of those dataframes together, one row stacked on top of the next, to create one large dataframe. We could write the equivalent code like this:\n\nbind_rows(\n    map(\n    .x = my_dbl_vec,\n    .f = function(.x) {\n        # Note we don't use , at the end of each line - it's as if we were\n        # running the code in the console\n        times_ten &lt;- .x * 10\n        divide_ten &lt;- .x / 10\n\n        # construct a tibble as normal (requires , between arguments)\n        tibble(\n            original_val = .x,\n            times_ten = times_ten,\n            divide_ten = divide_ten\n        )\n    }\n)\n)\n\n# A tibble: 10 × 3\n   original_val times_ten divide_ten\n          &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1            1        10        0.1\n 2            2        20        0.2\n 3            3        30        0.3\n 4            4        40        0.4\n 5            5        50        0.5\n 6            6        60        0.6\n 7            7        70        0.7\n 8            8        80        0.8\n 9            9        90        0.9\n10           10       100        1  \n\n\nmap_dfc() does exactly the same thing, but calls bind_cols() instead, to place the columns next to each other.\nThere is one more important variant to go through: pmap_*(). If map_*() takes one vector as an argument, pmap_*() takes a list of arguments. What this means is that we can iterate through the elements of as many arguments as we’d like, in sequence. For example, let’s multiply the elements of two double vectors together.\n\n# Create a second vector of numbers\nmy_second_dbl_vec &lt;- rnorm(length(my_dbl_vec), 20, 20)\nmy_second_dbl_vec\n\n [1] 45.583594  7.463083 20.505265 46.030180 15.004206 22.699967 17.066535\n [8] 44.678612 22.708520 21.344806\n\n# Remind ourselves what our original vector looks like\nmy_dbl_vec\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\npmap_dbl(\n    .l = list(first_num = my_dbl_vec, sec_num = my_second_dbl_vec),\n    .f = function(first_num, sec_num) {\n        first_num * sec_num\n    }\n)\n\n [1]  45.58359  14.92617  61.51580 184.12072  75.02103 136.19980 119.46575\n [8] 357.42890 204.37668 213.44806\n\n\nThere are a couple of important points to note here:\n\nAll vectors need to be the same length\nThe function is applied to each element index of the input vectors, i.e., the first elements of the vectors are multiplied together, the second element of the vectors are multiplied together, and so on, until the last elements are reached.\nWe use .l instead of .x to denote we are passing a list() of vectors.\nOur function specifies the names of the vectors in the list(), which are then used within the function itself (similar to how we used .x in our map_*() functions)\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nAs before, this is an unnecessary approach as R would vectorize the operation, but it is useful to demonstrate the principle.\n\nmy_dbl_vec * my_second_dbl_vec\n\n [1]  45.58359  14.92617  61.51580 184.12072  75.02103 136.19980 119.46575\n [8] 357.42890 204.37668 213.44806\n\n\n\n\n\nnest()\nNesting is a relatively complex, but powerful, concept, particularly when combined with the map_*() functions. Commonly, as in this workshop, it is used to apply a model function to multiple different datasets, and store them all in one dataframe for easy of manipulation. What it effectively does is group your existing dataframe by a variable, and then shrink all the columns (except the grouping column), into a single list column, leaving you with as many rows as there are distinct groups. Each element of the new list column is itself a small dataframe that contains all the original variables and data, but only those that are relevant for the group. Hopefully this example will make it clearer. Here, we’ll take the mtcars dataset, and like before, we’ll group by the cyl variable, but this time we’ll nest the rest of the data.\n\nnested_mtcars &lt;- nest(mtcars, data = -cyl)\nnested_mtcars\n\n# A tibble: 3 × 2\n    cyl data              \n  &lt;dbl&gt; &lt;list&gt;            \n1     6 &lt;tibble [7 × 10]&gt; \n2     4 &lt;tibble [11 × 10]&gt;\n3     8 &lt;tibble [14 × 10]&gt;\n\n\nWe can see we’ve nested all columns, except cyl. Looking at the data column for just the first row (cyl == 6), we see we have a list with one item: the rest of the data that’s relevant to the rows where cyl == 6 (notice the [[1]] above the tibble).\n\nnested_mtcars[1, ]$data\n\n[[1]]\n# A tibble: 7 × 10\n    mpg  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  21    160    110  3.9   2.62  16.5     0     1     4     4\n2  21    160    110  3.9   2.88  17.0     0     1     4     4\n3  21.4  258    110  3.08  3.22  19.4     1     0     3     1\n4  18.1  225    105  2.76  3.46  20.2     1     0     3     1\n5  19.2  168.   123  3.92  3.44  18.3     1     0     4     4\n6  17.8  168.   123  3.92  3.44  18.9     1     0     4     4\n7  19.7  145    175  3.62  2.77  15.5     0     1     5     6\n\n\nNow we can use map to fit a model to this subsetted data.\n\nmutate(\n    nested_mtcars,\n    model_fit = map(data, ~glm(mpg ~ hp + wt + ordered(carb), data = .x))\n)\n\n# A tibble: 3 × 3\n    cyl data               model_fit\n  &lt;dbl&gt; &lt;list&gt;             &lt;list&gt;   \n1     6 &lt;tibble [7 × 10]&gt;  &lt;glm&gt;    \n2     4 &lt;tibble [11 × 10]&gt; &lt;glm&gt;    \n3     8 &lt;tibble [14 × 10]&gt; &lt;glm&gt;    \n\n\nThis creates a list column (because we used the map() function, which returns a list) that contains the relevant model fits.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to note that there is also a function called nest_by(). However, it returns a rowwise tibble, i.e., any later manipulations will be applied on a row-by-row basis, unlike a standard tibble that applies the manipulation to every row all at once, so we would need to use normal mutate() syntax (and explicitly return a list column) to get the same effect as before.\n\nnest_by(mtcars, .by = cyl) %&gt;%\n    mutate(model_fit = list(glm(mpg ~ hp + wt + ordered(carb), data = data)))\n\n# A tibble: 3 × 3\n# Rowwise:  .by\n    .by                data model_fit\n  &lt;dbl&gt; &lt;list&lt;tibble[,11]&gt;&gt; &lt;list&gt;   \n1     4           [11 × 11] &lt;glm&gt;    \n2     6            [7 × 11] &lt;glm&gt;    \n3     8           [14 × 11] &lt;glm&gt;    \n\n\n\n\nggplot()\nTo create out plots, we can use the base plot() functions, but ggplot2 package provides a clean and consistent interface to plotting that has many benefits. In essence, plots are built up in layers, with each stacking on top of the previous.\nTo initialize a plot, we simply use the ggplot() function call, that creates the background of a figure. Now we need to add data, and geoms to interpret that data.\nLet’s use the mtcars dataset again.\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nLooking at the data, we might be interested in how the mpg of a car is affected by it horsepower (hp). To add data, we just use the ggplot() function argument data = mtcars. We also need to tell ggplot() how to map the data points to the figure, i.e., the values for the x and y axes.\nBecause this depends on the underlying data, this must go within an argument called aes() i.e., aes(x = hp, y = mpg).\nTo add a layer to show the data, we add a geom. In this case, because we have continuous independent and dependent variables, we could use the geom_point() geom, that will give us a scatter plot. Much like basic arithmetic, we add layers using the + operator.\n\nggplot(data = mtcars, aes(x = hp, y = mpg)) +\n    geom_point()\n\n\n\n\n\n\n\nNow let’s imagine we wanted to explore this relationship, but separated by engine type (the vs column). We can use color to separate these points. Because this is an argument that depends on the underlying data, again, this must be placed within aes().\n\nggplot(data = mtcars, aes(x = hp, y = mpg, color = vs)) +\n    geom_point()\n\n\n\n\n\n\n\nWhat you’ll notice here is that despite vs being a binary choice, because it is of type double, ggplot() interprets this as a number, so provides a continuous color scale. To correct this, let’s convert vs into a factor before plotting.\n\nmtcars %&gt;%\n    mutate(vs = factor(vs)) %&gt;%\n    ggplot(aes(x = hp, y = mpg, color = vs)) +\n    geom_point()\n\n\n\n\n\n\n\nWe can change the theme by layering in more information, as we did with the other plotting layers. Here, let’s change the background to white, and add some different colors. We’ll also change the size of the points.\n\nmtcars %&gt;%\n    mutate(vs = factor(vs)) %&gt;%\n    ggplot(aes(x = hp, y = mpg, color = vs)) +\n    geom_point(size = 5) +\n    theme_minimal() +\n    # We don't need to specify the relationship between the levels and the colors\n    # and labels, but it means we're less likely to make a mistake in interpretation\n    # and labelling\n    scale_color_manual(\n        values = c(\"0\" = \"#6b3df5ff\", \"1\" = \"#f5c13cff\"),\n        labels = c(\"0\" = \"V-Shaped\", \"1\" = \"Straight\")\n    )\n\n\n\n\n\n\n\nImagine we wanted to use one more grouping: automatic vs manual transmission (am). Rather than adding yet another color, we could do something called a facet_wrap(), which creates separate panels for each group. Adding this to a ggplot() is very easy - it’s just another + operation! As before, we will add labels for easier interpretation.\n\nmtcars %&gt;%\n    mutate(vs = factor(vs)) %&gt;%\n    ggplot(aes(x = hp, y = mpg, color = vs)) +\n    geom_point(size = 5) +\n    theme_minimal() +\n    # We don't need to specify the relationship between the levels and the colors\n    # and labels, but it means we're less likely to make a mistake in interpretation\n    # and labelling\n    scale_color_manual(\n        values = c(\"0\" = \"#6b3df5ff\", \"1\" = \"#f5c13cff\"),\n        labels = c(\"0\" = \"V-Shaped\", \"1\" = \"Straight\")\n    ) +\n    facet_wrap(~am, labeller = as_labeller(c(\"0\" = \"Automatic\", \"1\" = \"Manual\")))\n\n\n\n\n\n\n\nThis is looking much better, but we might want to add a line to show the trends within the groups. Again, this is as simple as adding another layer. One thing to note about the plot below, because we specified the data and aes() arguments in the original ggplot() function call, those data relationships will also be applied to our new geom. We could just as easily write them within the geom_*() explicitly, but then we would have to do that for each geom_*() in our plot, which is unnecessary when they all have the same data relationships. To demonstrate this, let’s also make a small modification so that only the points are colored, and the lines are all red. To do that, we will remove color = vs from the global aes(), and add it to one specific to geom_point(). But because we still want to fit a linear model to the different engine types (vs) separately, we will add group = vs to the geom_smooth(aes(), ...) call, to let ggplot() know to treat them as separate groups for the geom_smooth() Because the line color doesn’t depend on the data, it is not in an aes() argument call.\n\nmtcars %&gt;%\n    mutate(vs = factor(vs)) %&gt;%\n    ggplot(aes(x = hp, y = mpg)) +\n    geom_point(aes(color = vs), size = 5) +\n    geom_smooth(aes(group = vs), color = \"red\", method = \"lm\") +\n    theme_minimal() +\n    # We don't need to specify the relationship between the levels and the colors\n    # and labels, but it means we're less likely to make a mistake in interpretation\n    # and labelling\n    scale_color_manual(\n        values = c(\"0\" = \"#6b3df5ff\", \"1\" = \"#f5c13cff\"),\n        labels = c(\"0\" = \"V-Shaped\", \"1\" = \"Straight\")\n    ) +\n    facet_wrap(~am, labeller = as_labeller(c(\"0\" = \"Automatic\", \"1\" = \"Manual\")))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAs you can see, once you get used to it, the layering system makes it relatively intuitive to build complex and interesting plots. We’ve only stratched the surface here, so be sure to read the suggested books and the {ggplot2} cheatsheet for more information.\n%*%\nThis is the matrix multiplication operator. It works exactly as you’d expect given matrix multiplication rules. As such, you can use it on any combination of vectors and matrices.\n\n\n\n\n\n\nImportant\n\n\n\nAs you can see below, R treats vectors as dimensionless, and will try to convert it to either a row or column vector, depending on what makes sense for the matrix multiplication\n\n\n\nmy_dbl_vec %*% my_second_dbl_vec\n\n         [,1]\n[1,] 1412.086\n\n\n\nmy_matrix &lt;- matrix(1:60, nrow = 10)\nmy_matrix\n\n      [,1] [,2] [,3] [,4] [,5] [,6]\n [1,]    1   11   21   31   41   51\n [2,]    2   12   22   32   42   52\n [3,]    3   13   23   33   43   53\n [4,]    4   14   24   34   44   54\n [5,]    5   15   25   35   45   55\n [6,]    6   16   26   36   46   56\n [7,]    7   17   27   37   47   57\n [8,]    8   18   28   38   48   58\n [9,]    9   19   29   39   49   59\n[10,]   10   20   30   40   50   60\n\nmy_dbl_vec\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nmy_dbl_vec %*% my_matrix\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  385  935 1485 2035 2585 3135\n\nmy_matrix %*% my_dbl_vec\n\nError in my_matrix %*% my_dbl_vec: non-conformable arguments\n\nmy_matrix %*% t(my_dbl_vec)\n\nError in my_matrix %*% t(my_dbl_vec): non-conformable arguments\n\nt(my_matrix) %*% my_dbl_vec\n\n     [,1]\n[1,]  385\n[2,]  935\n[3,] 1485\n[4,] 2035\n[5,] 2585\n[6,] 3135",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "project-management.html",
    "href": "project-management.html",
    "title": "Organizing A Project",
    "section": "",
    "text": "Project Structure\nThe purpose of this workshop is not to teach you how to manage everything about your modeling project. However, there are some tips that we think are useful and should reduce the amount of time you spend on finding and managing files, allowing you to focus more on the work at hand. If you want to get more in-depth, there are some suggested readings at the end that could be a starting point for further exploration.\nThere are many ways to structure a project, but we would recommend that each project has its own folder (directory), and all your project directories sit in a single place. For example, you could have a directory called Repos that holds all of your projects. That way, when you want to find a specific project, it’s easy and in one place. As part of this, we recommend you read the section about RStudio projects.\nA suggested layout could look like this (where ${HOME} denotes your home directory, i.e., ~/ on MacOS/Linux, and C:/ on Windows):",
    "crumbs": [
      "Pre-Requisites",
      "Organizing A Project"
    ]
  },
  {
    "objectID": "project-management.html#project-structure",
    "href": "project-management.html#project-structure",
    "title": "Organizing A Project",
    "section": "",
    "text": "Important\n\n\n\nIt is crucial that this folder does not live in a cloud-synced folder e.g., in OneDrive. Cloud accounts have an unfortunate habit of creating sync errors and often rename files to circumvent issues with merging differing copies. This will ruin any chance you have of using Git in the future, which is highly recommended, as Git relies on the file names being the same.\n\n\n\n${HOME}/\n└── Documents/\n    └── Repos/\n        └── Proj/\n            ├── data/\n            ├── figs/\n            ├── funs/\n            ├── out/\n            └── src/\n                ├── cleaning.R\n                └── analysis.R\n\ndata/\nAn important idea is that you should treat your data as read-only. You and your team have likely worked hard to collect the data and it’s easy to make a changes along the way that you either forget about, or need to reverse. As most projects span a long time between the data collection and analysis stages, it can be very difficult and time-consuming to try and reverse engineer exactly what changes have been made if the data files are directly edited. Therefore, to save yourself effort and help make your work reproducible, once the data is collected it should not be edited; all the work should happen in your code, allowing it to be easily checked.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you are reading in data files in your cleaning and analysis scripts, it is good practice to use relative paths. This means that if you share your code with others, everything should still work for them. If you use explicit paths e.g. read_csv(\"/Users/callumarnold/Documents/Repos/SISMID_2023/data/niamey.csv\") then this won’t work for your collaborators, as they don’t have the same computer set up as you!\nA package we recommend using in R is the {here} package, which would turn the above code into read_csv(here::here(\"data\", \"niamey.csv\")). Not only is this easier to read, but it leverages the principle that our projects are self-contained in their own folders and uses file paths that are relative to the root of the project, so it works regardless of where people install the project folder to.\n\n\n\n\nsrc/\nIt is common practice to keep your scripts (source code) in a folder named src/. Following this practice will make it easier for others to navigate your code, helping create a reproducible work environment. The files in here may be scripts to clean the data (remember, we are treating data as read-only), and others to produce the analysis. In our workshop, it would be a good idea to have a different file for each exercise e.g., r-session-01.R\n\n\nOther subdirectories\n\nfuns/: this contains the functions you write and might want to reference. The idea is to create functions so that can give code a meaningful name. It also helps if you need to repeat a code chunk multiple times, especially if you need to edit it at some point, as you can just call the function rather than typing it out each time.\nout/: this contains files that are produced from the original data e.g. cleaned data files. You can then call them in your analysis scripts.\nfigs/: this contains figures that may be generated from your scripts.",
    "crumbs": [
      "Pre-Requisites",
      "Organizing A Project"
    ]
  },
  {
    "objectID": "project-management.html#naming-files",
    "href": "project-management.html#naming-files",
    "title": "Organizing A Project",
    "section": "Naming Files",
    "text": "Naming Files\nPart of structuring a project is having creating file names that are easy to read; both for you and the computer. On that note, get rid of any spaces in your file and folder names! They make it much trickier to work with when you want to use them in code, whether that’s using bash/zsh for moving files quickly or using Git via the command line, or loading them in analysis scripts.\nJenny Bryan (of University of British Columbia and RStudio/Posit) has great slides here on the topic, but in summary:\n\nKISS (Keep It Simple Stupid): use simple and consistent file names\n\nIt needs to be machine readable\nIt needs to be human readable\nIt needs to order well in a directory (e.g., left-pad numbers)\n\nNo special characters and no spaces!\nUse YYYY-MM-DD date format\n\nFile systems will automatically order them sensibly\nUnambiguous, which is particularly important with international collaborators\n\nUse - to delimit words and _ to delimit sections\n\ni.e. 2019-01-19_my-data.csv\n\nLeft-pad numbers\n\ni.e. 01_my-data.csv vs 1_my-data.csv\nIf you don’t, file orders get messed up when you get to double-digits",
    "crumbs": [
      "Pre-Requisites",
      "Organizing A Project"
    ]
  },
  {
    "objectID": "project-management.html#other-resources",
    "href": "project-management.html#other-resources",
    "title": "Organizing A Project",
    "section": "Other Resources",
    "text": "Other Resources\n\nGit\nGit is an essential component of reproducible computation research, although it is very much out of the scope of this workshop. Think of it as a more powerful version of tracked changes for your code, merged with some of the collaborative abilities of Google Docs. If you would like to learn more about what it is, and how you could add it to your workflow, Callum created a small online book to accompany a Git and GitHub workshop he developed for Penn State. The link can be found here, and this will be continuously updated to add more complicated workflows and troubleshooting tips.\nJenny Bryan and co put togther a fantastic resource about using Git with R here. It has more of a focus on R and the use of R-specific tools and packages to help with Git (e.g. the {usethis} package), but is still plenty general for anyone to learn about Git and best practices.",
    "crumbs": [
      "Pre-Requisites",
      "Organizing A Project"
    ]
  },
  {
    "objectID": "project-management.html#project-structure-1",
    "href": "project-management.html#project-structure-1",
    "title": "Organizing A Project",
    "section": "Project Structure",
    "text": "Project Structure\n\nCallum wrote a short blog post about reproducible work that can be found here. The section about Jupyter notebooks are unlikely to be relevant to R users, but the rest is still useful.",
    "crumbs": [
      "Pre-Requisites",
      "Organizing A Project"
    ]
  },
  {
    "objectID": "project-management.html#renv",
    "href": "project-management.html#renv",
    "title": "Organizing A Project",
    "section": "{renv}",
    "text": "{renv}\n{renv} is a package that helps manage your project’s dependencies by creating a self-contained environment for your project. What this means is that each project will have a list of the required packages and their versions, and when you share your project with others, they can install the packages you used in your project with a single command (renv::restore()). To find out more about {renv}, visit the website here.",
    "crumbs": [
      "Pre-Requisites",
      "Organizing A Project"
    ]
  },
  {
    "objectID": "L01_intro-to-modeling.html",
    "href": "L01_intro-to-modeling.html",
    "title": "1  Intro to Modeling",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to Modeling</span>"
    ]
  },
  {
    "objectID": "L02_sir-basics.html",
    "href": "L02_sir-basics.html",
    "title": "2  Basics of SIR Models",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics of SIR Models</span>"
    ]
  },
  {
    "objectID": "r-session-01.html",
    "href": "r-session-01.html",
    "title": "\n3  R Session 01\n",
    "section": "",
    "text": "3.1 Interactive Plot\nCodeinit_beta = 0.3\ninit_dur_inf = 6.0\ninit_I0 = 0.01\ninit_births = 0.0\ninit_tmax = 200\nCodefunction set(input, value) {\n  input.value = value;\n  input.dispatchEvent(new Event(\"input\", {bubbles: true}));\n}",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Session 01</span>"
    ]
  },
  {
    "objectID": "r-session-01.html#sec-interactive-plots",
    "href": "r-session-01.html#sec-interactive-plots",
    "title": "\n3  R Session 01\n",
    "section": "",
    "text": "Codeviewof reset = Inputs.button([\n  [\"Reset all sliders\", () =&gt; {\n    set(viewof beta, init_beta)\n    set(viewof dur_inf, init_dur_inf)\n    set(viewof I0, init_I0)\n    set(viewof births, init_births)\n    set(viewof tmax, init_tmax)\n  }]\n])\n\nviewof beta = Inputs.range(\n  [0.0, 2.0],\n  {value: init_beta, step: 0.01, label: \"Transmission rate (per day)\"}\n)\n\nviewof dur_inf = Inputs.range(\n  [0.0, 20],\n  {value: init_dur_inf, step: 0.5, label: \"Duration of Infection (days)\"}\n)\n\nviewof I0 = Inputs.range(\n  [0.0, 1.0],\n  {value: init_I0, step: 0.01, label: \"Initial fraction infected\"}\n)\n\nviewof births = Inputs.range(\n  [0, 0.05],\n  {value: init_births, step: 0.001, label: \"Birth rate\"}\n)\n\nviewof tmax = Inputs.range(\n  [200, 600],\n  {value: init_tmax, step: 10.0, label: \"Maximum simulation time (years)\"}\n)\n\nviewof area = Inputs.toggle(\n  {label: \"Cumulative Area plot\", value: false}\n)\n\nmd`${tex`R_0 = ${R0_str}`}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode{\n  if (births == 0){\n    var finalsize = sir_sol.get(\"R\", sir_sol.numRows()-1)\n    var finalsize_str = finalsize.toLocaleString(undefined, {minimumFractionDigits: 2})\n\n    return md`${tex`\\text{Final size} = ${finalsize_str}`}`\n  } else {\n    return md``\n  }\n}\n\n\n\n\n\n\n\n\n\n\nCodegamma = 1 / dur_inf\ndt = 0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeimport {odeRK4} from '@rreusser/integration@3064'\nimport { aq, op } from '@uwdata/arquero'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefunction sir(dydt, y, t) {\n  dydt[0] = - beta * y[0] * y[1] + births * (1 - y[0])\n  dydt[1] = beta * y[0] * y[1] - gamma * y[1] - births * y[1]\n  dydt[2] = gamma * y[1] - births * y[2]\n}\n\n\n\n\n\n\n\nCodefunction simulate(f, t0, y0, dt, tmax) {\n  var t = t0\n  var y = y0\n  var i = 0\n\n  var tsim = [t0]\n  var ysim = [y0]\n\n  for (t = t0 + dt; t &lt;= tmax; t += dt) {\n    ysim.push(odeRK4([], ysim[i], f, dt))\n    tsim.push(t)\n    i += 1\n  }\n\n  return aq.table({\n    Time: tsim,\n    S: ysim.map(d =&gt; d[0]),\n    I: ysim.map(d =&gt; d[1]),\n    R: ysim.map(d =&gt; d[2])\n    })\n}\n\n\n\n\n\n\n\nCodesir_sol = simulate(sir, 0, [1.0-I0, I0, 0.0], dt, tmax)\nsir_sol_long = sir_sol.fold(aq.not('Time'), {as: ['State', 'Fraction']})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeSIRcolors = [\"#1f77b4\", \"#ff7f0e\", \"#FF3851\"]\n\n\n\n\n\n\n\nCodeR0 = beta / (gamma + births)\nR0_str = R0.toLocaleString(undefined, {minimumFractionDigits: 2})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefunction calculate_equil(R0){\n  if (births &gt; 0) {\n    var eq_S = 1 / R0\n    var eq_I = births / beta * (R0 - 1)\n    var eq_R = 1 - (eq_S + eq_I)\n\n    const eq_vals = aq.table({\n      State: [\"S\", \"I\", \"R\"],\n      Fraction: [eq_S, eq_I, eq_R]\n    })\n\n    return eq_vals\n  } else {\n    return null\n  }\n}\n\n\n\n\n\n\n\nCodeeq_vals = calculate_equil(R0)\n\n\n\n\n\n\n\n\n\nCodePlot.plot({\n  color: {\n    legend: true,\n    domain: [\"S\", \"I\", \"R\"],\n    range: SIRcolors\n  },\n  style: {fontSize: \"20px\"},\n  marginLeft: 65,\n  marginTop: 40,\n  marginBottom: 55,\n  grid: true,\n  width: 800,\n  height: 670,\n  y: {domain: [0, 1]},\n  marks: [\n    area ?\n      Plot.areaY(sir_sol_long, {x: \"Time\", y: \"Fraction\", fill: \"State\"}) :\n      [\n        R0 &gt;= 1.0 && births &gt; 0 ?\n        Plot.ruleY(\n          eq_vals,\n          {y: \"Fraction\", stroke: \"State\", strokeWidth: 2, strokeDasharray: [10]}\n        ) :\n        null,\n        Plot.lineY(\n          sir_sol_long,\n          {x: \"Time\", y: \"Fraction\", stroke: \"State\", strokeWidth: 6}\n        )\n      ]\n  ]\n})",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Session 01</span>"
    ]
  },
  {
    "objectID": "r-session-01.html#closed-population-model-intuition",
    "href": "r-session-01.html#closed-population-model-intuition",
    "title": "\n3  R Session 01\n",
    "section": "\n3.2 Closed Population Model Intuition",
    "text": "3.2 Closed Population Model Intuition\nWe’ll first get develop an intuition for the closed population model. We’ll then extend this intuition to the open population model. When the Birth rate slider in the side panel of the interactive figure is set to 0, we have no births or deaths, so their is no replenishment of the susceptible population i.e., it is a closed population.\n\n\n\n\n\n\nSET\n\n\n\nTransmission rate = 1\nDuration of infection = 4.\n\n\n\n3.2.1 What is \\(R_0\\)?\n\n3.2.2 What is epidemic final size?\n\n3.2.3 Does this make sense given our definition of \\(R_0\\)?\n\n\n\n\n\n\nInstruction\n\n\n\nToggle on the cumulative area button and see what the epidemic final size is (approximately)\n\n\n\n3.2.4 At approximately what time does the epidemic end?\n\n\n\n\n\n\nSET\n\n\n\nDuration of infection = 8 days\nTransmission rate so you get the same \\(R_0\\) in Section 3.2.1\n\n\n\n3.2.5 How does the epidemic final size compare?\n\n3.2.6 At what time (approx) does the epidemic end?\n\n\n\n\n\n\nNote\n\n\n\nSize is determined by \\(R_0\\), duration is determined by recovery rate \\(\\left(\\gamma = \\frac{1}{\\text{duration of infection}}\\right)\\)\n\n\nNow, imagine that we have a drug (or vaccine) available to everyone that either reduced transmission OR shortened the duration of infection.\n\n\n\n\n\n\nSET\n\n\n\nTransmission rate = 1\nDuration of infection = 8 days\n\n\n\n3.2.7 Note the epidemic final size and the time until the epidemic is over.\nNow, imagine everyone has access to the drug (unrealistic) that reduces transmission by \\(P \\%\\)\n\n3.2.8 What happens to the final size and outbreak duration?\nNow, imagine everyone has access to a drug that reduces the duration of infection from 8 to 2 days (75% reduction).\n\n3.2.9 What happens to the final size and outbreak duration?\n\n3.2.10 Which assumption would you prefer and why?\n\n\n\n\n\n\nNote\n\n\n\nThis is the only really open ended question, but should be pretty straightforward discussion",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Session 01</span>"
    ]
  },
  {
    "objectID": "r-session-01.html#demographic-model-intuition",
    "href": "r-session-01.html#demographic-model-intuition",
    "title": "\n3  R Session 01\n",
    "section": "\n3.3 Demographic Model Intuition",
    "text": "3.3 Demographic Model Intuition\n\n\n\n\n\n\nSET\n\n\n\nTransmission = 1\nDuration = 8\nBirth rate = .002\n\n\n\n3.3.1 What is \\(R_0\\)?\n\n3.3.2 What is the equilibrium proportion that is susceptible?\n\n3.3.3 If you were to test for antibodies against infection in the population, what proportion would you expect to be positive?\n\n\n\n\n\n\nNote\n\n\n\nAssume a perfectly accurate serological test\n\n\n\n3.3.4 At what time (approximately) does the system reach equilibrium?\n\n\n\n\n\n\nSET\n\n\n\nBirth rate = 0.005\n\n\n\n3.3.5 What is the new \\(R_0\\)?\n\n3.3.6 At what time (approximately) does the system reach equilibrium?\n\n\n\n\n\n\nSET\n\n\n\nTransmission rate so you get the same \\(R_0\\) in Section 3.3.1\n\n\n\n3.3.7 What is the new equilibrium proportion that is susceptible?\n\n3.3.8 What is different about the prevalence of infection (equilibrium proportion that is infected) in the scenarios Section 3.3.1 and Section 3.3.5 i.e. higher birth rate with the same \\(R_0\\)?",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Session 01</span>"
    ]
  },
  {
    "objectID": "r-session-01.html#model-building-with-r",
    "href": "r-session-01.html#model-building-with-r",
    "title": "\n3  R Session 01\n",
    "section": "\n3.4 Model Building With R",
    "text": "3.4 Model Building With R\n\n3.4.1 Setting Up A Script\nNow we have some intuition behind how the different parameters affect the dynamics of the SIR system, let’s look at how we can implement this in R. Below is some R code that implements the basic closed-population SIR model. The purpose of the questions in this exercise is to guide you through the code and help you understand how it works so you can modify it to answer your own questions.\n\n\n\n\n\n\n\nInstruction\n\n\n\nCopy the code below into a new R script. You can open a new script in RStudio using ctrl+shift+N (Windows) or cmd+shift+N (macOS). Save it with the name S01_basic-sir.R. This script should live in your SISMID directory, as described previously. Run it to check you get the same figure output as above.\n\n\n\nCodelibrary(tidyverse)\nlibrary(deSolve)\n\ntheme_set(theme_minimal())\n\nsir_model &lt;- function(time, state, params, ...) {\n  transmission &lt;- params[\"transmission\"]\n  recovery &lt;- 1 / params[\"duration\"]\n\n  S &lt;- state[\"S\"]\n  I &lt;- state[\"I\"]\n  R &lt;- state[\"R\"]\n\n  dSdt &lt;- -transmission * S * I\n  dIdt &lt;- (transmission * S * I) - (recovery * I)\n  dRdt &lt;- recovery * I\n\n  return(list(c(dSdt, dIdt, dRdt)))\n}\n\nsir_params &lt;- c(transmission = 0.3, duration = 6)\nsir_init_states &lt;- c(S = 0.99, I = 0.01, R = 0)\nsim_times &lt;- seq(0, 200, by = 0.1)\n\nsir_sol &lt;- deSolve::ode(\n  y = sir_init_states,\n  times = sim_times,\n  func = sir_model,\n  parms = sir_params\n)\n\n# Turn the output from the ODE solver into a tibble (dataframe)\n# so we can manipulate and plot it easily\nsir_sol_df &lt;- as_tibble(sir_sol) %&gt;%\n  # Convert all columns to numeric (they are currently type\n  # deSolve so will produce warnings when plotting etc)\n  mutate(\n    # Rather than repeatedly type the same function for every\n    # column, use the across() function to apply the function\n    # to a selection of columns\n    across(\n      # The cols argument takes a selection of columns to apply\n      # a function to. Here, we want to apply the as.numeric()\n      # function to all columns, so we use the function\n      # everything() to select all columns.\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  ) %&gt;%\n  # Convert the dataframe from wide to long format, so we have a\n  # column for the time, a column for the state, and a column\n  # for the proportion of the population in that state at that\n  # time\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -time,\n    names_to = \"state\",\n    values_to = \"proportion\"\n  ) %&gt;%\n  # Update the state column to be a factor, so the plot will\n  # show the states in the correct order\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\")))\n\nSIRcolors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\")\n\nggplot(sir_sol_df, aes(x = time, y = proportion, color = state)) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(values = SIRcolors) +\n  labs(\n    x = \"Time\",\n    y = \"Fraction\",\n    color = \"State\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n3.4.2 Commenting the code\nFor this part of the exercise, go through the basic SIR code and add comments to each section of code explaining what it does. To get you started, we’ve added some comments to the creating of the dataframe object sir_sol_df between lines 32-62 in the code block above, as some of the functions used there are a bit more complicated.\n\n\n\n\n\n\nNote\n\n\n\nNormally you would not use nearly as extensive comments. Here, we’ve gone overboard to help you understand what each line does, as some may not be familiar with all the functions used. We’ve also broken up the comments into multiple lines so that it is easier to read on this website. For your code that you view in RStudio (or some other text editor), use one line per sentence of the comment i.e., start a new comment line after each period.\nGenerally, you want to use comments to explain why you are doing something, not what you are doing. Sometimes that is unavoidable (e.g., you had to look up how to do a particular thing in R and need the hints to be able to understand the code), but try to stick to this guideline where possible.\n\n\nAs you’re going through the code, if you don’t understand what a particular function does, try looking up the documentation for it! You can do this by clicking on the function within the website (as described in the intro), or by typing ?function_name into the R console (Google also is your friend here!).\n\n3.4.3 Adding in demographics\nNow we have a better sense of how the code works, let’s add in some demographic structure. To recreate the demographic model from the interactive plot in Section 3.1, we just need to add births and deaths to the system.\nRecall the equations for the demographic model:\n\\[\n\\begin{aligned}\n\\frac{dS}{dt} &= \\mu N - \\beta S I - \\mu S \\\\\n\\frac{dI}{dt} &= \\beta S I - \\gamma I - \\mu I \\\\\n\\frac{dR}{dt} &= \\gamma I - \\mu R\n\\end{aligned}\n\\tag{3.1}\\]\n\n3.4.3.1 Create a new R script called S01_demographic-sir.R and copy the code from S01_basic-sir.R into it.\n\n3.4.3.2 Rename the function sir_model() to demographic_sir_model() in your new script (S01_demographic-sir.R).\n\n3.4.3.3 Adapt the function demographic_sir_model() to match the above equations (Equation 3.1).\n\n\n\n\n\n\nSET\n\n\n\nbirth rate = 0.05\n\n\n\n3.4.3.4 Rename the variables to reflect that we are now working with a demographic model, not the basic SIR model.\n\n3.4.3.5 Run the code in the S01_demographic-sir.R file. Plot the results of your demographic model. Does it look like this?\n\n\n\n\n\n\n\n\n\n3.4.3.6 Update the comments in your code to reflect the changes you have made.",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Session 01</span>"
    ]
  },
  {
    "objectID": "r-session-01.html#exercise-solutions",
    "href": "r-session-01.html#exercise-solutions",
    "title": "\n3  R Session 01\n",
    "section": "\n3.5 Exercise Solutions",
    "text": "3.5 Exercise Solutions\n\n\n\n\n\n\nNote\n\n\n\nHere, we’re using the roxygen2 package to create the comments for the functions we’ve created, i.e., sir_model &lt;- function(...). This provides a consistent framework for commenting functions, and if we wanted, we could use the comments to create documentation for our functions. The main benefit for our purposes is that the framework allows us to quickly understand exactly what the function does, as well as the context it should be used in.\n\n\n\n3.5.1 Section 3.4.2: Commented basic SIR code\n\nCode# Load packages\nlibrary(tidyverse)\nlibrary(deSolve)\n\n# Set the ggplot2 theme\ntheme_set(theme_minimal())\n\n#' Basic SIR model\n#'\n#' A basic SIR model with no demographic structure to be used in deSolve\n#'\n#' @param time deSolve passes the time parameter to the function.\n#' @param state A vector of states.\n#' @param params A vector of parameter values .\n#' @param ... Other arguments passed by deSolve.\n#'\n#' @return A deSolve matrix of states at each time step.\n#' @examples\n#' sir_params &lt;- c(transmission = 0.3, duration = 6)\n#' sir_init_states &lt;- c(S = 0.99, I = 0.01, R = 0)\n#' sim_times &lt;- seq(0, 200, by = 0.1)\n#'\n#' sir_sol &lt;- deSolve::ode(\n#'    y = sir_init_states,\n#'    times = sim_times,\n#'    func = sir_model,\n#'    parms = sir_params\n#' ))\nsir_model &lt;- function(time, state, params, ...) {\n  # Extract parameters for cleaner calculations\n  transmission &lt;- params[\"transmission\"]\n  recovery &lt;- 1 / params[\"duration\"]\n\n  # Extract states for cleaner calculations\n  S &lt;- state[\"S\"]\n  I &lt;- state[\"I\"]\n  R &lt;- state[\"R\"]\n\n  # Differential equations of the SIR model\n  dSdt &lt;- -transmission * S * I\n  dIdt &lt;- (transmission * S * I) - (recovery * I)\n  dRdt &lt;- recovery * I\n\n  # Return a list whose first element is a vector of the\n  # state derivatives - must be in the same order as the\n  # state vector (S, I, R)\n  return(list(c(dSdt, dIdt, dRdt)))\n}\n\n# Create the parameter, initial state, and time vectors\nsir_params &lt;- c(transmission = 0.3, duration = 6)\nsir_init_states &lt;- c(S = 0.99, I = 0.01, R = 0)\nsim_times &lt;- seq(0, 200, by = 0.1)\n\n# Solve the SIR model with deSolve's ode() function\nsir_sol &lt;- deSolve::ode(\n  y = sir_init_states,\n  times = sim_times,\n  func = sir_model,\n  parms = sir_params\n)\n\n# Turn the output from the ODE solver into a tibble (dataframe)\n# so we can manipulate and plot it easily\nsir_sol_df &lt;- as_tibble(sir_sol) %&gt;%\n  # Convert all columns to numeric (they are currently type\n  # deSolve so will produce warnings when plotting etc)\n  mutate(\n    # Rather than repeatedly type the same function for every\n    # column, use the across() function to apply the function\n    # to a selection of columns\n    across(\n      # The cols argument takes a selection of columns to apply\n      # a function to. Here, we want to apply the as.numeric()\n      # function to all columns, so we use the function\n      # everything() to select all columns.\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  ) %&gt;%\n  # Convert the dataframe from wide to long format, so we have a\n  # column for the time, a column for the state, and a column\n  # for the proportion of the population in that state at that\n  # time\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -time,\n    names_to = \"state\",\n    values_to = \"proportion\"\n  ) %&gt;%\n  # Update the state column to be a factor, so the plot will\n  # show the states in the correct order\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\")))\n\n# Save the colors to a vector\nSIRcolors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\")\n\n# Plot the results\nggplot(sir_sol_df, aes(x = time, y = proportion, color = state)) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(values = SIRcolors) +\n  labs(\n    x = \"Time\",\n    y = \"Fraction\",\n    color = \"State\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n3.5.2 Section 3.4.3: Commented demographic SIR code\n\nCode# Load packages\nlibrary(tidyverse)\nlibrary(deSolve)\n\n# Set the ggplot2 theme\ntheme_set(theme_minimal())\n\n#' Demographic SIR model\n#'\n#' An SIR model with births and deaths (constant pop) to be used in deSolve\n#'\n#' @param time deSolve passes the time parameter to the function.\n#' @param state A vector of states.\n#' @param params A vector of parameter values .\n#' @param ... Other arguments passed by deSolve.\n#'\n#' @return A deSolve matrix of states at each time step.\n#' @examples\n#' sir_params &lt;- c(transmission = 0.3, duration = 6, birth_rate = 0.05)\n#' sir_init_states &lt;- c(S = 0.99, I = 0.01, R = 0)\n#' sim_times &lt;- seq(0, 200, by = 0.1)\n#'\n#' sir_sol &lt;- deSolve::ode(\n#'    y = sir_init_states,\n#'    times = sim_times,\n#'    func = sir_demog_model,\n#'    parms = sir_params\n#' ))\nsir_demog_model &lt;- function(time, state, params, ...) {\n  transmission &lt;- params[\"transmission\"]\n  recovery &lt;- 1 / params[\"duration\"]\n  birth_rate &lt;- params[\"birth_rate\"]\n\n  S &lt;- state[\"S\"]\n  I &lt;- state[\"I\"]\n  R &lt;- state[\"R\"]\n\n  dSdt &lt;- birth_rate - transmission * S * I - (birth_rate * S)\n  dIdt &lt;- (transmission * S * I) - (recovery * I) - (birth_rate * I)\n  dRdt &lt;- (recovery * I) - (birth_rate * R)\n\n  return(list(c(dSdt, dIdt, dRdt)))\n}\n\n# Create the parameter, initial state, and time vector\nsir_demog_params &lt;- c(transmission = 0.3, duration = 6, birth_rate = 0.05)\nsir_init_states &lt;- c(S = 0.99, I = 0.01, R = 0)\nsim_times &lt;- seq(0, 200, by = 0.1)\n\n# Solve the SIR model with deSolve's ode() function\nsir_demog_sol &lt;- deSolve::ode(\n  y = sir_init_states,\n  times = sim_times,\n  func = sir_demog_model,\n  parms = sir_demog_params\n)\n\n# Turn the output from the ODE solver into a tibble (dataframe)\n# so we can manipulate and plot it easily\nsir_demog_sol_df &lt;- as_tibble(sir_demog_sol) %&gt;%\n  # Convert all columns to numeric (they are currently type\n  # deSolve so will produce warnings when plotting etc)\n  mutate(\n    # Rather than repeatedly type the same function for every\n    # column, use the across() function to apply the function\n    # to a selection of columns\n    across(\n      # The cols argument takes a selection of columns to apply\n      # a function to. Here, we want to apply the as.numeric()\n      # function to all columns, so we use the function\n      # everything() to select all columns.\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  ) %&gt;%\n  # Convert the dataframe from wide to long format, so we have a\n  # column for the time, a column for the state, and a column\n  # for the proportion of the population in that state at that\n  # time\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -time,\n    names_to = \"state\",\n    values_to = \"proportion\"\n  ) %&gt;%\n  # Update the state column to be a factor, so the plot will\n  # show the states in the correct order\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\")))\n\n# Save the colors to a vector\nSIRcolors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\")\n\n# Plot the results\nggplot(sir_demog_sol_df, aes(x = time, y = proportion, color = state)) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(values = SIRcolors) +\n  labs(\n    x = \"Time\",\n    y = \"Fraction\",\n    color = \"State\"\n  ) +\n  theme(legend.position = \"top\")",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Session 01</span>"
    ]
  },
  {
    "objectID": "L03_hit-and-vaccinations.html",
    "href": "L03_hit-and-vaccinations.html",
    "title": "4  Herd Immunity & Vaccination",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Herd Immunity & Vaccination</span>"
    ]
  },
  {
    "objectID": "L05_age-structure.html",
    "href": "L05_age-structure.html",
    "title": "5  Heterogeneity and Age Structure",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Heterogeneity and Age Structure</span>"
    ]
  },
  {
    "objectID": "r-session-02.html",
    "href": "r-session-02.html",
    "title": "\n6  R Session 02\n",
    "section": "",
    "text": "6.1 Load Packages\nCodelibrary(diagram)\nlibrary(deSolve)\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(rio)\nCodetheme_set(theme_minimal())",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Session 02</span>"
    ]
  },
  {
    "objectID": "r-session-02.html#a-model-with-2-classes",
    "href": "r-session-02.html#a-model-with-2-classes",
    "title": "\n6  R Session 02\n",
    "section": "\n6.2 A Model With 2 Classes",
    "text": "6.2 A Model With 2 Classes\nWe’ll start with the simplest mechanistic model of two classes we can think of, which has separate classes for two groups \\(a\\) and \\(b\\). These groups could represent different socioeconomic classes, for example.\n\n\n\n\n\n\n\n\nWhich can be written in equations as, \\[\n\\begin{aligned}\n    \\frac{\\dd{S_a}}{\\dd{t}} &= -\\lambda_a\\,S_a \\phantom{-\\gamma\\,I_b}\\\\\n    \\frac{\\dd{S_b}}{\\dd{t}} &= -\\lambda_b\\,S_b \\phantom{-\\gamma\\,I_b}\\\\\n    \\frac{\\dd{I_a}}{\\dd{t}} &= \\phantom{-}\\lambda_a\\,S_a -\\gamma\\,I_a\\\\\n    \\frac{\\dd{I_b}}{\\dd{t}} &= \\phantom{-}\\lambda_b\\,S_b-\\gamma\\,I_b\\\\\n    \\frac{\\dd{R_a}}{\\dd{t}} &= \\phantom{-\\lambda_a\\,S_b}+\\gamma\\,I_a\\\\\n    \\frac{\\dd{R_b}}{\\dd{t}} &= \\phantom{-\\lambda_a\\,S_b}+\\gamma\\,I_b\\\\\n  \\end{aligned}\n\\]\nThe \\(\\lambda\\)s denote the group-specific force of infections:\n\\[\n\\begin{aligned}\n        \\lambda_a &= \\beta_{aa}\\,I_a+\\beta_{ab}\\,I_b\\\\\n        \\lambda_b &= \\beta_{ba}\\,I_a+\\beta_{bb}\\,I_b\n\\end{aligned}\n\\]\nIn this model, each population can infect each other but the infection moves through the populations separately. Let’s simulate such a model. To make things concrete, we’ll assume that the transmission rates \\(\\beta\\) are greater within groups than between them.\n\nCode# Create a named parameter vector that we can index by name in the model\nab_params &lt;- c(\n  beta_within = 0.025,\n  beta_between = 0.005,\n  recovery = 10\n)\n\n\n\nCode# Here we set up the ODE model that matches the equations above\nab_model &lt;- function(t, x, p, ...) {\n  # Unpack the state variables\n  Sa &lt;- x[\"Sa\"]\n  Sb &lt;- x[\"Sb\"]\n  Ia &lt;- x[\"Ia\"]\n  Ib &lt;- x[\"Ib\"]\n\n  # Unpack the parameters\n  beta_within &lt;- p[\"beta_within\"]\n  beta_between &lt;- p[\"beta_between\"]\n  recovery &lt;- p[\"recovery\"]\n\n  # group A force of infection\n  lambda_a &lt;- beta_within * Ia + beta_between * Ib\n\n  # group B force of infection\n  lambda_b &lt;- beta_within * Ib + beta_between * Ia\n\n  # The ODEs\n  dSadt &lt;- -lambda_a * Sa\n  dSbdt &lt;- -lambda_b * Sb\n  dIadt &lt;- lambda_a * Sa - recovery * Ia\n  dIbdt &lt;- lambda_b * Sb - recovery * Ib\n  dRadt &lt;- recovery * Ia\n  dRbdt &lt;- recovery * Ib\n\n  # Return the derivatives\n  list(c(\n    dSadt,\n    dSbdt,\n    dIadt,\n    dIbdt,\n    dRadt,\n    dRbdt\n  ))\n}\n\n\n\nCode# initial conditions\nab_yinit &lt;- c(Sa = 1000, Sb = 2000, Ia = 1, Ib = 1, Ra = 0, Rb = 0)\n\n# Run the ODE solver from the deSolve package\nab_sol &lt;- deSolve::ode(\n  y = ab_yinit,\n  times = seq(0, 2, by = 0.001),\n  func = ab_model,\n  parms = ab_params,\n)\n\n\n\nCodeab_df &lt;- ab_sol %&gt;%\n  # Convert the solution to a tibble for manipulation\n  as_tibble() %&gt;%\n  # Create and modify columns\n  mutate(\n    # Convert all columns into type numeric\n    across(everything(), as.numeric),\n    # Create new columns to track pop sizes in each group\n    Na = Sa + Ia + Ra,\n    Nb = Sb + Ib + Rb\n  ) %&gt;%\n  # Go from a wide to long dataframe for ggplot\n  pivot_longer(\n    cols = -time,\n    names_to = c(\"state\", \"group\"),\n    names_sep = 1,\n    values_to = \"value\"\n  ) %&gt;%\n  # Clean pivoted columns for ordered plots\n  mutate(\n    state = factor(state, levels = c(\"S\", \"I\", \"R\", \"N\")),\n    group = paste(\"Group\", str_to_upper(group))\n  )\n\n\n\nCode# Create a vector of colors to be used throughout the ggplots\nSIRcolors &lt;- c(\"#1f77b4\", \"#ff7f0e\", \"#FF3851\", \"#591099\")\n\nggplot(ab_df, aes(x = time, y = value, color = state)) +\n  geom_line(linewidth = 1.5) +\n  facet_wrap(~group, scales = \"free_y\") +\n  scale_color_manual(\n    values = SIRcolors,\n    labels = c(\"Susceptible\", \"Infected\", \"Recovered\", \"Total\")\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"Number of individuals\",\n    color = \"State\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nDespite using the same transmission rates, the epidemic in group B is much larger than in group A. Why do you think this is?\n\nNow let’s plot the proportion of individuals in each state for the two groups.\n\nCodeab_df_props &lt;- ab_df %&gt;%\n  # Remove total pop count as we only want the group-specific values\n  filter(state != \"N\") %&gt;%\n  mutate(\n    # Concatenate the state variable and the group letter for each row\n    state_group = paste0(state, str_extract_all(group, \"[^Group ]\")),\n    # Factor new variable for nicer plotting\n    state_group = factor(\n      state_group,\n      levels = c(\"RA\", \"RB\", \"IA\", \"IB\", \"SA\", \"SB\")\n    )\n  ) %&gt;%\n  # Group by time and state_group so we can calculate the relevant\n  # proportions over time\n  group_by(time, state_group) %&gt;%\n  mutate(\n    prop = value / sum(ab_yinit)\n  ) %&gt;%\n  ungroup()\n\n\n\nCode# Create new vectors of colors as using 6: one of each for A and J groups\nScolors &lt;- RColorBrewer::brewer.pal(3, \"Blues\")[c(2, 3)]\nIcolors &lt;- RColorBrewer::brewer.pal(3, \"Oranges\")[c(2, 3)]\nRcolors &lt;- RColorBrewer::brewer.pal(3, \"Greens\")[c(2, 3)]\n\nggplot(ab_df_props, aes(x = time, y = prop, fill = state_group)) +\n  geom_area() +\n  scale_fill_manual(\n    values = c(Scolors, Icolors, Rcolors),\n    limits = c(\"SA\", \"SB\", \"IA\", \"IB\", \"RA\", \"RB\"),\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"Proportion of individuals\",\n    fill = \"State\"\n  ) +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Session 02</span>"
    ]
  },
  {
    "objectID": "r-session-02.html#a-model-with-2-age-classes",
    "href": "r-session-02.html#a-model-with-2-age-classes",
    "title": "\n6  R Session 02\n",
    "section": "\n6.3 A Model With 2 Age Classes",
    "text": "6.3 A Model With 2 Age Classes\nNote that age is a special kind of heterogeneity in an epidemic model because individuals necessarily move from one class (younger) to another class (older) in a directional fashion that is independent of the infection and recovery process.\nWe’ll start by introducing age into the model above. So now \\(a\\) becomes juveniles and \\(b\\) becomes adults. And, independent of the disease process, juveniles (of any category) age into adults. Additionally, new juveniles are added through births (always first susceptible) and old individuals are lost to death.\n\n\n\n\n\n\n\n\nWe can do this very simply using the same ingredients that go into the basic SIR model. In that model, the waiting times in the S and I classes are exponential. Let’s assume the same thing about the aging process. We’ll also add in births into the juvenile susceptible class and deaths from the adult classes.\n\\[\n  \\begin{aligned}\n    \\frac{\\dd{S_J}}{\\dd{t}} &= B -\\lambda_J\\,S_J \\phantom{- \\gamma\\,I_A} -\\alpha\\,S_J \\phantom{-\\mu\\,S_A}\\\\\n    \\frac{\\dd{S_A}}{\\dd{t}} &= \\phantom{B} - \\lambda_A\\,S_A \\phantom{- \\gamma\\,I_A} +\\alpha\\,S_J -\\mu\\,S_A\\\\\n    \\frac{\\dd{I_J}}{\\dd{t}} &= \\phantom{B} +\\lambda_J\\,S_J - \\gamma\\,I_J -\\alpha\\,I_J \\phantom{-\\mu\\,S_A}\\\\\n    \\frac{\\dd{I_A}}{\\dd{t}} &= \\phantom{B} +\\lambda_A\\,S_A - \\gamma\\,I_A + \\alpha\\,I_J - \\mu\\,I_A\\\\\n    \\frac{\\dd{R_J}}{\\dd{t}} &= \\phantom{B - \\lambda_J\\,S_A} + \\gamma\\,I_J - \\alpha\\,R_J \\phantom{- \\mu\\,S_A}\\\\\n    \\frac{\\dd{R_A}}{\\dd{t}} &= \\phantom{B - \\lambda_J\\,S_A} + \\gamma\\,I_A + \\alpha\\,R_J -\\mu\\,R_A\\\\\n  \\end{aligned}\n\\]\nNow, let’s simulate this model, under the same assumptions about transmission rates as above.\n\nCode# define the parameters for the demographic model\ndemog_params &lt;- c(\n  beta_within = 0.004,\n  beta_between = 0.002,\n  recovery = 10,\n  births = 100,\n  # Width of age bands in years\n  age_band_j = 20,\n  age_band_a = 60\n)\n\n\n\nCodedemog_model &lt;- function(t, x, p, ...) {\n  # Unpack states\n  Sj &lt;- x[\"Sj\"]\n  Sa &lt;- x[\"Sa\"]\n  Ij &lt;- x[\"Ij\"]\n  Ia &lt;- x[\"Ia\"]\n  Rj &lt;- x[\"Rj\"]\n  Ra &lt;- x[\"Ra\"]\n\n  # Unpack parameters from vector\n  beta_within &lt;- p[\"beta_within\"]\n  beta_between &lt;- p[\"beta_between\"]\n  recovery &lt;- p[\"recovery\"]\n  births &lt;- p[\"births\"]\n  # Calculate rate of aging from each age group\n  aging_j &lt;- 1 / p[\"age_band_j\"]\n  aging_a &lt;- 1 / p[\"age_band_a\"]\n\n  # juv. force of infection\n  lambda_j &lt;- beta_within * Ij + beta_between * Ia\n\n  # adult. force of infection\n  lambda_a &lt;- beta_within * Ia + beta_between * Ij\n\n  # Calculate the ODEs\n  dSjdt &lt;- births - (lambda_j * Sj) - (aging_j * Sj)\n  dSadt &lt;- -(lambda_a * Sa) + (aging_j * Sj) - (aging_a * Sa)\n  dIjdt &lt;- (lambda_j * Sj) - (recovery * Ij) - (aging_j * Ij)\n  dIadt &lt;- (lambda_a * Sa) - (recovery * Ia) + (aging_j * Ij) - (aging_a * Ia)\n  dRjdt &lt;- (recovery * Ij) - (aging_j * Rj)\n  dRadt &lt;- (recovery * Ia) + (aging_j * Rj) - (aging_a * Ra)\n\n  # Return the ODEs\n  list(c(\n    dSjdt,\n    dSadt,\n    dIjdt,\n    dIadt,\n    dRjdt,\n    dRadt\n  ))\n}\n\n\nNote that in this function, \\(\\mu=\\) aging_a \\(=\\) 1 / p[\"age_band_a\"], i.e., death, is just like another age class.\n\nCode# initial conditions\ndemog_yinit &lt;- c(Sj = 2000, Sa = 3000, Ij = 0, Ia = 1, Rj = 0, Ra = 1000)\n\n# Solve the demographic model\ndemog_sol &lt;- deSolve::ode(\n  y = demog_yinit,\n  times = seq(0, 200, by = 0.1),\n  func = demog_model,\n  parms = demog_params\n)\n\ndemog_df &lt;- demog_sol %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    across(everything(), as.numeric),\n    Nj = Sj + Ij + Rj,\n    Na = Sa + Ia + Ra,\n    # Calculate total population as need for proportional area plots\n    N = Nj + Na\n  ) %&gt;%\n  pivot_longer(\n    cols = -c(time, N),\n    names_to = c(\"state\", \"group\"),\n    names_sep = 1,\n    values_to = \"value\"\n  ) %&gt;%\n  mutate(\n    state = factor(state, levels = c(\"S\", \"I\", \"R\", \"N\")),\n    group = paste(\"Group\", str_to_upper(group))\n  )\n\n\n\n6.3.1 Exercise 1: Use this code to plot the number of susceptible, infected, and recovered individuals over time\n\nCodeggplot(demog_df, aes(x = time, y = value, color = state)) +\n  geom_line(linewidth = 1.5) +\n  facet_wrap(\n    ~group,\n    nrow = 2,\n    scales = \"free_y\",\n    labeller = as_labeller(c(\n      `Group A` = \"Adults\",\n      `Group J` = \"Juveniles\"\n    ))\n  ) +\n  scale_color_manual(\n    values = SIRcolors,\n    labels = c(\"Susceptible\", \"Infected\", \"Recovered\", \"Total\")\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"Number of individuals\",\n    color = \"State\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nNote that now that births are replenishing susceptibles, infection persists. The results of the above are plotted here:\nNow let’s plot the proportion of individuals in each state for the two groups.\n\nCode# Calculate the proportions in each state and group at each time point\ndemog_df_props &lt;- demog_df %&gt;%\n  filter(state != \"N\") %&gt;%\n  mutate(\n    state_group = paste0(state, str_extract_all(group, \"[^Group ]\")),\n    state_group = factor(\n      state_group,\n      levels = c(\"RJ\", \"RA\", \"IJ\", \"IA\", \"SJ\", \"SA\")\n    )\n  ) %&gt;%\n  group_by(time, state_group) %&gt;%\n  mutate(\n    # Calculate the proportion of the total population, not the group pop\n    prop = value / N\n  ) %&gt;%\n  ungroup()\n\n\n\nCodeggplot(demog_df_props, aes(x = time, y = prop, fill = state_group)) +\n  geom_area() +\n  scale_fill_manual(\n    values = c(Scolors, Icolors, Rcolors),\n    limits = c(\"SJ\", \"SA\", \"IJ\", \"IA\", \"RJ\", \"RA\")\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"Proportion of individuals\",\n    fill = \"State\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nNow let’s plot the equilibrium seroprevalence for each age group.\n\nCode# Select the last row (time point) of the data frame\ndemog_equil_seroprev &lt;- tail(demog_df) %&gt;%\n  mutate(\n    # Calculate the proportion of individuals in each state and age group\n    prop = value / sum(value),\n    # Relabel groups for plots\n    group = case_when(group == \"Group J\" ~ \"Juveniles\", TRUE ~ \"Adults\"),\n    group = factor(group, levels = c(\"Juveniles\", \"Adults\")),\n    .by = group\n  ) %&gt;%\n  filter(state == \"R\")\n\n\n\nCode# Create vector of colors to distinguish between age groups\nage_group_colors &lt;- c(\"#2980B9\", \"#154360\")\n\nggplot(demog_equil_seroprev, aes(x = group, y = prop, fill = group)) +\n  geom_col(position = \"identity\") +\n  scale_fill_manual(\n    values = age_group_colors\n  ) +\n  labs(\n    x = \"Age group\",\n    y = \"Equilibrium seroprevalence\",\n    fill = \"Age group\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nOne thing we are often interested in is the \\(R_0\\) of a system. The details are beyond the scope of this workshop and are not required to complete the exercises in this worksheet, but we have outlined them in Section 6.6.1.1, particularly in Equation 6.3, at the end of this page.\nIn our system, \\(R_0 =\\) 2.66.",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Session 02</span>"
    ]
  },
  {
    "objectID": "r-session-02.html#getting-more-realistic-adding-more-age-classes",
    "href": "r-session-02.html#getting-more-realistic-adding-more-age-classes",
    "title": "\n6  R Session 02\n",
    "section": "\n6.4 Getting more realistic: adding more age classes",
    "text": "6.4 Getting more realistic: adding more age classes\nIn the models above, the aging process follows an exponential distribution, which means that whether an individual is 1~year old or 10 years old, the chance of them becoming an adult is the same! To improve on this, we can assume that the time a juvenile must wait before becoming an adult follows a gamma distribution. This is equivalent to saying that the waiting time is a sum of some number of exponential distributions. This suggests that we can achieve such a distribution by adding age classes to the model, so that becoming an adult means passing through some number of stages. We’ll use 30 age classes, and since they don’t have to be of equal duration, we’ll assume that they’re not. Specifically, we’ll have 20 1-yr age classes to take us up to adulthood and break adults into 10 age classes of 5~yr duration each. The last age class covers age 66-80.\nNow, when we had just two age classes, we could write out each of the equations easily enough, but now that we’re going to have 30, we’ll need to be more systematic. In particular, we’ll need to think of \\(\\beta\\) as a matrix of transmission rates. Let’s see how to define such a matrix in R. So that we don’t change too many things all at once, let’s keep the same contact structure as in the juvenile-adult model.\n\nCode# Set up the parameters for model that incorporates a more realistic age matrix\nages_params &lt;- c(\n  beta_j = 0.02,\n  beta_a = 0.01,\n  beta_aj = 0.01 / 2,\n  recovery = 10,\n  births = 100\n)\n\n# Create a vector of ages\nages &lt;- c(seq(1, 20, by = 1), seq(25, 65, by = 5), 80)\n\n# Calculate the widths of the age bands\nda_ages &lt;- diff(c(0, ages))\n\n# set up a matrix of contact rates between classes: more contact\n# within juveniles and adults than between\nages_beta_mat &lt;- matrix(nrow = 30, ncol = 30)\n\n# transmission rate for juveniles\nages_beta_mat[1:20, 1:20] &lt;- ages_params[\"beta_j\"]\n\n# transmission rate for adults\nages_beta_mat[21:30, 21:30] &lt;- ages_params[\"beta_a\"]\n\n# lower transmission rate between juveniles and adults\nages_beta_mat[1:20, 21:30] &lt;- ages_params[\"beta_aj\"]\n\n# lower transmission rate between juveniles and adults\nages_beta_mat[21:30, 1:20] &lt;- ages_params[\"beta_aj\"]\n\n\n\n\nCode# Convert matrix to data frame\ntile_df &lt;- expand.grid(x = 1:30, y = 1:30)\ntile_df$value &lt;- as.vector(ages_beta_mat)\n\n# Convert to factor to treat as discrete categories and define colors\ntile_df$value &lt;- factor(tile_df$value)\nbeta_colors &lt;- c(\"0.005\" = \"#fcae91\", \"0.01\" = \"#de2d26\", \"0.02\" = \"#a50f15\")\n\n# Create tile plot with 3 betas\nggplot(tile_df, aes(x = x, y = y, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_manual(values = beta_colors, name = expression(beta)) +\n  labs(x = \"Age of Contactor\", y = \"Age of Contactee\") +\n  scale_x_continuous(breaks = 1:30, labels = as.character(ages)) +\n  scale_y_continuous(breaks = 1:30, labels = as.character(ages))\n\n\n\n\n\n\n\nWe’ll assume that, at the time of introduction, all children are susceptible, as are adults over 45, but that individuals aged 20–45 have seen the pathogen before and are immune. The vector yinit expresses these initial conditions.\n\nCode# Create a long vector of initial states with only one\n# initial infection in age 50\ndemog_yinit_ages &lt;- c(\n  S = c(rep(100, 20), rep(0, 5), rep(200, 5)),\n  I = c(rep(0, 25), 1, rep(0, 4)),\n  R = c(rep(0, 20), rep(1000, 5), rep(0, 5))\n)\n\n\nNote that we’re starting out with 1 infected individual in the 26th age class (age 50).\nThe codes that follow will be a bit easier to follow if we introduce some indexes that will allow us to pick out certain bits of the yinit vector.\n\nCode# Create vectors of indices relating to each state\n# Note that there are are 30 age classes, 1-20 are 1 year wide, 21-30 are 5 years wide, taking us up to age\n# (ordered S1-80, I1-80, R1-80)\nsindex &lt;- 1:30\niindex &lt;- 31:60\nrindex &lt;- 61:90\n# Create vectors of indices relating to age group\njuvies &lt;- 1:20\nadults &lt;- 21:30\n\n\nNow, to capture the aging process, it’s convenient to define another matrix to hold the rates of movement between age classes. Generally, this matrix would look like this:\n\\[\n\\begin{pmatrix}\n    -\\alpha_1 & 0 & 0 & \\cdots & 0\\\\\n    \\alpha_1 & -\\alpha_2 & 0 & \\cdots & 0\\\\\n    0 & \\alpha_2 & -\\alpha_3 & \\cdots & 0\\\\\n    \\vdots &  & \\ddots & \\ddots & \\vdots \\\\\n    0 & \\cdots & & \\alpha_{29} & -\\alpha_{30}\\\\\n\\end{pmatrix}\n\\tag{6.1}\\]\n\nCode# Create a diagonal matrix that holds the rates of aging out of each age class\n# The rows represent the age class you're in, the columns represent the age\n# class you're moving to\naging_mat &lt;- diag(-1 / da_ages)\n\n# Fill in the rates of aging into each age class\naging_mat[row(aging_mat) - col(aging_mat) == 1] &lt;- 1 / head(da_ages, -1)\n\n\nHave a look at the aging matrix, for example by doing:\n\nCode# Move fast through the 1-year age classes - negatives are moves out, positives\n# are moves in. Cannot move between non-adjacent age classes\naging_mat[1:5, 1:5]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   -1    0    0    0    0\n[2,]    1   -1    0    0    0\n[3,]    0    1   -1    0    0\n[4,]    0    0    1   -1    0\n[5,]    0    0    0    1   -1\n\n\n\nCode# Move slowly between the wider age classes\naging_mat[25:30, 25:30]\n\n     [,1] [,2] [,3] [,4] [,5]        [,6]\n[1,] -0.2  0.0  0.0  0.0  0.0  0.00000000\n[2,]  0.2 -0.2  0.0  0.0  0.0  0.00000000\n[3,]  0.0  0.2 -0.2  0.0  0.0  0.00000000\n[4,]  0.0  0.0  0.2 -0.2  0.0  0.00000000\n[5,]  0.0  0.0  0.0  0.2 -0.2  0.00000000\n[6,]  0.0  0.0  0.0  0.0  0.2 -0.06666667\n\n\n\nCodeaging_mat %&gt;%\n  as.data.frame.table() %&gt;%\n  mutate(\n    age_recipient = rep(ages, 30),\n    # Repeat each age in ages vector 30 times before moving to next\n    age_source = rep(ages, each = 30)\n  ) %&gt;%\n  ggplot(aes(\n    x = as.factor(age_source),\n    y = as.factor(age_recipient),\n    z = Freq\n  )) +\n  geom_tile(colour = \"grey\", size = 0.4, aes(fill = Freq)) +\nscale_fill_gradientn(\n    colours = c(\"red\", \"white\", \"blue\"),\n    breaks = c(-1, -0.2, 0, 0.2, 1),\n    labels = c(\"-1\", \"-0.2\", \"0\", \"0.2\", \"1\")\n  ) +\n  labs(x = \"Source Age Group\", y = \"Recipient Age Group\")\n\n\n\n\n\n\n\n\n6.4.1 Exercise 2: What can you say about its structure? How are the different age groups in contact with each other?\nNow we can put the pieces together to write a simulator for the age-structured SIR dynamics.\n\nCode# Using a list instead of a vector to hold the parameters, as ages_beta_mat and\n# aging are both matrices, so we want to keep them as matrices, rather than\n# flattening\nmultistage_params &lt;- list(\n  beta_mat = ages_beta_mat,\n  recovery = ages_params[\"recovery\"],\n  births = ages_params[\"births\"],\n  aging_mat = aging_mat\n)\n\nmultistage_model &lt;- function(t, x, p, ...) {\n  # Unpack all states from the vector using the relevant indices\n  s &lt;- x[sindex]\n  i &lt;- x[iindex]\n  r &lt;- x[rindex]\n\n  # Unpack parameters\n  beta_mat &lt;- p[[\"beta_mat\"]]\n  recovery &lt;- p[[\"recovery\"]]\n  births &lt;- p[[\"births\"]]\n  aging_mat &lt;- p[[\"aging_mat\"]]\n\n  # Calculate force of infection using matrix multiplication\n  lambda &lt;- beta_mat %*% i\n\n  # Calculate the ODEs at every time step\n  # Note that R add element-wise for vectors i.e. lambda * s results\n  # in a vector length 30 (30 age groups), as does aging_mat %*% s,\n  # so v1[i] + v2[i] for i in 1:30\n  dsdt &lt;- -lambda * s + aging_mat %*% s\n  didt &lt;- lambda * s + aging_mat %*% i - recovery * i\n  drdt &lt;- aging_mat %*% r + recovery * i\n  # Add the birth rate to the first age group\n  dsdt[1] &lt;- dsdt[1] + births\n\n  # Return the ODEs in a list\n  list(c(dsdt, didt, drdt))\n}\n\n\nWe can plug this into ode just as we did the simpler models to simulate an epidemic. We’ll then plot the epidemic curve.\n\nCode# Solve the model with a realistic age matrix\nmultistage_sol &lt;- deSolve::ode(\n  y = demog_yinit_ages,\n  times = seq(0, 100, by = 0.1),\n  func = multistage_model,\n  parms = multistage_params\n)\n\n# Extract all infected age groups at all time points into a new vector\nmultistage_infecteds &lt;- multistage_sol[, 1 + iindex]\n\n\n\nCode# Create a dataframe of the sum of infectious individuals in Juv/Adult age groups\n# at each time point\nmultistage_df &lt;- tibble(\n  # Get all times from model run\n  time = multistage_sol[, 1],\n  # At each timepoint, apply the sum function to all juvenile infected\n  # individuals\n  Juveniles = apply(multistage_infecteds[, juvies], 1, sum),\n  # At each timepoint, apply the sum function to all adult infected\n  # individuals\n  Adults = apply(multistage_infecteds[, adults], 1, sum)\n) %&gt;%\n  # Pivot to create a long dataframe that works with ggplot\n  pivot_longer(\n    cols = c(Juveniles, Adults),\n    names_to = \"age_group\",\n    values_to = \"infections\"\n  ) %&gt;%\n  # Turn new pivoted variable into a factor to plot nicely\n  mutate(\n    age_group = factor(age_group, levels = c(\"Juveniles\", \"Adults\"))\n  )\n\n\n\nCodeggplot(multistage_df, aes(x = time, y = infections, color = age_group)) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(\n    values = age_group_colors\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"Number of infections\",\n    color = \"Age group\"\n  )\n\n\n\n\n\n\n\nLet’s mimic a situation where we have cross-sectional seroprevalence data (e.g. measures of antibodies that tell you someone is in the R class). In using such data, we’d typically assume that the system was at equilibrium.\n\n6.4.2 Exercise 3: What does the equilibrium age-specific seroprevalence look like in this example?\nUse the code below to display the age-specific seroprevalence (i.e., the seroprevalence for each age group at equilibrium)\n\nCode# Get the last values for all individuals. drop() removes the column name,\n# [-1] removes the time value\nmultistage_equil &lt;- drop(tail(multistage_sol, 1))[-1]\n# Calculate the equilibrium pop sizes of each age group\nmultistage_equil_n &lt;- multistage_equil[sindex] +\n  multistage_equil[iindex] +\n  multistage_equil[rindex]\n\n# Calculate equilibrium seroprevalence for each age group\nmultistage_equil_seroprev &lt;- multistage_equil[rindex] / multistage_equil_n\n\n# Create a dataframe to store equilibrium seroprev for plotting\nmultistage_equil_seroprev_df &lt;- tibble(\n  age = ages,\n  seroprev = multistage_equil_seroprev,\n  width = da_ages\n)\n\n\n\nCodeggplot(multistage_equil_seroprev_df, aes(x = age, y = seroprev, fill = age)) +\n  # Set column width to width of age bands, and justify to start at\n  # lower bound\n  geom_col(\n    width = multistage_equil_seroprev_df$width,\n    just = 1.0,\n    color = \"black\"\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Seroprevalence\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 80, 10)) +\n  scale_fill_continuous(\n    low = age_group_colors[1],\n    high = age_group_colors[2]\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION\n\n\n\nAt what age does the seroprevalence reach 75%?\n\n\nLet’s also compute \\(R_0\\). And because we’ve added a lot of age structure, with transitions between the age groups, we can’t just copy and paste the previous Next Generation Matrix code (from Section 6.6.1.1). As before, the details of this computation are out of the workshop’s scope, but they are outlined in Section 6.6.1.2. We have created a function to calculate R0 for an age-structured SIR and have added some comments, but read Section 6.6.1.2 for the full details and reasoning.\n\nCode# Calculate the stable disease-free age distribution.\n# Could also simulate without any infections.\nmultistage_stable_n &lt;- solve(\n  aging_mat,\n  c(-1 * multistage_params[[\"births\"]], rep(0, 29))\n)\n\n\n\n\nCode#' Calculate R0\n#'\n#' Calculate the R0 of an SIR model using the next generation matrix approach\n#' 'described in @heffernanPerspectivesBasicReproductive2005\n#'\n#' @param beta_mat A matrix of beta parameter values\n#' @param stable_n_mat A matrix of the stable age distributions\n#' @param aging_mat A matrix of the aging rates between age compartments\n#' @param recovery_rate The recover rate parameter value (of type double)\n#'\n#' @return The R0 value as type double\n#' @examples\n#' calculate_R0(\n#'    beta_mat = multistage_params[[\"beta_mat\"]],\n#'    stable_n_mat = multistage_stable_n,\n#'    aging_mat = multistage_params[[\"aging_mat\"]],\n#'    recovery_rate = multistage_params[[\"recovery\"]]\n#')\ncalculate_R0 &lt;- function(beta_mat, stable_n_mat, aging_mat, recovery_rate) {\n  # evaluate new inf jac pde at dfe\n  f_mat &lt;- beta_mat * stable_n_mat\n\n  # set off-diag of non-inf transition jac pde to neg aging of prev age group\n  # (use aging matrix as already calculated in correct places)\n  v_mat &lt;- -aging_mat\n  # Update the diagonal of non-inf transition jac to add recovery rate\n  diag(v_mat) &lt;- diag(v_mat) + recovery_rate\n\n  ## Alternative method of calculating using age bands directly\n  # v_mat &lt;- diag(recovery_rate + 1 / da_ages)\n  # v_mat[row(v_mat) - col(v_mat) == 1] &lt;- - 1 / head(da_ages, -1)\n\n  # spectral trace\n  R0 &lt;- max(Re(eigen(solve(v_mat, f_mat), only.values = TRUE)$values))\n\n  return(R0)\n}\n\n\n\nCodecalculate_R0(\n  beta_mat = multistage_params[[\"beta_mat\"]],\n  stable_n_mat = multistage_stable_n,\n  aging_mat = multistage_params[[\"aging_mat\"]],\n  recovery_rate = multistage_params[[\"recovery\"]]\n)\n\n[1] 6.991242\n\n\n\n6.4.3 Exercise 4: Updating the contact matrix\n\n\n\n\n\n\nImportant\n\n\n\nYou will need to read and edit the following code carefully so that it runs with your updated parameters. We have highlighted the relevant lines in the code chunks, so hopefully you won’t miss them, though make sure you do copy all the code!\n\n\n\n6.4.3.1 Change the juvenile-juvenile contact rate to be 0.025.\n\nCodeupdate_age_beta_mat &lt;- ages_beta_mat\nupdate_age_beta_mat[1:20, 1:20] &lt;- ?update_age_params &lt;- multistage_params\nupdate_age_params[[\"beta_mat\"]] &lt;- update_age_beta_mat\n\n\n\n6.4.3.1.1 Answer: Change the juvenile-juvenile contact rate to be 0.025.\n\nCodeupdate_age_beta_mat &lt;- ages_beta_mat\nupdate_age_beta_mat[1:20, 1:20] &lt;- 0.025\n\nupdate_age_params &lt;- multistage_params\nupdate_age_params[[\"beta_mat\"]] &lt;- update_age_beta_mat\n\n\n\n6.4.3.2 Simulate and plot the age-structured SIR dynamics under your assumptions and record how the age-specific seroprevalence has changed.\n\nCodeupdate_age_sol &lt;- deSolve::ode(\n    y = demog_yinit_ages,\n    times = seq(0, 400, by = 0.1),\n    func = multistage_model,\n    parms = ?\n)\n\n# Get the time series for each infectious age group\nupdate_age_infecteds &lt;- update_age_sol[, 1 + iindex]\n\n# Get the last values in the time series\nupdate_age_equil &lt;- drop(tail(update_age_sol, 1))[-1]\n\n# Calculate the number of individuals in each age group at the final timepoint\nupdate_age_equil_n &lt;- update_age_equil[ ? ] +\n    update_age_equil[ ? ] +\n    update_age_equil[ ? ]\n\n# Calculate final seroprevalence\n# Hint: You need PREVIOUSLY infected individuals\nupdate_age_equil_seroprev &lt;- update_age_equil[ ? ] / update_age_equil_n\n\nupdate_age_equil_seroprev_df &lt;- tibble(\n    age = ages,\n    seroprev = update_age_equil_seroprev,\n    width = da_ages\n)\n\nggplot(update_age_equil_seroprev_df, aes(x = age, y = seroprev, fill = age)) +\n    # Set column width to width of age bands, and justify to start at\n    # lower bound\n    geom_col(\n        width = update_age_equil_seroprev_df$width,\n        just = 1.0, color = \"black\"\n    ) +\n    labs(\n        x = \"Age\",\n        y = \"Seroprevalence\"\n    ) +\n    scale_x_continuous(breaks = seq(0, 80, 10)) +\n    scale_fill_continuous(\n        low = age_group_colors[1],\n        high = age_group_colors[2]\n    )\n\n\n\n6.4.3.2.1 Answer: Simulate and plot the age-structured SIR dynamics under your assumptions and record how the age-specific seroprevalence has changed.\n\nCodeupdate_age_sol &lt;- deSolve::ode(\n  y = demog_yinit_ages,\n  times = seq(0, 400, by = 0.1),\n  func = multistage_model,\n  parms = update_age_params\n)\n\n# Get the time series for each infectious age group\nupdate_age_infecteds &lt;- update_age_sol[, 1 + iindex]\n\nupdate_age_equil &lt;- drop(tail(update_age_sol, 1))[-1]\n\nupdate_age_equil_n &lt;- update_age_equil[sindex] +\n  update_age_equil[iindex] +\n  update_age_equil[rindex]\n\nupdate_age_equil_seroprev &lt;- update_age_equil[rindex] / update_age_equil_n\n\nupdate_age_equil_seroprev_df &lt;- tibble(\n  age = ages,\n  seroprev = update_age_equil_seroprev,\n  width = da_ages\n)\n\n\n\n\n\n\n\n\nQUESTION\n\n\n\nAt what age does the seroprevalence reach 75%? How does this compare to the answer in Section 6.4.2?\n\n\n\n6.4.3.3 Compute \\(R_0\\) for your assumptions.\nAs described previously, the calculation for \\(R_0\\) is difficult due to all the age categories and transitions. Use the calculate_R0() function we defined earlier to calculate \\(R_0\\) for our updated system.\n\nCodecalculate_R0(\n    beta_mat = ?,\n    stable_n_mat = ?,\n    aging_mat = ?,\n    recovery_rate = ?\n)\n\n\n\n6.4.3.3.1 Answer: Compute \\(R_0\\) for your assumptions.\n\nCodeupdate_age_R0 &lt;- round(\n  calculate_R0(\n    beta_mat = update_age_params[[\"beta_mat\"]],\n    stable_n_mat = multistage_stable_n,\n    aging_mat = update_age_params[[\"aging_mat\"]],\n    recovery_rate = update_age_params[[\"recovery\"]]\n  ),\n  digits = 2\n)\n\n\nIf you’ve done everything correctly, you should get \\(R_0 =\\) 7.29. This is higher than previously. Does this match your intuition, given our changes to the beta matrix?",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Session 02</span>"
    ]
  },
  {
    "objectID": "r-session-02.html#r0-and-the-mean-age-of-infection",
    "href": "r-session-02.html#r0-and-the-mean-age-of-infection",
    "title": "\n6  R Session 02\n",
    "section": "\n6.5 R0 and the Mean Age of Infection",
    "text": "6.5 R0 and the Mean Age of Infection\nTo develop some intuition about the relationship between \\(R_0\\) and the mean age of infection, let’s play with an interactive plot. We will assume that the population is completely susceptible and that the force of infection is constant. We’ll also assume that there is heterogenous mixing i.e. no age structure.\nAs we’ve seen in Matt’s lecture on age structure, we can calculate the mean age of infection using the equation below:\n\\[\nA \\approx \\frac{L}{R_E - 1}\n\\tag{6.2}\\]\nwhere \\(L\\) is the life expectancy \\(\\left(L = \\frac{1}{\\mu}\\right)\\) and \\(R_E\\) is the effective reproductive number (\\(R_E = R_0 * (1 - p)\\) where \\(p\\) is the fraction of individuals vaccinated).\n\n\n\n\n\n\nNote\n\n\n\n\n\nSee Section 6.6.2 for code you can run in R to investigate the relationship between \\(R_0\\), vaccination coverage, life expectancy, and the mean age of infection.\n\n\n\n\n6.5.1 Exercise 5: mean age of infection interactions\n\n6.5.1.1 When you increase \\(R_0\\) from 2.0 to 4.0, what happens to the mean age of infection?\n\n6.5.1.1.1 Is there a linear change? If not, why not?\n\n6.5.1.2 With \\(R_0\\) to 4.0, approximately what level of vaccination coverage is required for a mean age of infection of 40 years?\n\n6.5.1.3 Leaving \\(R_0\\) and vaccination coverage the same, decrease the life expectancy to 50 years. What happens to the mean age of infection?\n\n6.5.1.3.1 If it changed, why do you think it did?\n\n\nCodeinit_R0 = 2.0\ninit_vacc = 0.0\ninit_lifeexp = 75\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefunction set(input, value) {\n  input.value = value;\n  input.dispatchEvent(new Event(\"input\", {bubbles: true}));\n}\n\n\n\n\n\n\n\n\nCodeviewof reset = Inputs.button([\n  [\"Reset all sliders\", () =&gt; {\n    set(viewof R0, init_R0)\n    set(viewof vacc, init_vacc)\n    set(viewof lifeexp, init_lifeexp)\n  }]\n])\nviewof R0 = Inputs.range(\n    [1.0, 10.0],\n    {value: 2.0, step: 0.01, label: md`${tex`R_0`}`}\n)\n\nviewof vacc = Inputs.range(\n    [0.0, 1.0],\n    {value: 0.0, step: 0.01, label: \"Vaccination coverage\"}\n)\n\nviewof lifeexp = Inputs.range(\n    [50, 100],\n    {value: 75, step: 1, label: \"Life expectancy\"}\n)\n\nmd`${tex`R_E = ${Re_str}`}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodemd`${tex`\\text{Mean age of infection} = ${Re_mean_age_str}`}`\n\n\n\n\n\n\n\n\n\n\nCodeRe = R0 * (1 - vacc)\nRe_str = Re.toPrecision(4).toLocaleString()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefunction calc_mean_age(Re, lifeexp) {\n    if(Re &gt;= 1) {\n        var mean_age = (lifeexp / (Re - 1))\n    } else {\n        var mean_age = Infinity\n    }\n    return mean_age\n}\n\n\n\n\n\n\n\nCodeR0_mean_age = calc_mean_age(R0, lifeexp)\nRe_mean_age = calc_mean_age(Re, lifeexp)\nRe_mean_age_str = Re_mean_age.toPrecision(4).toLocaleString()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeimport { aq, op } from '@uwdata/arquero'\n\n\n\n\n\n\n\nCodefunction calc_mean_age_arr(vacc, lifeexp, R0_min, R0_max, dR0) {\n    var R0_sim = R0_min\n\n    var R0 = []\n    var Re = []\n    var R0_mean_age = []\n    var Re_mean_age = []\n\n    for (R0_sim = R0_min; R0_sim &lt;= R0_max; R0_sim += dR0) {\n        var Re_sim = R0_sim * (1 - vacc)\n        var R0_mean_age_sim = calc_mean_age(R0_sim, lifeexp)\n        var Re_mean_age_sim = calc_mean_age(Re_sim, lifeexp)\n\n        R0.push(R0_sim)\n        Re.push(Re_sim)\n        R0_mean_age.push(R0_mean_age_sim)\n        Re_mean_age.push(Re_mean_age_sim)\n    }\n\n    return {\n        Re: aq.table({\n                R0: R0,\n                mean_age: Re_mean_age\n            }).filter((d) =&gt; d.mean_age &lt;= 100),\n        R0: aq.table({\n                R0: R0,\n                mean_age: R0_mean_age\n            }).filter((d) =&gt; d.mean_age &lt;= 100)\n    }\n}\n\n\n\n\n\n\n\nCodemean_age_arrs = calc_mean_age_arr(vacc, lifeexp, 1.0, 10.0, 0.01)\n\n\n\n\n\n\n\nCodemean_age_dots = [({\n    arrow_start: R0_mean_age &lt;= 100 ? R0_mean_age : 100,\n    arrow_end: Re_mean_age &lt;= 100 ? Re_mean_age : 100,\n    R0: R0.toPrecision(3),\n    Re: Re.toPrecision(3),\n    R0_mean_age,\n    Re_mean_age\n})]\n\n\n\n\n\n\n\n\n\nCode{\n    let R0Color = \"#1f77b4\"\n    let ReColor = \"#ff7f0e\"\n\n    let plot = Plot.plot({\n        color: {\n            legend: true,\n            domain: [\"Unvaccinated\", \"Vaccinated\"],\n            range: [\"#1f77b4\", \"#ff7f0e\"]\n        },\n        style: {fontSize: \"20px\"},\n        marginLeft: 65,\n        marginTop: 40,\n        marginBottom: 55,\n        grid: true,\n        width: 800,\n        height: 670,\n        x: {label: \"R0\", domain: [0, 10]},\n        y: {label: \"Mean Age of Infection\", domain: [0, 100]},\n        marks: [\n            Plot.line(mean_age_arrs.Re, {x: \"R0\", y: \"mean_age\", stroke: ReColor, strokeWidth: 6}),\n            Plot.line(mean_age_arrs.R0, {x: \"R0\", y: \"mean_age\", stroke: R0Color, strokeWidth: 6}),\n            Re_mean_age &lt;= 100 ?\n                [\n                    vacc &gt; 0.00 ? Plot.dot(mean_age_dots, {x: \"R0\", y: \"Re_mean_age\", r: 12, stroke: ReColor, fill:     ReColor,  fillOpacity: 0.6}) : null,\n                    Plot.text(\n                        mean_age_dots,\n                        {x: \"R0\", y: \"Re_mean_age\", text: (d) =&gt; `Re = ${d.Re}`, dx: 55, dy: -25, fontWeight: \"bold\", fill: ReColor}\n                    )\n                ] :\n            null,\n            R0_mean_age &lt;= 100 ?\n                [\n                Plot.dot(mean_age_dots, {x: \"R0\", y: \"R0_mean_age\", r: 12, stroke: R0Color, fill: R0Color, fillOpacity: 0.6}),\n                Plot.text(\n                    mean_age_dots,\n                    {x: \"R0\", y: \"R0_mean_age\", text: (d) =&gt; `R0 = ${d.R0}`, dx: -60, dy: 30, fontWeight: \"bold\", fill: R0Color}\n                )\n                ] :\n            null,\n            Plot.arrow(mean_age_dots, {x1: \"R0\", x2: \"R0\", y1: \"arrow_start\", y2: \"arrow_end\", strokeWidth: 4, headLength: 5, inset: 15}),\n        ]\n    });\n\n  return plot;\n}",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Session 02</span>"
    ]
  },
  {
    "objectID": "r-session-02.html#bonus-materials",
    "href": "r-session-02.html#bonus-materials",
    "title": "\n6  R Session 02\n",
    "section": "\n6.6 Bonus Materials",
    "text": "6.6 Bonus Materials\n\n6.6.1 Calculating \\(R_0\\) with the Next Generation Matrix\n\n6.6.1.1 Simple model structure\nTo compute \\(R_0\\), we need to know the stable age distribution (the relative proportion in the juvenile and adult age classes) of the population, which we can find by solving for the disease-free equilibrium: \\(S_J^*=B/\\alpha\\) and \\(S_A^*=B/\\mu\\). With the stable age distribution, we can calculate \\(R_0\\) by constructing the next generation matrix. The code below outlines how the next generation matrix is constructed using the \\(\\alpha\\) (aging from juvenile to adult), \\(\\mu\\) (death), \\(n\\) (total births), \\(\\gamma\\) (recovery), \\(da\\) (width of age groups in years), and \\(\\beta\\) (transmission) parameters.\nThe next generation matrix is a matrix that specifies how many new age-specific infections are generated by a typical infected individual of each age class (in a fully susceptible population). For example, let’s consider an infected adult and ask how many new juvenile infections it generates: this is the product of the number of susceptible juveniles (from the stable age distribution), the per capita transmission rate from adults to juveniles and the average duration of infection, i.e. \\(S_J^* \\times \\beta_{JA} \\times 1/ (\\gamma+\\mu)\\). This forms one element of our next generation matrix. The other elements look very similar, except there are extra terms when we consider an infected juvenile because there is a (very small) chance they may age during the infectious period and therefore cause new infections as an adult:\n\\[\n\\mathrm{NGM} = \\begin{pmatrix}\n        \\frac{S_J^* \\beta_{JJ}}{(\\gamma + \\alpha)} +\n        \\frac{\\alpha}{(\\gamma+\\mu)} \\frac{S_J^* \\beta_{JA}}{(\\gamma + \\mu)} &\n        \\frac{S_J^* \\beta_{JA}}{(\\gamma + \\mu)} \\\\\n        \\frac{S_A^* \\beta_{AJ}}{(\\gamma + \\alpha)} +\n            \\frac{\\alpha}{(\\gamma + \\mu)} \\frac{S_A^*\\beta_{AA}}{(\\gamma+\\mu)} &\n            \\frac{S_A^* \\beta_{AA}}{(\\gamma + \\mu)}\n    \\end{pmatrix}\n\\tag{6.3}\\]\n\\(R_0\\) can then be computed as the dominant eigenvalue (i.e., the one with the largest real part) of this matrix. Let’s take an example from a model with 2 age classes, from above. First, let’s define the components of the next generation matrix:\n\nCodengm_params &lt;- c(\n  beta_within = 0.011,\n  beta_between = 0.005,\n  age_band_j = 20,\n  age_band_a = 60,\n  recovery = 10\n)\n\nalpha_ngm &lt;- 1 / ngm_params[\"age_band_j\"]\nmu_ngm &lt;- 1 / ngm_params[\"age_band_a\"]\nn_ngm &lt;- demog_params[\"births\"] / c(alpha_ngm, mu_ngm)\n\nbeta_ngm &lt;- matrix(\n  c(\n    ngm_params[\"beta_within\"],\n    ngm_params[\"beta_between\"],\n    ngm_params[\"beta_between\"],\n    ngm_params[\"beta_within\"]\n  ),\n  nrow = 2,\n  ncol = 2\n)\n\n\nThe Next Generation Matrix can be calculated in R as:\n\nCodengm &lt;- matrix(\n  c(\n    n_ngm[1] *\n      (beta_ngm[1, 1] / (ngm_params[\"recovery\"] + alpha_ngm)) +\n      alpha_ngm /\n        (ngm_params[\"recovery\"] + mu_ngm) *\n        n_ngm[1] *\n        beta_ngm[1, 2] /\n        (ngm_params[\"recovery\"] + mu_ngm),\n\n    n_ngm[2] *\n      beta_ngm[2, 1] /\n      (ngm_params[\"recovery\"] + alpha_ngm) +\n      alpha_ngm /\n        (ngm_params[\"recovery\"] + mu_ngm) *\n        n_ngm[2] *\n        (beta_ngm[2, 2] / (ngm_params[\"recovery\"] + mu_ngm)),\n\n    n_ngm[1] * beta_ngm[1, 2] / (ngm_params[\"recovery\"] + mu_ngm),\n\n    n_ngm[2] * beta_ngm[2, 2] / (ngm_params[\"recovery\"] + mu_ngm)\n  ),\n  nrow = 2,\n  ncol = 2\n)\n\n\nWe can then calculate the eigenvalues and eigenvectors of this matrix:\n\nCodeeigen(ngm)\n\neigen() decomposition\n$values\n[1] 7.191869 1.591188\n\n$vectors\n           [,1]       [,2]\n[1,] -0.1958841 -0.8560336\n[2,] -0.9806271  0.5169202\n\n\nWe can also choose to just output the eigenvalues:\n\nCodeeigen(ngm, only.values = TRUE)\n\n$values\n[1] 7.191869 1.591188\n\n$vectors\nNULL\n\n\nFinally, let’s print \\(R_0\\):\n\nCodemax(\n  Re(\n    eigen(ngm, only.values = TRUE)$values\n  )\n)\n\n[1] 7.191869\n\n\n\n6.6.1.2 Age-structured NGM\nMany times it would be impractical to write out the NGM: there are often too many compartments in an age-structured model. In this instance, we want to use a slightly different approach, but the underlying principles are the same: each element of the NGM balances the number of new infections expected to be produced with the rates of individuals coming in and out of that compartment.\n\n6.6.1.2.1 Stable Age Distribution\nThe first thing we need, as before, is the stable age distribution i.e., the disease-free equilibrium. There are two ways we can do this:\n\nSimulate the model without any infections for a sufficiently long time (simple, but less accurate)\nDo the math.\n\n\n6.6.1.2.1.1 Disease-Free Simulation\n\nCode# Set up initial conditions without any infections\nmultistage_sonly_yinit &lt;- c(\n  S = c(rep(250, 30)),\n  I = c(rep(0, 30)),\n  R = c(rep(0, 30))\n)\n\n# Solve disease free sim to get dfe\nmultistage_sonly_sol &lt;- deSolve::ode(\n  y = multistage_sonly_yinit,\n  times = seq(0, 300, by = 1),\n  func = multistage_model,\n  parms = multistage_params\n)\n\n# Calculate population size at each time point and save to dataframe\nmultistage_sonly_pop &lt;- tibble(\n  time = multistage_sonly_sol[, 1],\n  pop = apply(multistage_sonly_sol[, -1], 1, sum)\n)\n\n\n\nCodeggplot(multistage_sonly_pop, aes(x = time, y = pop)) +\n  geom_area(fill = SIRcolors[4], alpha = 0.6) +\n  labs(\n    x = \"Time\",\n    y = \"Population size\"\n  )\n\n\n\n\n\n\n\n\n6.6.1.2.1.2 Doing the Math\nAlternatively, we can get the stable age distribution by finding the population structure that balances the birth, aging, and death processes. We have already seen the aging matrix in Equation 6.1, and at equilibrium, we have the matrix equation\n\\[\n\\begin{pmatrix}\n    -\\alpha_1 & 0 & 0 & \\cdots & 0\\\\\n    \\alpha_1 & -\\alpha_2 & 0 & \\cdots & 0\\\\\n    0 & \\alpha_2 & -\\alpha_3 & \\cdots & 0\\\\\n    \\vdots &  & \\ddots & \\ddots & \\vdots \\\\\n    0 & \\cdots & & \\alpha_{29} & -\\alpha_{30}\\\\\n\\end{pmatrix} .\n\\begin{pmatrix}\n    n_1 \\\\ n_2 \\\\ n_3 \\\\ \\vdots \\\\ n_{30}\n\\end{pmatrix} +\n\\begin{pmatrix}\n    B \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0\n\\end{pmatrix} =\n\\begin{pmatrix}\n    0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0\n\\end{pmatrix}\n\\]\nTo solve this equation in R, we can do\n\nCode# solve(a, b) solves the equation a %*% x = b for x, so rearrange equation\n# above so b is on the RHS of the equation\nmultistage_stable_n &lt;- solve(\n  aging_mat,\n  c(-1 * multistage_params[[\"births\"]], rep(0, 29))\n)\n\nmultistage_stable_n\n\n [1]  100  100  100  100  100  100  100  100  100  100  100  100  100  100  100\n[16]  100  100  100  100  100  500  500  500  500  500  500  500  500  500 1500\n\n\n\nCode# Check the final pop value of the S-only sim is equal to the sum of the\n# stable age distribution calculated above\nround(tail(multistage_sonly_pop$pop, 1)) == sum(multistage_stable_n)\n\n[1] TRUE\n\n\nThe following lines then compute \\(R_0\\) using the next generation matrix method. This calculation comes from a recipe described in detail previously (Diekmann and Heesterbeek 2000; Heesterbeek 2002; Bjørnstad 2018; Heffernan, Smith, and Wahl 2005; Hurford, Cownden, and Day 2009) (we would recommend starting with (Bjørnstad 2018; and Heffernan, Smith, and Wahl 2005)).\nThe steps below are copied from (Bjørnstad 2018)\n\nIdentify all n infected compartments\nConstruct a n × 1 matrix, \\(\\mathbf{F}\\), that contains expressions for all completely new infections entering each infected compartment\nConstruct a n × 1 matrix, \\(\\mathbf{V^−}\\), that contains expressions for all losses out of each infected compartment\nConstruct a n × 1 matrix, \\(\\mathbf{V^+}\\), that contains expressions for all gains into each infected compartment that does not represent new infections but transfers among infectious classes\nConstruct a n × 1 matrix, \\(\\mathbf{V} = \\mathbf{V^−} − \\mathbf{V^+}\\)\n\nGenerate two n × n Jacobian matrices \\(f\\) and \\(v\\) that are the partial derivatives of \\(\\mathbf{F}\\) and \\(\\mathbf{V}\\) with respect to the \\(n\\) infectious state variables\nEvaluate the matrices at the disease free equilibrium (dfe), and finally\n\n\\(R_0\\) is the spectral trace (greatest non-negative real eigenvalue) of \\(\\mathbf{fv}^{−1}|_{\\text{dfe}}\\).\n\nWorking through these steps looks like this:\n\nOur only infected compartments are the \\(I_i\\) states, for each age group (\\(i \\in [1, 30]\\)) To start, let’s write out our differential equation:\n\n\\[\\begin{equation}\n    \\frac{\\dd{I_i}}{\\dd{t}} = \\lambda_i S_i - \\gamma I_i + \\alpha_{i-1} I_{i-1} - \\alpha_i I_i\n\\end{equation}\\]\n\nWe’ll now calculate \\(\\mathbf{F}\\) and \\(\\mathbf{f}\\)\n\n\n\\[\\begin{align*}\n    \\mathbf{F} &= \\begin{pmatrix}\n        \\lambda_1 S_1 + \\cancelto{0}{\\alpha_0 I_0} \\\\\n        \\vdots \\\\\n        \\lambda_{30} S_{30} + \\alpha_{29} I_{29}\n    \\end{pmatrix} \\\\ \\\\\n    \\mathbf{F} &= \\begin{pmatrix}\n        \\left(\\beta_{1, 1} I_1 + \\cdots + \\beta_{1, 30} I_{30} \\right)S_1 \\\\\n        \\vdots \\\\\n        \\left(\\beta_{30, 1} I_1 + \\cdots + \\beta_{30, 30} I_{30} \\right) S_{30} + \\alpha_{29} I_{29}\n    \\end{pmatrix} \\\\\n    \\mathbf{f} &= \\begin{pmatrix}\n        \\frac{\\partial F_1}{\\partial I_1} & \\cdots & \\frac{\\partial F_1}{\\partial I_{30}} \\\\\n        \\vdots & \\ddots & \\vdots \\\\\n        \\frac{\\partial F_{30}}{\\partial I_1} & \\cdots & \\frac{\\partial F_{30}}{\\partial I_{30}}\n    \\end{pmatrix} & \\frac{\\partial F_1}{\\partial I_1} &= \\frac{\\partial}{\\partial I_1} \\left( \\beta_{1, 1} I_1 + \\cancelto{0}{\\beta_{1, 2} I_2 + \\cdots + \\beta_{1, 30} I_{30}}\\right) S_1 \\\\\n    & & \\frac{\\partial F_1}{\\partial I_1} &= \\beta_{1, 1} S_1 \\\\\n    \\mathbf{f} &= \\begin{pmatrix}\n        \\beta_{1, 1} S_1 & \\cdots & \\beta_{1, 30} S_1 \\\\\n        \\vdots & \\ddots & \\vdots \\\\\n        \\beta_{30, 1} S_{30} & \\cdots & \\beta_{30, 30} S_{30}\n    \\end{pmatrix}\n\\end{align*}\\]\n\nNow let’s calculate \\(\\mathbf{V^-}\\), \\(\\mathbf{V^+}\\), \\(\\mathbf{V}\\), and \\(\\mathbf{v}\\)\n\n\n\\[\\begin{align*}\n    \\mathbf{V^-} &= \\begin{pmatrix}\n        \\gamma I_1 + \\alpha_1 I_1 \\\\\n        \\gamma I_2 + \\alpha_2 I_2 \\\\\n        \\vdots \\\\\n        \\gamma I_{30} + \\alpha_{30} I_{30}\n    \\end{pmatrix} & \\mathbf{V^+} &= \\begin{pmatrix}\n        \\cancelto{0}{\\alpha_0 I_0} \\\\\n        \\alpha_1 I_1 \\\\\n        \\vdots \\\\\n        \\alpha_{29} I_{29}\n    \\end{pmatrix} \\\\ \\\\\n    \\mathbf{V} &= \\mathbf{V^-} - \\mathbf{V^+} = \\begin{pmatrix}\n        \\gamma I_1 + \\alpha_1 I_1 \\\\\n        \\gamma I_2 + \\alpha_2 I_2 - \\alpha_1 I_1\\\\\n        \\vdots \\\\\n        \\gamma I_{30} + \\alpha_{30} I_{30} - \\alpha_{29} I_{29}\n    \\end{pmatrix} \\\\ \\\\\n    \\mathbf{v} &= \\begin{pmatrix}\n        \\pdv{V_1}{I_1} & \\cdots & \\pdv{V_1}{I_{30}} \\\\\n        \\vdots & \\ddots & \\vdots \\\\\n        \\pdv{V_{30}}{I_1} & \\cdots & \\pdv{V_{30}}{I_{30}}\n    \\end{pmatrix} & \\pdv{V_1}{I_1} &= \\pdv{I_1} I_1 \\left( \\gamma + \\alpha_1  \\right)\\\\\n    & & \\pdv{V_1}{I_1} &= \\gamma + \\alpha_1 \\\\ \\\\\n    & & \\pdv{V_2}{I_1} &= \\pdv{I_1} \\left( \\cancelto{0}{I_2 \\left( \\gamma + \\alpha_2  \\right)} - \\alpha_1 I_1 \\right)\\\\\n    & & \\pdv{V_2}{I_1} &= - \\alpha_1 \\\\ \\\\\n    & & \\pdv{V_1}{I_2} &= \\pdv{I_2} \\cancelto{0}{I_1 \\left( \\gamma + \\alpha_1  \\right)} \\\\\n    & & \\pdv{V_1}{I_2} &= 0 \\\\ \\\\\n    \\mathbf{v} &= \\begin{pmatrix}\n        \\gamma + \\alpha_1 & 0 &  \\cdots  & 0\\\\\n        - \\alpha_1 & \\gamma + \\alpha_2 & \\cdots & 0 \\\\\n        \\vdots & \\ddots & \\ddots & \\vdots \\\\\n        0 & \\cdots & - \\alpha_{29} & \\gamma + \\alpha_{30}\n    \\end{pmatrix}\n\\end{align*}\\]\n\nTo evaluate \\(\\mathbf{f}\\) and \\(\\mathbf{v}\\) at the disease-free equilibrium, we can use the results from our previous calculations. \\(\\mathbf{v}\\) doesn’t have any state terms in the equation, so it is already evaluated at \\(\\text{dfe}\\). \\(\\mathbf{f}|_{\\text{dfe}}\\) involves subsituting \\(S_i\\) for the equilibrium population distribution that balances the births and aging processes.\n\nThis translates to the function we defined earlier.\n\n6.6.2 Mean age of infection R code\nNow let’s look at how we can investigate our the relationships between the mean age of infection and \\(R_0\\) and the vaccination coverage using R. Unlike the interactive plot that simply uses Equation 6.2 to calculate the mean age of infection, we will use a more realistic age-structured model.\nLet’s return to the earlier models with an age-class mixing matrix. But this time, we’ll calculate \\(R_0\\), the mean age of infection, and the number of cases that occur in individuals between 15-35 years as we increase the contact rate.\nRecall from the rubella and congenital rubella syndrome (CRS) example that the risk of severe disease outcomes depends on the risk of infection in reproductive age women (here we’ll use individuals between 15 and 35 years as a proxy; in reality we would want to account for the differential rate of reproduction at different ages, including those above 35 years). Recall also that increasing vaccination reduces \\(R_E\\) – for simplicity here, so we don’t have to add vaccination into the code, we’ll simply change \\(R_0\\) because we already know that will give us outcomes that are dynamically equivalent to increasing the proportion of children born who are vaccinated. We’ll then calculate how the mean age of infection changes, and specifically how the absolute number of cases among individuals between the ages of 15-35 (as a proxy for reproductive age women) changes. To do so, we’ll make a loop and evaluate the code for each of 10 decreaing levels of mixing (which will reduce \\(R_0\\) and we can interpret as analogous to the reduction in \\(R_E\\) that would result from increasing vaccination).\n\n\n\n\n\n\nNote about map()\n\n\n\n\n\nAs you may have noticed previously, we often use the map_*() series of functions. We’ll use that again here (map_dfr()). The full reasons are too complicated to get into here, but broadly speaking, the map_*() functions provide us guarantees over the output of our loops. If it runs, we know that something didn’t get silently skipped, and that out output vector/list/dataframe is the same length as the inputs. The same can not be said for for() loops, and the base apply functions are more awkward to work with as they don’t have a consistent syntax across the family of functions.\nTo learn more, read this section of our R primer.\n\n\n\n\nCode# Create vector of scalings to reduce R0\nscale_contact &lt;- seq(1, .2, length = 10)\n\n# Create a new transmission matrix\nbeta_low &lt;- 0.007\nbeta_medium &lt;- 0.02\nbeta_high &lt;- 0.03\n\nbeta_mat &lt;- matrix(beta_low, nrow = 30, ncol = 30)\nbeta_mat[1:20, 1:20] &lt;- beta_medium\nbeta_mat[6:16, 6:16] &lt;- beta_high\n\nscaled_params &lt;- multistage_params\n\n\n\nCode# Create a dataframe where each row relates to a different R0 value\nR0_mean_age_contacts_df &lt;- map_dfr(\n  # Apply the function to each item in the vector of R0 scaling factors\n  .x = scale_contact,\n  .f = function(.x) {\n    # Scale contacts\n    scaled_beta_mat &lt;- beta_mat * .x\n\n    # Set up parameters\n    scaled_params[[\"beta_mat\"]] &lt;- scaled_beta_mat\n\n    # Solve the model\n    sol &lt;- deSolve::ode(\n      y = demog_yinit_ages,\n      times = seq(0, 400, by = 0.1),\n      func = multistage_model,\n      parms = scaled_params\n    )\n\n    # Get stable age distribution\n    stable_n &lt;- solve(\n      scaled_params[[\"aging_mat\"]],\n      -c(scaled_params[[\"births\"]], rep(0, 29))\n    )\n\n    R0 &lt;- calculate_R0(\n      beta_mat = scaled_params[[\"beta_mat\"]],\n      stable_n_mat = stable_n,\n      aging_mat = scaled_params[[\"aging_mat\"]],\n      recovery = scaled_params[[\"recovery\"]]\n    )\n\n    sol_dims &lt;- dim(sol)\n\n    final_age_sizes &lt;- sol[sol_dims[1], 2:sol_dims[2]]\n\n    # Get final number of infected individuals for each S, I, and R class\n    susceptibles &lt;- final_age_sizes[sindex]\n    infecteds &lt;- final_age_sizes[iindex]\n    recovereds &lt;- final_age_sizes[rindex]\n\n    # Calculate mean age of infection\n    mean_age &lt;- sum(ages * infecteds / sum(infecteds))\n\n    # Calculate sum of cases between 15-35 years recall, from the figures\n    # above, that that this is the equilibrium prevalence of infection in\n    # these age classes, or the average number of individuals that are\n    # infected at any given time in these age classes. Note that we're not\n    # differentiating between individuals who can and cannot get pregnant\n    # here. So we're making an implicit assumption that there no\n    # difference in the risk of rubella infection in these groups so that\n    # if prevalence goes up in one group, it goes up in the other.\n    sum_cases &lt;- sum(infecteds[15:23])\n\n    total_15_35 &lt;- sum(susceptibles[15:23]) +\n      sum_cases +\n      sum(recovereds[15:23])\n\n    # Calculate the prevalence as a proportion per 100000 population\n    prev_perc &lt;- sum_cases * 100000 / total_15_35\n\n    # Return a dataframe with the values\n    return(tibble(R0, mean_age, sum_cases, prev_perc))\n  }\n)\n\n\nNow we can make a table of the results and plot mean age and the sum of cases between 15-35 years of age as a function of \\(R_0\\).\n\nCode# Create a table from the dataframe\ngt(R0_mean_age_contacts_df) %&gt;%\n  fmt_number(\n    columns = everything(),\n    decimals = 2\n  ) %&gt;%\n  # Relabel the column headers\n  cols_label(\n    R0 = md(\"**R0**\"),\n    mean_age = md(\"**Mean age of&lt;br&gt;infection**\"),\n    sum_cases = md(\"**Total cases between&lt;br&gt;15-35 years**\"),\n    prev_perc = md(\"**Prevalence (per 100_000) &lt;br&gt; between 15-35 years**\")\n  ) %&gt;%\n  # Apply style to the table with gray alternating rows\n  opt_stylize(style = 1, color = 'gray') %&gt;%\n  # Increate space between columns\n  opt_horizontal_padding(scale = 3) %&gt;%\n  cols_align(\"center\")\n\n\n\n\n\n\n\n\n\n\n\nR0\nMean age ofinfection\nTotal cases between15-35 years\nPrevalence (per 100_000)  between 15-35 years\n\n\n\n6.85\n6.08\n0.54\n25.91\n\n\n6.24\n6.61\n0.66\n31.49\n\n\n5.63\n7.28\n0.81\n38.34\n\n\n5.02\n8.15\n0.98\n46.74\n\n\n4.41\n9.30\n1.20\n56.95\n\n\n3.81\n10.87\n1.45\n69.10\n\n\n3.20\n13.11\n1.74\n82.80\n\n\n2.59\n16.40\n2.01\n95.94\n\n\n1.98\n21.50\n2.11\n100.71\n\n\n1.37\n29.72\n1.48\n70.44\n\n\n\n\n\n\n\nCodeR0_mean_age_contacts_df %&gt;%\n  select(-sum_cases) %&gt;%\n  # Convert to long data frame for facet plotting\n  pivot_longer(-R0, names_to = \"metric\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = R0, y = value)) +\n  geom_line(color = \"slategray4\") +\n  geom_point(shape = 21, size = 5, fill = \"slategray4\", alpha = 0.8) +\n  facet_wrap(\n    ~metric,\n    scales = \"free_y\",\n    labeller = as_labeller(c(\n      mean_age = \"Mean Age of Infection\",\n      prev_perc = \"Prevalence (per 100_000) between 15-35 years\"\n    ))\n  ) +\n  labs(\n    x = \"R0\",\n    y = \"Value\"\n  )\n\n\n\n\n\n\n\n\n6.6.3 What do real contact networks look like?\nThe POLYMOD study (Mossong et al. 2008) was a journal-based look into the contact network in contemporary European society. Let’s have a look what these data tell us about the contact structure.\n\nCodemossong_cont_net &lt;- rio::import(\n  \"https://raw.githubusercontent.com/arnold-c/SISMID-Module-02_2023/main/data/mossong-matrix.csv\"\n)\n# mossong_cont_net &lt;- rio::import(here::here(\"data\", \"mossong-matrix.csv\"))\n\nmossong_ages &lt;- unique(mossong_cont_net$contactor)\nmossong_cont_net$contactor &lt;- ordered(\n  mossong_cont_net$contactor,\n  levels = mossong_ages\n)\n\nmossong_cont_net$contactee &lt;- ordered(\n  mossong_cont_net$contactee,\n  levels = mossong_ages\n)\n\n\nSince contacts are symmetric, we’ll need to estimate the symmetric contact matrix.\n\nCodemossong_mat &lt;- mossong_cont_net %&gt;%\n  pivot_wider(\n    names_from = contactor,\n    values_from = contact.rate\n  ) %&gt;%\n  select(-contactee) %&gt;%\n  as.matrix()\n\nrownames(mossong_mat) &lt;- mossong_ages\n\n# Create a symmetrical contact matrix\nmossong_mat_sym &lt;- (mossong_mat + t(mossong_mat)) / 2\n\n\nHere we’ll use the filled.contour function to visualize the contact matrix, to show you an alternative way of visualizing contact matrices. Notices that we are using the raw matrix object, not a long dataframe, as previously.\n\nCodefilled.contour(\n  ages,\n  ages,\n  log10(mossong_mat),\n  plot.title = title(\n    main = \"Log10 of Raw Contact Rate\",\n    xlab = \"Age of Contactor\",\n    ylab = \"Age of Contactee\"\n  )\n)\n\n\n\n\n\n\n\n\nCodefilled.contour(\n  ages,\n  ages,\n  log10(mossong_mat_sym),\n  plot.title = title(\n    main = \"Log10 of Symmetrical Contact Rate\",\n    xlab = \"Age of Contactor\",\n    ylab = \"Age of Contactee\"\n  )\n)\n\n\n\n\n\n\n\n\nCodemossong_cont_sums &lt;- tibble(\n  age = factor(mossong_ages, levels = mossong_ages),\n  contactees = rowSums(mossong_mat),\n  contactors = colSums(mossong_mat)\n) %&gt;%\n  pivot_longer(-age, names_to = \"type\", values_to = \"total_contacts\")\n\n\n\nCodeggplot(\n  mossong_cont_sums,\n  aes(\n    x = age,\n    y = total_contacts,\n    color = type,\n    fill = type,\n    group = type\n  )\n) +\n  geom_path(linewidth = 1) +\n  geom_point(\n    position = \"identity\",\n    alpha = 0.8,\n    shape = 21,\n    size = 4\n  ) +\n  scale_color_manual(\n    values = c(\"slategray4\", \"navy\"),\n    labels = c(\"Contactees\", \"Contactors\"),\n    aesthetics = c(\"color\", \"fill\")\n  ) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Age\",\n    y = \"Total contacts\",\n    fill = \"Type of contact\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWhile this matrix tells us how many contacts are made per year by an individual of each age, it doesn’t tell us anything about the probability that a contact results in communication of infection. Let’s assume that each contact has a constant probability \\(q\\) of resulting in a transmission event.\n\nCodeq &lt;- 3e-5\nmossong_beta_mat &lt;- q * mossong_mat_sym\n\n\n\nCodefilled.contour(\n  ages,\n  ages,\n  log10(mossong_beta_mat),\n  plot.title = title(\n    main = \"WAIFW matrix based on POLYMOD data\",\n    xlab = \"Age\",\n    ylab = \"Age\"\n  )\n)\n\n\n\n\n\n\n\nNow let’s simulate the introduction of such a pathogen into a population characterized by this contact structure.\n\nCode# Update the parameters with the POLYMOD-based beta matrix\nmossong_params &lt;- multistage_params\nmossong_params[[\"beta_mat\"]] &lt;- mossong_beta_mat\n\n# Solve the model with the updated parameters\nmossong_sol &lt;- deSolve::ode(\n  y = demog_yinit_ages,\n  times = seq(0, 200, by = 0.5),\n  func = multistage_model,\n  parms = mossong_params\n)\n\n# Extract the timeseries of infectious individuals\nmossong_infecteds &lt;- mossong_sol[, 1 + iindex]\n\n# Convert infectious individual timeseries to dataframe for plotting\nmossong_infecteds_df &lt;- tibble(\n  time = mossong_sol[, 1],\n  Juveniles = apply(mossong_infecteds[, juvies], 1, sum),\n  Adults = apply(mossong_infecteds[, adults], 1, sum)\n) %&gt;%\n  pivot_longer(\n    cols = c(Juveniles, Adults),\n    names_to = \"age_group\",\n    values_to = \"infections\"\n  ) %&gt;%\n  mutate(\n    age_group = factor(age_group, levels = c(\"Juveniles\", \"Adults\"))\n  )\n\n\n\nCodeggplot(\n  mossong_infecteds_df,\n  aes(x = time, y = infections, color = age_group)\n) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(\n    values = age_group_colors\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"Number of infections\",\n    color = \"Age group\"\n  )\n\n\n\n\n\n\n\nAs before, we can also look at the equilibrium seroprevalence\n\nCode# Get last time point\nmossong_equil &lt;- drop(tail(mossong_sol, 1))[-1]\n\n# Calculate number of individuals in each age group at end of simulation\nmossong_equil_n &lt;- mossong_equil[sindex] +\n  mossong_equil[iindex] +\n  mossong_equil[rindex]\n\n# Calculate equilibrium seroprevalence\nmossong_equil_seroprev &lt;- mossong_equil[rindex] / mossong_equil_n\n\n# Convert to dataframe for plotting\nmossong_equil_seroprev_df &lt;- tibble(\n  # We can reuse the ages vectors from before as they are the same\n  # as the POLYMOD data\n  age = ages,\n  seroprev = mossong_equil_seroprev,\n  width = da_ages\n)\n\n\n\nCodeggplot(mossong_equil_seroprev_df, aes(x = age, y = seroprev, fill = age)) +\n  geom_col(\n    width = mossong_equil_seroprev_df$width,\n    just = 1.0,\n    color = \"black\"\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Seroprevalence\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 80, 10)) +\n  scale_fill_continuous(\n    low = age_group_colors[1],\n    high = age_group_colors[2]\n  )\n\n\n\n\n\n\n\nand compute the \\(R_0\\) for this infection.\n\nCodemossong_stable_n &lt;- solve(\n  mossong_params[[\"aging_mat\"]],\n  -c(mossong_params[[\"births\"]], rep(0, 29))\n)\n\ncalculate_R0(\n  beta_mat = mossong_params[[\"beta_mat\"]],\n  stable_n_mat = mossong_stable_n,\n  aging_mat = mossong_params[[\"aging_mat\"]],\n  recovery = mossong_params[[\"recovery\"]]\n)\n\n[1] 7.058675\n\n\n\n\n\n\n\n\nQUESTION\n\n\n\nHow does this R0 value compare to the R0 value obtained from Section 6.4.2?\n\n\n\n\n\n\n\n\nBjørnstad, Ottar N. 2018. “Advanced: The Next-Generation Matrix.” In Epidemics: Models and Data Using R, 51. Use R! Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-97487-3.\n\n\nDiekmann, O., and J. A. P. Heesterbeek. 2000. Mathematical Epidemiology of Infectious Diseases: Model Building, Analysis and Interpretation. Wiley Series in Mathematical & Computational Biology. Wiley. https://books.google.ca/books?id=5VjSaAf35pMC.\n\n\nHeesterbeek, J. A. P. 2002. “A Brief History of R0 and a Recipe for Its Calculation.” Acta Biotheoretica 50 (3): 189–204. https://doi.org/10.1023/A:1016599411804.\n\n\nHeffernan, J. M, R. J Smith, and L. M Wahl. 2005. “Perspectives on the Basic Reproductive Ratio.” J R Soc Interface 2 (4): 281–93. https://doi.org/10.1098/rsif.2005.0042.\n\n\nHurford, Amy, Daniel Cownden, and Troy Day. 2009. “Next-Generation Tools for Evolutionary Invasion Analyses.” Journal of The Royal Society Interface 7 (45): 561–71. https://doi.org/10.1098/rsif.2009.0448.\n\n\nKing, Aaron A, and Helen J Wearing. 2011. “Age Structured Models.” In. https://ms.mcmaster.ca/~bolker/eeid/2011_eco/waifw.pdf.\n\n\nMossong, Joël, Niel Hens, Mark Jit, Philippe Beutels, Kari Auranen, Rafael Mikolajczyk, Marco Massari, et al. 2008. “Social Contacts and Mixing Patterns Relevant to the Spread of Infectious Diseases.” PLOS Medicine 5 (3): e74. https://doi.org/10.1371/journal.pmed.0050074.",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Session 02</span>"
    ]
  },
  {
    "objectID": "L06_parameter-estimation.html",
    "href": "L06_parameter-estimation.html",
    "title": "7  Parameter Estimation",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "r-session-03.html",
    "href": "r-session-03.html",
    "title": "\n8  R Session 03\n",
    "section": "",
    "text": "8.1 Setup\nCodelibrary(here)\nlibrary(rio)\nlibrary(deSolve)\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(gt)\nCodetheme_set(theme_minimal())\nCode# Loads the datasets: flu, measles, niamey, plauge\nflu &lt;- rio::import(\n  \"https://raw.githubusercontent.com/arnold-c/SISMID-Module-02_2025/main/data/flu.csv\"\n)\nniamey &lt;- rio::import(\n  \"https://raw.githubusercontent.com/arnold-c/SISMID-Module-02_2025/main/data/niamey.csv\"\n)\n# flu &lt;- rio::import(here::here(\"data\", \"flu.csv\"))\n# niamey &lt;- rio::import(here::here(\"data\", \"niamey.csv\"))",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#estimating-r_0-problem-background",
    "href": "r-session-03.html#estimating-r_0-problem-background",
    "title": "\n8  R Session 03\n",
    "section": "\n8.2 Estimating \\(R_0\\) Problem Background",
    "text": "8.2 Estimating \\(R_0\\) Problem Background\nSo far in this class we have focused on the theory of infectious disease. Often, however, we will want to apply this theory to particular situations. One of the key applied problems in epidemic modeling is the estimation of \\(R_0\\) from outbreak data. In this session, we study two methods for estimating \\(R_0\\) from an epidemic curve. As a running example, we will use the data on influenza in a British boarding school.\n\nCodeggplot(flu, aes(x = day, y = flu)) +\n  geom_line(color = \"slategray4\") +\n  geom_point(shape = 21, size = 5, fill = \"slategray4\", alpha = 0.8) +\n  labs(x = \"Day\", y = \"Active Influenza Cases\")",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#estimating-r_0-from-the-final-outbreak-size",
    "href": "r-session-03.html#estimating-r_0-from-the-final-outbreak-size",
    "title": "\n8  R Session 03\n",
    "section": "\n8.3 Estimating \\(R_0\\) From The Final Outbreak Size",
    "text": "8.3 Estimating \\(R_0\\) From The Final Outbreak Size\nOur first approach is to estimate \\(R_0\\) from the final outbreak size. Although unhelpful at the early stages of an epidemic (before the final epidemic size is observed), this method is nonetheless a useful tool for post hoc analysis. The method is general and can be motivated by the argument listed in (Keeling and Rohani 2008):\nFirst, we assume that the epidemic is started by a single infectious individual in a completely susceptible population. On average, this individual infects \\(R_0\\) others. The probability a particular individual escaped infection is therefore \\(e^{-R_0 / N}\\).\nIf \\(Z\\) individuals have been infected, the probability of an individual escaping infection from all potential sources is \\(e^{-Z R_0 / N}\\). It follows that at the end of the epidemic a proportion \\(R(\\infty) = Z / N\\) have been infected and the fraction remaining susceptible is \\(S(\\infty) = e^{-R(\\infty) R_0}\\), which is equal to \\(1 - R(\\infty)\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\n\\(S(\\infty) = e^{-R(\\infty) R_0}\\) can be calculated by acknowledging that at equilibrium (\\(t = \\infty\\)), \\(S(\\infty) = 1 - R(\\infty) = Z / N\\), so substituting \\(R(\\infty)\\) into \\(1 - e^{-Z R_0 / N}\\) gives the desired result.\nIt could also be calculated by dividing \\(\\frac{\\dd{S}}{\\dd{t}}\\) by \\(\\frac{\\dd{R}}{\\dd{t}}\\):\n\\[\\begin{aligned}\n\\frac{\\dd{S}}{\\dd{R}} &= - \\frac{\\beta S}{\\gamma} \\\\\n&= - R_0 S\n\\end{aligned}\\]\nwhich is a separable differential equation, so can be integrated as follows:\n\\[\\begin{aligned}\n- \\int_{0}^{t} \\frac{1}{R_0 S} \\dd{S} &= \\int_{0}^{t} \\dd{R} \\\\\n- \\frac{1}{R_0} \\left(\\ln{S(t)} - \\ln{S(0)} \\right) &= R(t) - \\cancelto{0}{R(0)} \\\\\n\\ln{S(t)} &= \\ln{S(0)} - R_0 R(t) \\\\\nS(t) &= S(0) e^{-R_0 R(t)}\n\n\\end{aligned}\\]\n\n\n\nPutting this together, we get:\n\\[\n1 - R(\\infty) - e^{-R(\\infty) R_0} = 0\n\\]\nRearranging, we have the estimator\n\\[\n  \\hat{R_0} = \\frac{\\ln(1 - Z / N)}{-Z / N},\n\\]\nwhich, in this case, evaluates to \\(\\frac{\\ln(1 - 512 / 764)}{-512 / 764} = 1.655\\).\n\n8.3.1 Exercise 1\nThis equation shows the important one-to-one relationship between \\(R_0\\) and the final epidemic size. Plot the relationship between the total epidemic size and \\(R_0\\) for the complete range of values between 0 and 1.",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#linear-approximation",
    "href": "r-session-03.html#linear-approximation",
    "title": "\n8  R Session 03\n",
    "section": "\n8.4 Linear Approximation",
    "text": "8.4 Linear Approximation\nThe next method we introduce takes advantage of the fact that during the early stages of an outbreak, the number of infected individuals is given approximately as \\(I(t) \\approx I_0 e^{((R_0 - 1)(\\gamma + \\mu)t)}\\). Taking logarithms of both sides, we have \\(\\ln(I(t)) \\approx \\ln(I_0) + (R_0 - 1)(\\gamma + \\mu)t\\), showing that the log of the number of infected individuals is approximately linear in time with a slope that reflects both \\(R_0\\) and the recovery rate.\nThis suggests that a simple linear regression fit to the first several data points on a log-scale, corrected to account for \\(\\gamma\\) and \\(\\mu\\), provides a rough and ready estimate of \\(R_0\\). For flu, we can assume \\(\\mu =0\\) because the epidemic occurred over a time period during which natural mortality is negligible. Further, assuming an infectious period of about 2.5 days, we use \\(\\gamma = (2.5)^{-1} = 0.4\\) for the correction. Fitting to the first four data points, we obtain the slope as follows.\n\nCode# Fit a linear model\nlinear_model &lt;- lm(log(flu[1:4]) ~ day[1:4], data = flu)\n\n# Summary statistics for fit model\nsummary(linear_model)\n\n\nCall:\nlm(formula = log(flu[1:4]) ~ day[1:4], data = flu)\n\nResiduals:\n       1        2        3        4 \n 0.03073 -0.08335  0.07450 -0.02188 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -0.02703    0.10218  -0.265  0.81611   \nday[1:4]     1.09491    0.03731  29.346  0.00116 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08343 on 2 degrees of freedom\nMultiple R-squared:  0.9977,    Adjusted R-squared:  0.9965 \nF-statistic: 861.2 on 1 and 2 DF,  p-value: 0.001159\n\nCode# Extract slope parameter\ncoef(linear_model)[2]\n\nday[1:4] \n1.094913 \n\n\nRearranging the linear equation above and denoting the slope coefficient by \\(\\hat \\beta_1\\) we have the estimator \\(\\hat R_0 = \\hat \\beta_1 / \\gamma + 1\\) giving \\(\\hat R_0 = 1.094913 / 0.4 + 1 \\approx 3.7\\).\n\n8.4.1 Exercise 2\nOur estimate assumes that boys remained infectious during the natural course of infection. The original report on this epidemic indicates that boys found to have symptoms were immediately confined to bed in the infirmary. The report also indicates that only 1 out of 130 adults at the school exhibited any symptoms. It is reasonable, then, to suppose that transmission in each case ceased once he had been admitted to the infirmary. Supposing admission happened within 24 hours of the onset of symptoms. How does this affect our estimate of \\(R_0\\)? Twelve hours?\n\n8.4.2 Exercise 3\nBiweekly data for outbreaks of measles in three communities in Niamey, Niger are provided in the dataframe niamey. Use this method to obtain estimates of \\(R_0\\) for measles from the first community assuming that the infectious period is approximately two weeks or \\(\\frac{14}{365} \\approx 0.0384\\) years.\n\n8.4.3 Exercise 4\nA defect with this method is that it uses only a small fraction of the information that might be available, i.e., the first few data points. Indeed, there is nothing in the method that tells one how many data points to use–this is a matter of judgment. Further, there is a tradeoff in that as more and more data points are used the precision of the estimate increases, but this comes at a cost of additional bias. Plot the estimate of \\(R_0\\) obtained from \\(n=3, 4, 5, ...\\) data points against the standard error of the slope from the regression analysis to show this tradeoff.",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#estimating-dynamical-parameters-with-least-squares",
    "href": "r-session-03.html#estimating-dynamical-parameters-with-least-squares",
    "title": "\n8  R Session 03\n",
    "section": "\n8.5 Estimating dynamical parameters with least squares",
    "text": "8.5 Estimating dynamical parameters with least squares\nThe objective of the previous exercise was to estimate \\(R_0\\). Knowing \\(R_0\\) is critical to understanding the dynamics of any epidemic system. It is, however, a composite quantity and is not sufficient to completely describe the epidemic trajectory. For this, we require estimates for all parameters of the model. In this exercise, we introduce a simple approach to model estimation called least squares fitting, sometimes called trajectory matching. The basic idea is that we find the values of the model parameters that minimize the squared differences between model predictions and the observed data. To demonstrate least squares fitting, we consider an outbreak of measles in Niamey, Niger, reported on by (Grais et al. 2006).\n\nCode# Replace an \"NA\"\nniamey[5, 3] &lt;- 0\n\nniamey_df &lt;- niamey %&gt;%\n  # Rename columns to remove automatic \"V1\" etc columns names\n  rename_with(., ~ paste0(\"Site_\", str_remove(.x, \"V\"))) %&gt;%\n  # Add a column for the biweekly time period\n  mutate(biweek = 1:16) %&gt;%\n  # Convert to long format for plotting\n  pivot_longer(\n    cols = contains(\"Site\"),\n    names_to = \"site\",\n    values_to = \"cases\"\n  )\n\n\n\nCode# Create a vector of colors for each site in the Niamey dataset\nniamey_site_colors &lt;- RColorBrewer::brewer.pal(3, \"Dark2\")\n# Assign names to the colors\nnames(niamey_site_colors) &lt;- unique(niamey_df$site)\n\n# Create a vector of labels for each site for nicer plotting legends\nniamey_site_labels &lt;- str_replace_all(names(niamey_site_colors), \"_\", \" \")\nnames(niamey_site_labels) &lt;- names(niamey_site_colors)\n\n\n\nCodeggplot(\n  niamey_df,\n  aes(x = biweek, y = cases, color = site, fill = site, group = site)\n) +\n  geom_line() +\n  geom_point(shape = 21, size = 5, alpha = 0.8) +\n  scale_color_manual(\n    values = niamey_site_colors,\n    aesthetics = c(\"color\", \"fill\"),\n    labels = niamey_site_labels\n  ) +\n  guides(color = \"none\") +\n  labs(x = \"Biweek\", y = \"Number of Cases\", fill = \"Site Number\") +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#dynamical-model",
    "href": "r-session-03.html#dynamical-model",
    "title": "\n8  R Session 03\n",
    "section": "\n8.6 Dynamical Model",
    "text": "8.6 Dynamical Model\nFirst, we write a specialized function for simulating the SIR model in a case where the removal rate is “hard-wired” and with no demography.\n\nCode#' Basic SIR model\n#'\n#' A basic SIR model with no demographic structure to be used in deSolve\n#'\n#' @param time deSolve passes the time parameter to the function.\n#' @param state A vector of states.\n#' @param params The beta parameter\n#' @param ... Other arguments passed by deSolve.\n#'\n#' @return A deSolve matrix of states at each time step.\n#' @examples\n#' sir_params &lt;- 0.0005\n#' sir_init_states &lt;- c(S = 5000, I = 1, R = 0)\n#' sim_times &lt;- seq(0, 16 / 365, by = 0.1 / 365)\n#'\n#' sir_sol &lt;- deSolve::ode(\n#'    y = sir_init_states,\n#'    times = sim_times,\n#'    func = closed_sir_model,\n#'    parms = sir_params\n#' ))\nclosed_sir_model &lt;- function(time, state, params, ...) {\n  # Unpack states\n  S &lt;- state[\"S\"]\n  I &lt;- state[\"I\"]\n\n  # Unpack parameters\n  beta &lt;- params\n  dur_inf &lt;- 14 / 365\n  gamma &lt;- 1 / dur_inf\n\n  new_inf &lt;- beta * S * I\n\n  # Calculate the ODEs\n  dSdt &lt;- -new_inf\n  dIdt &lt;- new_inf - (gamma * I)\n\n  # Return the ODEs\n  return(list(c(dSdt, dIdt, new_inf)))\n}",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#interactive-optimization",
    "href": "r-session-03.html#interactive-optimization",
    "title": "\n8  R Session 03\n",
    "section": "\n8.7 Interactive Optimization",
    "text": "8.7 Interactive Optimization\n\nCodefiltered_niamey_data = aq.table(niamey_data)\n    .filter(aq.escape(d =&gt; d.site == site_select))\n\n\n\n\n\n\n\nCodereset_S = 10000\nreset_I = 20\nreset_beta = 5.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefunction set(input, value) {\n  input.value = value;\n  input.dispatchEvent(new Event(\"input\", {bubbles: true}));\n}\n\n\n\n\n\n\n\nCodefunction sse(obs, preds) {\n\n    if(obs.length == preds.length) {\n        var squared_errs = obs.map((e, i) =&gt; (e - preds[i])**2 )\n        return squared_errs.reduce((a, b) =&gt; a + b, 0)\n    } else {\n        return(\"lengths are not the same\")\n    }\n}\n\n\n\n\n\n\n\n\nCodeviewof reset = Inputs.button([\n  [\"Reset all sliders\", () =&gt; {\n    set(viewof S0, reset_S)\n    set(viewof I0, reset_I)\n    set(viewof beta_input, reset_beta)\n  }]\n])\nviewof S0 = Inputs.range(\n    [500, 15000],\n    {value: reset_S, step: 1, label: md`${tex`S(0)`}`}\n)\n\nviewof I0 = Inputs.range(\n    [0.001, 50],\n    {value: reset_I, step: 0.001, label: md`${tex`I(0)`}`}\n)\n\nviewof beta_input = Inputs.range(\n    [1, 100],\n    {value: reset_beta, step: 0.001, label: md`${tex`\\beta (\\times 10^{-3})`}`}\n)\n\nviewof site_select = Inputs.select(\n    [\"Site 1\", \"Site 2\", \"Site 3\"],\n    {label: \"Select a site:\"}\n)\n\n// convert to daily time scale as easier to manipulate\nbeta = (beta_input / 365) * (10 ** (-3))\n\nmd`${tex`R_0 = ${R0_str}`}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodedur_inf = 14\ngamma = 1 / dur_inf\nR0 = beta * (S0 + I0)/ gamma\n\nR0_str = R0.toPrecision(2).toLocaleString()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodedt = 0.01\ntmax = 16 * 14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeimport {odeRK4} from '@rreusser/integration@3064'\nimport { aq, op } from '@uwdata/arquero'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefunction sir(dydt, y, t) {\n    dydt[3] = beta * y[0] * y[1]\n\n    dydt[0] = - dydt[3]\n    dydt[1] = dydt[3] - gamma * y[1]\n    dydt[2] = gamma * y[1]\n}\n\n\n\n\n\n\n\nCodefunction simulate(f, t0, y0, dt, tmax) {\n    var t = t0\n    var y = y0\n    var i = 0\n\n    var ysim = [y0]\n\n    for (t = t0 + dt; t &lt;= tmax; t += dt) {\n        ysim.push(odeRK4([], ysim[i], f, dt))\n        i += 1\n    }\n\n    // return cumulative infections\n    return ysim.map(d =&gt; d[3])\n}\n\n\n\n\n\n\n\nCodesir_sol = simulate(sir, 0, [S0, I0, 0.0, 0.0], dt, tmax)\n\n\n\n\n\n\n\nCodesiteColors = [\"#1b9e77\", \"#d95f02\", \"#7570b3\"]\n\n\n\n\n\n\n\nCodetimes = Array.from({length: 17}, (_, i) =&gt; i * 14).slice(1)\ntindex = times.map((e, i) =&gt; e * (1 / dt))\n\ncum_inc = tindex.map((i) =&gt; sir_sol[i])\n\npreds = [cum_inc[0] + I0, ...cum_inc.map((e, i) =&gt; cum_inc[i] - cum_inc[i-1]).slice(1)]\n\nsir_tbl = aq.table({\n    Biweek: times.map(t =&gt; t / 14),\n    \"Cumulative Incidence\": cum_inc,\n    \"Number of Individuals\": preds\n})\n\nsim_sse = [({\n    sse: sse(\n            filtered_niamey_data.array(\"Number of Individuals\"),\n            preds\n        ).toPrecision(4),\n    Biweek: 3,\n    \"Number of Individuals\": Math.max(\n        ...sir_tbl.array(\"Number of Individuals\"),\n        ...filtered_niamey_data.array(\"Number of Individuals\")\n    ) * 0.9\n})]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodePlot.plot({\n    color: {\n        legend: true,\n        domain: [\"Site 1\", \"Site 2\", \"Site 3\"],\n        range: siteColors,\n    },\n    style: {fontSize: \"20px\"},\n    marginLeft: 75,\n    marginTop: 40,\n    marginBottom: 55,\n    grid: true,\n    width: 800,\n    height: 670,\n    marks: [\n        Plot.lineY(\n            sir_tbl,\n            {x: \"Biweek\", y: \"Number of Individuals\", stroke: \"#4d4d4dff\", strokeWidth: 6, strokeOpacity: 0.8}\n        ),\n        Plot.dot(\n            filtered_niamey_data,\n            {x: \"Biweek\", y: \"Number of Individuals\", stroke: \"site\", fill: \"site\", fillOpacity: 0.6, r: 12}\n        ),\n        Plot.lineY(\n            filtered_niamey_data,\n            {x: \"Biweek\", y: \"Number of Individuals\", stroke: \"site\"}\n        ),\n        Plot.text(\n            sim_sse,\n            {x: \"Biweek\", y: \"Number of Individuals\", text: (d) =&gt; `SSE = ${d.sse}`, dx: 0, dy: 0, fontWeight: \"bold\"}\n        )\n    ]\n})",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#objective-function",
    "href": "r-session-03.html#objective-function",
    "title": "\n8  R Session 03\n",
    "section": "\n8.8 Objective Function",
    "text": "8.8 Objective Function\nNow we set up a function that will calculate the sum of the squared differences between the observations and the model at any parameterization (more commonly known as “sum of squared errors”). In general, this is called the objective function because it is the quantity that optimization seeks to minimize.\n\nCode#' Calculate the Sum of Squared Errors\n#'\n#' A function to take in biweekly incidence data, and SIR parameters, and\n#' calculate the SSE\n#'\n#' @param params A vector of parameter values\n#' @param data A dataframe containing biweekly incidence data in the case column\n#'\n#' @return The SSE of type double\n#' @examples\nsse_sir &lt;- function(params, data) {\n  # Convert biweekly time series into annual time scale\n  # Daily time scale has requires beta values to be too small - doesn't\n  # optimize well\n  dt &lt;- 0.01\n  max_biweek &lt;- max(data$biweek)\n  t &lt;- seq(0, max_biweek * 14, dt) / 365\n\n  # Extract the number of observed incidence\n  obs_inc &lt;- data$cases\n\n  # Note the parameters are updated throughout the optimization process by\n  # the optim() function\n  # Unpack the transmission parameter and exponentiate to fit on ln scale\n  beta &lt;- exp(params[[\"beta\"]])\n\n  # Unpack the initial states and exponentiate to fit on normal scale\n  S_init &lt;- exp(params[[\"S_init\"]])\n  I_init &lt;- exp(params[[\"I_init\"]])\n\n  # Fit SIR model to the parameters\n  sol &lt;- deSolve::ode(\n    y = c(S = S_init, I = I_init, new_inf = 0),\n    times = t,\n    func = closed_sir_model,\n    parms = beta,\n    # Use rk4 as fixed time steps, which is important for indexing\n    method = \"rk4\"\n  )\n\n  # Extract the cumulative incidence\n  cum_inc &lt;- sol[, \"new_inf\"]\n\n  # Find the indices of the cumulative incidence to extract\n  biweek_index &lt;- seq(1, max_biweek) * (14 / dt) + 1\n\n  # Index cumulative incidence to get the values at the end of the biweeks\n  biweek_cum_inc &lt;- cum_inc[biweek_index]\n\n  # Calculate the biweekly incidence by using the difference between\n  # consecutive biweeks. Need to manually prepend first week's incidence\n  # and add in the initial number of infectious individuals, as ODE model\n  # only returns the cumulative differences, which is 0 at the start.\n  biweek_inc &lt;- c(biweek_cum_inc[1] + I_init, diff(biweek_cum_inc, lag = 1))\n\n  # return SSE of predicted vs observed incidence\n  return(sum((biweek_inc - obs_inc)^2))\n}\n\n\nNotice that the code for sse_sir() makes use of the following modeling trick. We know that \\(\\beta\\), \\(S_0\\), and \\(I_0\\) must be positive, but our search to optimize these parameters will be over the entire number line. We could constrain the search using a more sophisticated algorithm, but this might introduce other problems (i.e., stability at the boundaries). Instead, we parameterize our objective function (sse_sir) in terms of some alternative variables \\(\\ln(\\beta)\\), \\(\\ln(S_0)\\), and \\(\\ln(I_0)\\). While these numbers range from \\(-\\infty\\) to \\(\\infty\\) (the range of our search) they map to our model parameters on a range from \\(0\\) to \\(\\infty\\) (the range that is biologically meaningful).",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#optimization",
    "href": "r-session-03.html#optimization",
    "title": "\n8  R Session 03\n",
    "section": "\n8.9 Optimization",
    "text": "8.9 Optimization\nOur final step is to use the function optim to find the values of \\(\\beta\\), \\(S_0\\), and \\(I_0\\) that minimize the sum of squared errors as calculated using our function.\nFinally, we plot these fits against the data.\n\nCode# Initial guess\nsse_optim_params &lt;- c(beta = log(0.055), S_init = log(5000), I_init = log(1))\n\n# Create a dataframe of optimized parameters\nniamey_optims &lt;- niamey_df %&gt;%\n  # Create a nested dataframe i.e. one row for each site, and the data column\n  # now is a list column that contains a separate dataframe of times and\n  # cases for each site\n  nest(data = -site) %&gt;%\n  mutate(\n    # Map the optim() function call to each of the separate dataframes\n    # stored in the nested data column we just created\n    fit = map(data, ~ optim(sse_optim_params, sse_sir, data = .x)),\n    # Map the exp() function to each of the model fits just created, and\n    # output to a dataframe instead of a list (like in map()), for easier\n    # use in the plottinge predictions later\n    map_dfr(fit, ~ exp(.x$par))\n  )\n\n\n\nCodeniamey_optims %&gt;%\n  select(-c(data, fit)) %&gt;%\n  mutate(site = str_replace_all(site, \"_\", \" \")) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = -site, decimals = 3) %&gt;%\n  fmt_scientific(columns = beta, decimals = 3) %&gt;%\n  # Relabel the column headers\n  cols_label(\n    site = md(\"**Site**\"),\n    beta = md(\"**Beta**\"),\n    S_init = md(\"**Initial S**\"),\n    I_init = md(\"**Initial I**\")\n  ) %&gt;%\n  # Apply style to the table with gray alternating rows\n  opt_stylize(style = 1, color = 'gray') %&gt;%\n  # Increate space between columns\n  opt_horizontal_padding(scale = 3) %&gt;%\n  cols_align(\"center\")\n\n\n\n\n\nSite\nBeta\nInitial S\nInitial I\n\n\n\nSite 1\n5.370 × 10−3\n\n8,566.648\n1.349\n\n\nSite 2\n8.317 × 10−3\n\n5,961.388\n0.201\n\n\nSite 3\n7.134 × 10−2\n\n792.990\n0.001\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may have noticed that you can achieve slightly different results for the optimal parameter values using the interactive plot than are being presented here (though they are very similar). This is because while the optimization code is running in R, the interactive plot and the calculation of the SSE is implemented using JavaScript. Therefore, despite using the same underlying model structure, the answers will vary slightly, because the ODE solvers are different, resulting in different model simulations. The difference is not enough to be concerned with here, but it is a point that’s worth being aware of when you build your own models - you may want to perform sensitivity to confirm that your model implementation is not driving the magnitude of the results you see, and the inferences you make.\n\n\n\nCodeniamey_predictions &lt;- niamey_optims %&gt;%\n  mutate(\n    # For each of the different site's nested dataframes, fit the SIR model\n    # with the optimal parameters to get best fit predictions\n    predictions = pmap(\n      .l = list(\n        S_init = S_init,\n        I_init = I_init,\n        beta = beta,\n        time_data = data\n      ),\n      .f = function(S_init, I_init, beta, time_data) {\n        site_times &lt;- time_data$biweek * 14 / 365\n\n        # Return a dataframe of model solutions\n        as_tibble(ode(\n          y = c(S = S_init, I = I_init, new_inf = 0),\n          times = site_times,\n          func = closed_sir_model,\n          parms = beta,\n          hmax = 1 / 120\n        )) %&gt;%\n          # Make sure all values are numeric for plotting purposes\n          mutate(across(everything(), as.numeric)) %&gt;%\n          mutate(\n            incidence = ifelse(\n              row_number() == 1,\n              new_inf[1],\n              diff(new_inf, lag = 1)\n            )\n          )\n      }\n    )\n  ) %&gt;%\n  unnest(c(data, predictions))\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAn important point to note is that our data is biweekly incidence (new cases in time period), whereas out SIR model produces prevalence (total cases at any time point). To account for this, our SIR model returns the cumulative incidence (line 32 of the model code), and our objective function extracts the biweekly incidence (lines 41-54), to ensure we are fitting the same data! This is a common source of error in interpretation when people fit models to data.\n\n\n\nCode# Create a dataframe to store the positions of the text labels\nniamey_preds_labels &lt;- tibble(\n  site = c(\"Site_1\", \"Site_2\"),\n  x_label = c(6.5, 6.5),\n  x_arrow_just = c(-0.5, -0.5),\n  x_arrow_end = c(7, 7.75),\n  y_label = c(900, 600),\n  y_arrow_just = c(-80, -70),\n  y_arrow_end = c(350, 290),\n  commentary = c(\"**Predicted\", \"**Observed\"),\n  color = c(\"grey20\", niamey_site_colors[\"Site_2\"])\n)\n\nggplot(niamey_predictions, aes(x = biweek, group = site)) +\n  # Plot the actual data in color\n  geom_line(aes(y = cases, color = site)) +\n  geom_point(aes(y = cases, color = site), size = 4, alpha = 0.8) +\n  # Plot the best-fit model predictions in black\n  geom_line(aes(y = incidence), color = \"black\") +\n  scale_color_manual(\n    values = niamey_site_colors,\n    aesthetics = c(\"color\", \"fill\")\n  ) +\n  # Place each site on it's own subplot and change labels\n  facet_wrap(\n    ~site,\n    ncol = 1,\n    scales = \"free_y\",\n    labeller = as_labeller(niamey_site_labels)\n  ) +\n  labs(x = \"Biweek\", y = \"Number of Case\") +\n  theme(legend.position = \"none\") +\n  ggtext::geom_textbox(\n    data = niamey_preds_labels,\n    aes(\n      label = paste0(\n        \"&lt;span style = \\\"color:\",\n        color,\n        \"\\\"&gt;\",\n        commentary,\n        \" Cases**\",\n        \"&lt;/span&gt;\"\n      ),\n      x = x_label,\n      y = y_label\n    ),\n    size = 4,\n    fill = NA,\n    box.colour = NA\n  ) +\n  geom_curve(\n    data = niamey_preds_labels,\n    aes(\n      x = x_label + x_arrow_just,\n      xend = x_arrow_end,\n      y = y_label + y_arrow_just,\n      yend = y_arrow_end\n    ),\n    linewidth = 0.75,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    curvature = list(0.25),\n    color = \"grey20\"\n  )\n\n\n\n\n\n\n\n\n8.9.1 Exercise 5\nTo make things easier, we have assumed the infectious period is known to be 14 days. In terms of years, \\(\\text{D} = \\frac{14}{365} \\approx 0.0384\\), and the recovery rate is the inverse i.e., \\(\\gamma = \\frac{14}{365}\\). Now, modify the code above to estimate \\(\\gamma\\) and \\(\\beta\\) simultaneously.\n\n8.9.2 Exercise 6\nWhat happens if one or both of the other unknowns (\\(S_0\\) and \\(I_0\\)) is fixed instead of \\(\\gamma\\)?",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#solutions",
    "href": "r-session-03.html#solutions",
    "title": "\n8  R Session 03\n",
    "section": "\n8.10 Solutions",
    "text": "8.10 Solutions\n\n8.10.1 Exercise 1\n\nCodep_infec &lt;- (seq(0, 1, by = 0.001))\nr0_p &lt;- (log(1 - p_infec)) / (-p_infec)\nplot(\n  x = p_infec,\n  y = r0_p,\n  main = \"Relationship between final proportion infected and R0\",\n  xlab = \"Final proportion infected\",\n  ylab = \"R0\"\n)\n\n\n\n\n\n\n\n\n8.10.2 Exercise 2\n\nCode#A: If the cases were isolated after 24 hours, then gamma would be 1/1 = 1, and if the cases were isolated after 12 hours, gamma would be 1/0.5 = 2. R0 would be calculated as the beta coefficient over gamma, below:\n\nr0_g1 &lt;- 1.094913 / 1 + 1\nr0_g1\n\n[1] 2.094913\n\nCoder0_g2 &lt;- 1.094913 / 2 + 1\nr0_g2\n\n[1] 1.547457\n\n\n\n8.10.3 Exercise 3\n\nCodeniamey[5, 3] &lt;- 0 #replace a \"NA\"\n#the command below organizes the data so it can be plotted and analyzed\nniamey &lt;- data.frame(\n  biweek = rep(seq(1, 16), 3),\n  site = c(rep(1, 16), rep(2, 16), rep(3, 16)),\n  cases = c(niamey[, 1], niamey[, 2], niamey[, 3])\n) #define \"biweeks\"\n\n# As the data are reported every two weeks, this corresponds to the 10th observation. Let’s fit a linear model\n\n\n\nCode# First let's see what the outbreak looks like for the first community on a linear scale\nplot(\n  niamey$biweek[niamey$site == 1],\n  niamey$cases[niamey$site == 1],\n  type = 'p',\n  col = niamey$site,\n  xlab = 'Biweek',\n  ylab = 'Cases'\n)\nlines(niamey$biweek[niamey$site == 1], niamey$cases[niamey$site == 1])\n\n\n\n\n\n\n\n\nCode# Now let’s try it on a log scale to see until when the outbreak is roughly linear\nplot(\n  niamey$biweek[niamey$site == 1],\n  niamey$cases[niamey$site == 1],\n  type = 'p',\n  col = niamey$site,\n  xlab = 'Biweek',\n  ylab = 'Cases',\n  log = 'y'\n)\nlines(niamey$biweek[niamey$site == 1], niamey$cases[niamey$site == 1])\n\n\n\n\n\n\n\n\nCode# here we create a \"week\" variable to run the analysis on a weekly\nniamey$week &lt;- niamey$biweek * 2\n# we use the `head` command to take the first N values of a vector. In this case, we're taking the first 10 values of our outcome (cases) and predictor (time) variables.\nmodel &lt;- lm(\n  log(head(niamey$cases[niamey$site == 1], 10)) ~\n    (head(niamey$week[niamey$site == 1], 10))\n)\nsummary(model) #summary statistics for fit model\n\n\nCall:\nlm(formula = log(head(niamey$cases[niamey$site == 1], 10)) ~ \n    (head(niamey$week[niamey$site == 1], 10)))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.51796 -0.07364  0.00790  0.10727  0.36805 \n\nCoefficients:\n                                        Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                              2.59679    0.17563   14.79 4.31e-07\nhead(niamey$week[niamey$site == 1], 10)  0.21960    0.01415   15.52 2.96e-07\n                                           \n(Intercept)                             ***\nhead(niamey$week[niamey$site == 1], 10) ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2571 on 8 degrees of freedom\nMultiple R-squared:  0.9678,    Adjusted R-squared:  0.9638 \nF-statistic: 240.8 on 1 and 8 DF,  p-value: 2.963e-07\n\n\n\nCode# Now let's get the slope and display it\nslope &lt;- coef(model)[2] #extract slope parameter\nslope #print to screen\n\nhead(niamey$week[niamey$site == 1], 10) \n                              0.2196041 \n\n\nAs we ran the model by weeks, the \\(\\gamma\\) value is \\(2^{-1}\\) and \\(\\hat R_0 = \\hat \\beta_1 / \\gamma +1\\) giving \\(\\hat R_0=0.2196041/0.5+1 \\approx 1.44\\).\n\n8.10.4 Exercise 4\nHere, we can use a loop and repeat the regression procedure we used above for varying numbers of initial data points in our model.\n\nCodeslope &lt;- NULL\nse &lt;- NULL\nfor (i in 3:18) {\n  model &lt;- lm(\n    log(head(niamey$cases[niamey$site == 1], i)) ~\n      (head(niamey$week[niamey$site == 1], i))\n  )\n  slope &lt;- c(slope, as.numeric(coef(model)[2]))\n  se &lt;- c(se, summary(model)$coefficients[4])\n}\nR0 &lt;- slope / 0.5 + 1\nplot(R0, se, ylab = 'Standard error')\n\n\n\n\n\n\n\n\n8.10.5 Exercise 5\nFirst, let’s add in \\(\\gamma\\) estimation into the sse.sir function and create a new sse.sir function called sse.sir.g\n\nCode#' Basic SIR model\n#'\n#' A basic SIR model with no demographic structure to be used in deSolve\n#'\n#' @param time deSolve passes the time parameter to the function.\n#' @param state A vector of states.\n#' @param params The beta parameter\n#' @param ... Other arguments passed by deSolve.\n#'\n#' @return A deSolve matrix of states at each time step.\n#' @examples\n#' sir_params &lt;- 0.0005\n#' sir_init_states &lt;- c(S = 5000, I = 1, R = 0)\n#' sim_times &lt;- seq(0, 16 / 365, by = 0.1 / 365)\n#'\n#' sir_sol &lt;- deSolve::ode(\n#'    y = sir_init_states,\n#'    times = sim_times,\n#'    func = closed_sir_model,\n#'    parms = sir_params\n#' )\n\n# Create a new SIR model to include gamma\nclosed_sir_model_g &lt;- function(time, state, params, ...) {\n  # Unpack states\n  S &lt;- state[\"S\"]\n  I &lt;- state[\"I\"]\n\n  # Unpack parameters\n  beta &lt;- params[[\"beta\"]]\n  dur_inf &lt;- 14 / 365\n  gamma &lt;- params[[\"gamma\"]]\n\n  new_inf &lt;- beta * S * I\n\n  # Calculate the ODEs\n  dSdt &lt;- -new_inf\n  dIdt &lt;- new_inf - (gamma * I)\n\n  # Return the ODEs\n  return(list(c(dSdt, dIdt, new_inf)))\n}\n\n\n#' Calculate the Sum of Squared Errors\n#'\n#' A function to take in biweekly incidence data, and SIR parameters, and\n#' calculate the SSE\n#'\n#' @param params A vector of parameter values\n#' @param data A dataframe containing biweekly incidence data in the case column\n#'\n#' @return The SSE of type double\n#' @examples\nsse_sir_g &lt;- function(params, data) {\n  # Convert biweekly time series into annual time scale\n  # Daily time scale has requires beta values to be too small - doesn't\n  # optimize well\n  dt &lt;- 0.01\n  max_biweek &lt;- max(data$biweek)\n  t &lt;- seq(0, max_biweek * 14, dt) / 365\n\n  # Extract the number of observed incidence\n  obs_inc &lt;- data$cases\n\n  # Note the parameters are updated throughout the optimization process by\n  # the optim() function\n  # Unpack the transmission parameter and exponentiate to fit on ln scale\n  beta &lt;- exp(params[[\"beta\"]])\n  gamma &lt;- exp(params[[\"gamma\"]])\n\n  in_parms &lt;- c(\"beta\" = beta, \"gamma\" = gamma)\n  # Unpack the initial states and exponentiate to fit on normal scale\n  S_init &lt;- exp(params[[\"S_init\"]])\n  I_init &lt;- exp(params[[\"I_init\"]])\n\n  # Fit SIR model to the parameters\n  sol &lt;- deSolve::ode(\n    y = c(S = S_init, I = I_init, new_inf = 0),\n    times = t,\n    func = closed_sir_model_g,\n    parms = in_parms,\n    # Use rk4 as fixed time steps, which is important for indexing\n    method = \"rk4\"\n  )\n\n  # Extract the cumulative incidence\n  cum_inc &lt;- sol[, \"new_inf\"]\n\n  # Find the indices of the cumulative incidence to extract\n  biweek_index &lt;- seq(1, max_biweek) * (14 / dt) + 1\n\n  # Index cumulative incidence to get the values at the end of the biweeks\n  biweek_cum_inc &lt;- cum_inc[biweek_index]\n\n  # Calculate the biweekly incidence by using the difference between\n  # consecutive biweeks. Need to manually prepend first week's incidence\n  # and add in the initial number of infectious individuals, as ODE model\n  # only returns the cumulative differences, which is 0 at the start.\n  biweek_inc &lt;- c(biweek_cum_inc[1] + I_init, diff(biweek_cum_inc, lag = 1))\n\n  # return SSE of predicted vs observed incidence\n  return(sum((biweek_inc - obs_inc)^2))\n}\n\n\nNow we can run the code to optimize, beta, gamma, I0, and S0\n\nCodesse_optim_params_g &lt;- c(\n  beta = log(0.055),\n  gamma = log(365 / 14),\n  S_init = log(5000),\n  I_init = log(1)\n)\n\n# Create a dataframe of optimized parameters\nniamey_optims_g &lt;- niamey_df %&gt;%\n  # Create a nested dataframe i.e. one row for each site, and the data column\n  # now is a list column that contains a separate dataframe of times and\n  # cases for each site\n  nest(data = -site) %&gt;%\n  mutate(\n    # Map the optim() function call to each of the separate dataframes\n    # stored in the nested data column we just created\n    fit = map(data, ~ optim(sse_optim_params_g, sse_sir_g, data = .x)),\n    # Map the exp() function to each of the model fits just created, and\n    # output to a dataframe instead of a list (like in map()), for easier\n    # use in the plottinge predictions later\n    map_dfr(fit, ~ exp(.x$par))\n  )\n\n\nNow we can get our optimized parameters\n\nCodeniamey_optims_g %&gt;%\n  select(-c(data, fit)) %&gt;%\n  mutate(site = str_replace_all(site, \"_\", \" \")) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = -site, decimals = 3) %&gt;%\n  fmt_scientific(columns = beta, decimals = 3) %&gt;%\n  # Relabel the column headers\n  cols_label(\n    site = md(\"**Site**\"),\n    beta = md(\"**Beta**\"),\n    gamma = md(\"**Gamma**\"),\n    S_init = md(\"**Initial S**\"),\n    I_init = md(\"**Initial I**\")\n  ) %&gt;%\n  # Apply style to the table with gray alternating rows\n  opt_stylize(style = 1, color = 'gray') %&gt;%\n  # Increate space between columns\n  opt_horizontal_padding(scale = 3) %&gt;%\n  cols_align(\"center\")\n\n\n\n\n\nSite\nBeta\nGamma\nInitial S\nInitial I\n\n\n\nSite 1\n5.854 × 10−3\n\n81.150\n17,113.450\n0.786\n\n\nSite 2\n9.066 × 10−3\n\n77.613\n11,030.375\n0.142\n\n\nSite 3\n8.517 × 10−2\n\n351.697\n4,462.286\n0.000\n\n\n\n\n\n\nGetting new predictions from the optimized parameters and a new model run for each site.\n\nCodeniamey_predictions_g &lt;- niamey_optims_g %&gt;%\n  mutate(\n    # For each of the different site's nested dataframes, fit the SIR model\n    # with the optimal parameters to get best fit predictions\n    predictions = pmap(\n      .l = list(\n        S_init = S_init,\n        I_init = I_init,\n        beta = beta,\n        gamma = gamma,\n        time_data = data\n      ),\n      .f = function(S_init, I_init, beta, gamma, time_data) {\n        site_times &lt;- time_data$biweek * 14 / 365\n\n        in_parms &lt;- c(\"beta\" = beta, \"gamma\" = gamma)\n        # Return a dataframe of model solutions\n        as_tibble(ode(\n          y = c(S = S_init, I = I_init, new_inf = 0),\n          times = site_times,\n          func = closed_sir_model_g,\n          parms = in_parms,\n          hmax = 1 / 120\n        )) %&gt;%\n          # Make sure all values are numeric for plotting purposes\n          mutate(across(everything(), as.numeric)) %&gt;%\n          mutate(\n            incidence = ifelse(\n              row_number() == 1,\n              new_inf[1],\n              diff(new_inf, lag = 1)\n            )\n          )\n      }\n    )\n  ) %&gt;%\n  unnest(c(data, predictions))\n\n\nFinally we can plot:\n\nCode# Create a dataframe to store the positions of the text labels\nniamey_preds_labels &lt;- tibble(\n  site = c(\"Site_1\", \"Site_2\"),\n  x_label = c(6.5, 6.5),\n  x_arrow_just = c(-0.5, -0.5),\n  x_arrow_end = c(7, 7.75),\n  y_label = c(900, 600),\n  y_arrow_just = c(-80, -70),\n  y_arrow_end = c(350, 290),\n  commentary = c(\"**Predicted\", \"**Observed\"),\n  color = c(\"grey20\", niamey_site_colors[\"Site_2\"])\n)\n\nggplot(niamey_predictions_g, aes(x = biweek, group = site)) +\n  # Plot the actual data in color\n  geom_line(aes(y = cases, color = site)) +\n  geom_point(aes(y = cases, color = site), size = 4, alpha = 0.8) +\n  # Plot the best-fit model predictions in black\n  geom_line(aes(y = incidence), color = \"black\") +\n  scale_color_manual(\n    values = niamey_site_colors,\n    aesthetics = c(\"color\", \"fill\")\n  ) +\n  # Place each site on it's own subplot and change labels\n  facet_wrap(\n    ~site,\n    ncol = 1,\n    scales = \"free_y\",\n    labeller = as_labeller(niamey_site_labels)\n  ) +\n  labs(x = \"Biweek\", y = \"Number of Case\") +\n  theme(legend.position = \"none\") +\n  ggtext::geom_textbox(\n    data = niamey_preds_labels,\n    aes(\n      label = paste0(\n        \"&lt;span style = \\\"color:\",\n        color,\n        \"\\\"&gt;\",\n        commentary,\n        \" Cases**\",\n        \"&lt;/span&gt;\"\n      ),\n      x = x_label,\n      y = y_label\n    ),\n    size = 4,\n    fill = NA,\n    box.colour = NA\n  ) +\n  geom_curve(\n    data = niamey_preds_labels,\n    aes(\n      x = x_label + x_arrow_just,\n      xend = x_arrow_end,\n      y = y_label + y_arrow_just,\n      yend = y_arrow_end\n    ),\n    linewidth = 0.75,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    curvature = list(0.25),\n    color = \"grey20\"\n  )\n\n\n\n\n\n\n\n\n8.10.6 Exercise 6\nWhat happens if one or both of the other unknowns (\\(X_0\\) and \\(Y_0\\)) is fixed instead of \\(\\gamma\\)?\n\n8.10.6.1 Solutions coming soon!\nFirst we modify the sse_sir_g function again to fix \\(X_0\\) and \\(Y_0\\) (or both)\nNow we can run the optim algorithm and find the best parameters for the SIR model for each of the scenario.\nFirst, for S0 fixed:\nSecond, for I0 fixed:\nThird, for fixing both S0 and I0:\nNow let’s pass these models through optim and fit the models:\n\n\n\n\n\n\nDrake, John M, and Pejman Rohani. 2019. “Estimation.” In. Seattle, Washington. http://daphnia.ecology.uga.edu/drakelab/wp-content/uploads/2019/07/estimation.pdf.\n\n\nGrais, R. F., M. J. Ferrari, C. Dubray, O. N. Bjørnstad, B. T. Grenfell, A. Djibo, F. Fermon, and P. J. Guerin. 2006. “Estimating Transmission Intensity for a Measles Epidemic in Niamey, Niger: Lessons for Intervention.” Transactions of The Royal Society of Tropical Medicine and Hygiene 100 (9): 867–73. https://doi.org/10.1016/j.trstmh.2005.10.014.\n\n\nKeeling, Matthew James, and Pejman Rohani. 2008. “Introduction to Simple Epidemic Models.” In Modeling Infectious Diseases in Humans and Animals, 21–22. Princeton: Princeton University Press.",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "L08_stochastic-models.html",
    "href": "L08_stochastic-models.html",
    "title": "9  Stochastic Models",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Stochastic Models</span>"
    ]
  },
  {
    "objectID": "r-session-04.html",
    "href": "r-session-04.html",
    "title": "\n10  R Session 04\n",
    "section": "",
    "text": "10.1 Setup\nCodelibrary(here)\nlibrary(rio)\nlibrary(deSolve)\nlibrary(tidyverse)\nCodetheme_set(theme_minimal())",
    "crumbs": [
      "Day 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>R Session 04</span>"
    ]
  },
  {
    "objectID": "r-session-04.html#setup",
    "href": "r-session-04.html#setup",
    "title": "\n10  R Session 04\n",
    "section": "",
    "text": "Note\n\n\n\nThis R-session will go in parallel with the lecture on stochastic algorithms. First we’ll go through the basics of each algorithm in the lecture and then we’ll walk through the code and you can implement it for yourselves.",
    "crumbs": [
      "Day 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>R Session 04</span>"
    ]
  },
  {
    "objectID": "r-session-04.html#what-is-stochasticity-and-where-does-it-come-from",
    "href": "r-session-04.html#what-is-stochasticity-and-where-does-it-come-from",
    "title": "\n10  R Session 04\n",
    "section": "\n10.2 What is stochasticity and where does it come from?",
    "text": "10.2 What is stochasticity and where does it come from?\nMuch of the world is uncertain (i.e. we don’t know exactly how things work, what values are, or what tomorrow will hold). Some of that uncertainty is, at least theoretically, knowable and some is not. For example, in our discussion of estimating \\(R_0\\), there may be a very real \\(R_0\\) for a given population and pathogen, even if we don’t know it. Thus, our estimate of \\(R_0\\) may be “uncertain” (e.g. has a confidence interval around it, reflecting our certainty), but the models we’ve been developing so far are “deterministic”, so conditional on a given value of \\(R_0\\) the resulting epidemic curve is exactly specified by the model. If we return to the code from R-session 1, we can plot a single deterministic realization of a model with \\(R_0 = 1.8\\).\n\nCodesir_model &lt;- function(time, state, params, ...) {\n  transmission &lt;- params[\"transmission\"]\n  recovery &lt;- 1 / params[\"duration\"]\n\n  S &lt;- state[\"S\"]\n  I &lt;- state[\"I\"]\n  R &lt;- state[\"R\"]\n\n  dSdt &lt;- -transmission * S * I\n  dIdt &lt;- (transmission * S * I) - (recovery * I)\n  dRdt &lt;- recovery * I\n\n  return(list(c(dSdt, dIdt, dRdt)))\n}\n\n\n\nCodesir_params &lt;- c(transmission = 0.3, duration = 6)\nsir_init_states &lt;- c(S = 0.99, I = 0.01, R = 0)\nsim_times &lt;- seq(0, 200, by = 0.1)\n\nsir_sol &lt;- ode(\n  y = sir_init_states,\n  times = sim_times,\n  func = sir_model,\n  parms = sir_params\n)\n\n\n\nCode# Turn the output from the ODE solver into a tibble (dataframe)\n# so we can manipulate and plot it easily\nsir_sol_df &lt;- as_tibble(sir_sol) %&gt;%\n  # Convert all columns to numeric (they are currently type\n  # deSolve so will produce warnings when plotting etc)\n  mutate(\n    # Rather than repeatedly type the same function for every\n    # column, use the across() function to apply the function\n    # to a selection of columns\n    across(\n      # The cols argument takes a selection of columns to apply\n      # a function to. Here, we want to apply the as.numeric()\n      # function to all columns, so we use the function\n      # everything() to select all columns.\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  ) %&gt;%\n  # Convert the dataframe from wide to long format, so we have a\n  # column for the time, a column for the state, and a column\n  # for the proportion of the population in that state at that\n  # time\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -time,\n    names_to = \"state\",\n    values_to = \"proportion\"\n  ) %&gt;%\n  # Update the state column to be a factor, so the plot will\n  # show the states in the correct order\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\")))\n\nsir_colors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\")\n\nggplot(sir_sol_df, aes(x = time, y = proportion, color = state)) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(values = sir_colors) +\n  labs(\n    x = \"Time\",\n    y = \"Fraction\",\n    color = \"State\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nNow, perhaps we used the tools from R-session 3 to estimate \\(R_0\\) and we think a reasonable \\(95\\%\\) confidence interval for \\(R_0\\) is \\((1.7,1.9)\\). If we’re quite certain the duration of infection is 6 days, then that means our corresponding confidence interval on the transmission rate is \\((.283,.317)\\). An entirely reasonable way to represent this uncertainty in the estimate of the transmission rate is to generate many random draws from within the confidence interval for the transmission rate and examine the resulting epidemic curves. For this we can modify the code above with a loop that does this multiple times.\n\nCode# the number of times we want to do this.\n# This is a matter of choice and computational capacity\n# (this model is small and quick, but that won't always be the case)\nnum_iterations &lt;- 100\n\n# since we're going to do this num_iterations times we need a place\n# to store the results\n\nfor (iter in 1:num_iterations) {\n  # each time take a random draw of the transmission rate from the\n  # confidence interval\n  sir_params &lt;- c(transmission = runif(1, .283, .317), duration = 6)\n  sir_init_states &lt;- c(S = 0.99, I = 0.01, R = 0)\n  sim_times &lt;- seq(0, 200, by = 0.1)\n\n  sir_sol &lt;- ode(\n    y = sir_init_states,\n    times = sim_times,\n    func = sir_model,\n    parms = sir_params\n  )\n  sir_sol &lt;- as_tibble(sir_sol)\n  # mark this as the iter iteration of the loop\n  sir_sol$iteration &lt;- iter\n  # if this is the first iteration, create a place to store the output\n  # if it's the second or higher, append the output to the storage\n  if (iter == 1) {\n    sir_sol_storage &lt;- sir_sol\n  }\n  if (iter &gt; 1) {\n    sir_sol_storage &lt;- rbind(sir_sol_storage, sir_sol)\n  }\n}\n\n\n\nCode# Turn the output from the ODE solver into a tibble (dataframe)\n# so we can manipulate and plot it easily\nsir_sol_df &lt;- as_tibble(sir_sol_storage) %&gt;%\n  # Convert all columns to numeric (they are currently type deSolve\n  # so will produce warnings when plotting etc)\n  mutate(\n    # Rather than repeatedly type the same function for every column,\n    # use the across() function to apply the function to a selection\n    # of columns\n    across(\n      # The cols argument takes a selection of columns to apply\n      # a function to. Here, we want to apply the as.numeric()\n      # function to all columns, so we use the function\n      # everything() to select all columns.\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  ) %&gt;%\n  # Convert the dataframe from wide to long format, so we have a\n  # column for the time, a column for the state, and a column\n  # for the proportion of the population in that state at that\n  # time\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -c(time, iteration),\n    names_to = \"state\",\n    values_to = \"proportion\"\n  ) %&gt;%\n  # Update the state column to be a factor, so the plot will\n  # show the states in the correct order\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\")))\n\nsir_colors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\")\n\nggplot(\n  sir_sol_df,\n  aes(\n    x = time,\n    y = proportion,\n    color = state,\n    group = interaction(iteration, state)\n  )\n) +\n  geom_line(linewidth = 1.5, alpha = .1) +\n  scale_color_manual(values = sir_colors) +\n  labs(\n    x = \"Time\",\n    y = \"Fraction\",\n    color = \"State\"\n  ) +\n  guides(color = guide_legend(override.aes = list(alpha = 1))) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nHere note that we are still using our original code for the SIR model, so first we randomly draw the transmission rate, then conditional on that value, we run the deterministic model. Note that each run of the model still has the smooth curves, but each draw is a different set of smooth curves.\nThere are lots of ways you could extend this. Note that since we are drawing the transmission rate and fixing the duration of infection, the \\(R_0\\) is slightly different in each simulation. You could make each run have the same \\(R_0\\) by recalculating the duration of infection based on the random draw of transmission rate; e.g. \\(R_0=1.8 = 0.29 * L\\) means that \\(L=5.5\\), so to ensure \\(R_0\\) is the same in each run, you would need to change \\(L\\) for each random draw. Alternatively, you might have uncertainty about transmission and duration, so could for e.g. make random draws for both (which would again give a setting where \\(R_0\\) varies from run to run). None of these are more correct than another, the use case depends on which elements \\(R_0\\), transmission, duration of infection, you are uncertain about.\nIn each of the above, the model is , meaning that for a given set of parameters, the resulting outbreak trajectory is always the same. For a model, each run of the model will vary, even if the parameters are the same. This is because each event that occurs (e.g. someone getting infected or recovering) is analogous to a coin flip; even though the rules of the coin (the parameters) are always the same, the side it lands on is random. Unlike a deterministic model, to fully understand the behavior of a stochastic model, you need to run it more than one time, often many times, so these are necessarily more time consuming to work with.\nThere are many, many ways to make stochastic models and the steps can be way more complicated than flipping coins. But there some foundational versions of these models that illustrate the trade-offs between exact representations of the random processes we think are happening and the computational time it takes to generate outputs. For what we’ll do here, everything will be kind of fast, but in practice, for models of realistic scale, even a single stochastic run can take a while. And if you have to do thousands of runs, even shaving a few seconds or minutes can be important.",
    "crumbs": [
      "Day 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>R Session 04</span>"
    ]
  },
  {
    "objectID": "r-session-04.html#the-gillespie-algorithm",
    "href": "r-session-04.html#the-gillespie-algorithm",
    "title": "\n10  R Session 04\n",
    "section": "\n10.3 The Gillespie Algorithm",
    "text": "10.3 The Gillespie Algorithm\nThe Gillespie algorithm is the most explicit translation of the ODE form of the SIR model into a stochastic model. It achieves this by noting that ODE-based models are written in terms of the rates at which events occur. At any given point in time, the rate of all events in the ODE is known; what known, is which of the possible events will happen first. Note that even though we expect that the next thing to happen will be the thing that happens with the highest rate, it’s possible that another thing may happen first by random chance. And then, since the rates in the SIR model are dependent on the value of the states (e.g. new infections depend on \\(\\beta\\) and S and I) any change in the states then changes the rates and the likelihood of what will happen next.\nThe Gillespie algorithm proceeds by 1) calculating all the current rates, 2) randomly drawing exponential random variables that equate to the time until each event happens (recall we talked about the relationship between rate and time in the lectures), then 3) comparing those times and assuming that the next event to occur is the one that had the smallest randomly drawn time. Then you increment the states; e.g. an infection increases I by 1, reduces S by 1, and doesn’t change R. Importantly, you then increment time forward by a step equal to the time until first event occurred. Then you recalculate the rates and randomly draw times, etc, and keep doing this over and over again. For this SIR model, that keeps happening until you run out of infected individuals and there are no new events that can happen.\n\nCode####################################################################\n# Parameters and initial conditions                                #\n####################################################################\n\nS &lt;- 998 # number susceptible\nI &lt;- 1 # number infected\nR &lt;- 1 # number recovered\ntime &lt;- 0\n\nbeta &lt;- 0.5 # transmission rate\ngamma &lt;- 1 / 7 # recovery rate\n\n\n\nCode####################################################################\n# Gillespie Step                                                   #\n####################################################################\ngil_step &lt;- function(SIR, beta, gamma) {\n  # SIR is a vector containing 4 elements\n  #   S = scalar number of susceptibles now\n  #   I = scalar number of infecteds now\n  #   R = scalar number of recovereds now\n  #   time = current time\n  # beta = transmission rate\n  # gamma = recovery rate\n\n  # draw two random exponential variables\n  times &lt;- rexp(\n    2,\n    c(\n      # the first is the rate of new infections: S*Beta*I/N\n      beta * SIR[1] * SIR[2] / sum(SIR[1:3]),\n      # the second is the rate of new recoveries: I*gamma\n      SIR[2] * gamma\n    )\n  )\n\n  return(list(\n    # which.min identifies which of the two random variables is smallest,\n    # and thus the first to happen: this is the transition that we'll make\n    change_state = which.min(times),\n    # this identifies how much time elapsed before the transition occurred,\n    # so we can increment time forward\n    time_step = min(times)\n  ))\n}\n\n\nThe gil_step() function increments the states forward by 1 event. Which event happens first, and the time it takes for that event to happen are both random variables. We then need to do this many times.\n\nCode####################################################################\n# Simulate over time                                               #\n####################################################################\n# here we set the random seed. This isn't necessary in general, but it allows us to write a \"random\" simulation\nset.seed(101)\ncounter &lt;- 0 # set counter at 0\nwhile (all(I &gt; 0)) {\n  # continue until I is depleted\n  counter &lt;- counter + 1\n  # counter for number of transitions: we don't know how many transitions\n  # will happen until the simulation is over\n  #\n  # current SIR states\n  sir_tmp &lt;- c(S[counter], I[counter], R[counter], time[counter])\n  step &lt;- gil_step(sir_tmp, beta, gamma)\n  if (step$change_state == 1) {\n    # if transition is an infection, reduce S and increase I\n    sir_tmp[1] &lt;- sir_tmp[1] - 1\n    sir_tmp[2] &lt;- sir_tmp[2] + 1\n  }\n  if (step$change_state == 2) {\n    # if transition is an recovery, reduce I and increase R\n    sir_tmp[2] &lt;- sir_tmp[2] - 1\n    sir_tmp[3] &lt;- sir_tmp[3] + 1\n  }\n  sir_tmp[4] &lt;- sir_tmp[4] + step$time_step # increment time\n\n  # Append changes\n  S &lt;- c(S, sir_tmp[1])\n  I &lt;- c(I, sir_tmp[2])\n  R &lt;- c(R, sir_tmp[3])\n  time &lt;- c(time, sir_tmp[4])\n\n  # cat(S[counter],\\\"-\\\",I[counter],\\\"-\\\",R[counter],\\\"-\\\",time[counter], \\\".\\\\n\\\")\n  # this prints the output as it goes\n  # reset the seed so that every subsequent simulation IS random\n  set.seed(NULL)\n}\n\n\nWe can now plot the one realization of this stochastic outbreak. Notice that it has the same general shape as the deterministic outbreak, but is no longer smooth because each individual event over time happens randomly.\n\nCode# Reuse the plotting code from above\n\n# Turn the output from gil_step() into a tibble (dataframe)\n# so we can manipulate and plot it easily\n# put elements in a data frame\ngil_df &lt;- tibble(time, S, I, R) %&gt;%\n  # Convert all columns to numeric (they are currently type\n  # deSolve so will produce warnings when plotting etc)\n  mutate(\n    # Rather than repeatedly type the same function for every\n    # column, use the across() function to apply the function\n    # to a selection of columns\n    across(\n      # The cols argument takes a selection of columns to apply\n      # a function to. Here, we want to apply the as.numeric()\n      # function to all columns, so we use the function\n      # everything() to select all columns.\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  ) %&gt;%\n  # Convert the dataframe from wide to long format, so we have a\n  # column for the time, a column for the state, and a column\n  # for the proportion of the population in that state at that\n  # time\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -c(time),\n    names_to = \"state\",\n    values_to = \"number\"\n  ) %&gt;%\n  # Update the state column to be a factor, so the plot will\n  # show the states in the correct order\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\")))\n\nsir_colors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\")\n\nggplot(gil_df, aes(x = time, y = number, color = state)) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(values = sir_colors) +\n  labs(\n    x = \"Time\",\n    y = \"Number\",\n    color = \"State\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nBecause each realization is stochastic, we need to generate many runs to see the general behavior. The code below runs 10 iterations. Generally, this would be considered a very small number of iterations. But even for this very small model, running 100 or more means you’ll be waiting for output. Note that this code is designed to be transparent not to be fast. Making these run fast is beyond the scope of this assignment.\n\nCode####################################################################\n# Simulate over time                                               #\nnum_iterations &lt;- 10 # number of realizations to simulate\n\nfor (iter in 1:num_iterations) {\n  ####################################################################\n  # Parameters and initial conditions                                #\n  ####################################################################\n\n  S &lt;- 998 # number susceptible\n  I &lt;- 1 # number infected\n  R &lt;- 1 # number recovered\n  time &lt;- 0\n  # initialize iteration counter\n  iteration &lt;- iter\n\n  beta &lt;- .5 # transmission rate\n  gamma &lt;- 1 / 7 # recovery rate\n\n  ####################################################################\n  # Simulate over time                                               #\n  ####################################################################\n  counter &lt;- 0 # set counter at 0\n  while (all(I &gt; 0)) {\n    # continue until I is depleted\n    counter &lt;- counter + 1\n    # counter for number of transitions: we don't know how many transitions\n    # will happen until the simulation is over\n\n    # current SIR states\n    sir_tmp &lt;- c(S[counter], I[counter], R[counter], time[counter])\n    step &lt;- gil_step(sir_tmp, beta, gamma)\n    if (step$change_state == 1) {\n      # if transition is an infection, reduce S and increase I\n      sir_tmp[1] &lt;- sir_tmp[1] - 1\n      sir_tmp[2] &lt;- sir_tmp[2] + 1\n    }\n    if (step$change_state == 2) {\n      # if transition is an recovery, reduce I and increase R\n      sir_tmp[2] &lt;- sir_tmp[2] - 1\n      sir_tmp[3] &lt;- sir_tmp[3] + 1\n    }\n    # increment time\n    sir_tmp[4] &lt;- sir_tmp[4] + step$time_step\n\n    # Append changes\n    S &lt;- c(S, sir_tmp[1])\n    I &lt;- c(I, sir_tmp[2])\n    R &lt;- c(R, sir_tmp[3])\n    time &lt;- c(time, sir_tmp[4])\n    iteration &lt;- c(iteration, iter)\n\n    # cat(S[counter],\\\"-\\\",I[counter],\\\"-\\\",R[counter],\\\"-\\\",time[counter], \\\".\\\\n\\\")\n    # this prints the output as it goes\n  }\n  if (iter == 1) {\n    sir_gil_storage &lt;- tibble(S, I, R, time, iteration)\n  }\n  if (iter &gt; 1) {\n    sir_gil_storage &lt;- bind_rows(\n      sir_gil_storage,\n      tibble(S, I, R, time, iteration)\n    )\n  }\n}\n\n\n\nCode# Reuse the plotting code from above\n\n# Turn the output from gil_step() into a tibble (dataframe)\n# so we can manipulate and plot it easily\n# put elements in a data frame\ngil_df &lt;- sir_gil_storage %&gt;%\n  # Convert all columns to numeric (they are currently type\n  # deSolve so will produce warnings when plotting etc)\n  mutate(\n    # Rather than repeatedly type the same function for every\n    # column, use the across() function to apply the function\n    # to a selection of columns\n    across(\n      # The cols argument takes a selection of columns to apply\n      # a function to. Here, we want to apply the as.numeric()\n      # function to all columns, so we use the function\n      # everything() to select all columns.\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  ) %&gt;%\n  # Convert the dataframe from wide to long format, so we have a\n  # column for the time, a column for the state, and a column\n  # for the proportion of the population in that state at that\n  # time\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -c(time, iteration),\n    names_to = \"state\",\n    values_to = \"number\"\n  ) %&gt;%\n  # Update the state column to be a factor, so the plot will\n  # show the states in the correct order\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\")))\n\nsir_colors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\")\n\nggplot(\n  gil_df,\n  aes(\n    x = time,\n    y = number,\n    color = state,\n    group = interaction(iteration, state)\n  )\n) +\n  geom_line(linewidth = 1.5, alpha = .1) +\n  scale_color_manual(values = sir_colors) +\n  labs(\n    x = \"Time\",\n    y = \"Number\",\n    color = \"State\"\n  ) +\n  guides(color = guide_legend(override.aes = list(alpha = 1))) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nThe Gillespie algorithm doesn’t cut any corners relative to the ODE model, but that comes at the cost of computational efficiency. For every event that happens (e.g. an infection) you also have to generate a random draw (e.g. a recovery) that you don’t use, except for comparison. And the bigger your population, the more possible events can happen. So the bigger the model (e.g. adding exposed or vaccinated classes, or heterogeneity, add transitions) and the bigger the population, the more calculation you need and and therefore the slower the model.",
    "crumbs": [
      "Day 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>R Session 04</span>"
    ]
  },
  {
    "objectID": "r-session-04.html#the-tau-leaping-algorithm",
    "href": "r-session-04.html#the-tau-leaping-algorithm",
    "title": "\n10  R Session 04\n",
    "section": "\n10.4 The Tau Leaping Algorithm",
    "text": "10.4 The Tau Leaping Algorithm\nThe Gillespie algorithm is great because it is an exact interpretation of the transitions in the ODE model: every change of state (e.g. infection or recovery) changes the rates for the next transition. Doing this comes at a computational cost. One reasonable approximation is the Tau Leaping algorithm. Here, we move in discrete chunks of time and make random draws for multiple events occurring within that chunk. Here the computation scales with the number of time steps (and the number of states requiring transitions). But, we rely on the result that, if rates stay constant, the number of events that occur in a discrete chunk of time can be approximated by a Poisson random variable. Thus, we don’t need to make random draws for every event. Instead we can make 1 draw for the multiple events that will occur in 1 day, or 1 week, etc. The key here is the assumption that the rates stay constant; since each new infection or recovery will change the rates, we don’t want to occur (in which case the rate at the start of the time step will be very different than the rate at the end of the time step). So we are faced with a trade-off; very small time steps don’t violate the assumptions, but the smaller the steps, the closer we are to Gillespie and the smaller the computational savings. There’s no right answer to what time step to use. Here we’ll use 1 day, for convenience.\n\nCode####################################################################\n# Parameters and initial conditions                                #\n####################################################################\n\nS &lt;- 998 # number susceptible\nI &lt;- 1 # number infected\nR &lt;- 1 # number recovered\ntime &lt;- 0\n\nbeta &lt;- .5 # transmission rate\ngamma &lt;- 1 / 7 # recovery rate\n\n\n\nCode####################################################################\n# Tau Leaping Single time step                                     #\n####################################################################\ntau_sir_step &lt;- function(sims, S, I, R, beta, gamma, delta_t, ...) {\n  # adapted from Aaron King's code\n  # sims = number of simulations\n  # S = initial susceptible population\n  # I = initial infected population\n  # R = initial recovered population\n  # beta = transmission rate\n  # gamma = recovery rate\n\n  # total population size\n  N &lt;- S + I + R\n  # new incident infections\n  dSI &lt;- rpois(n = sims, beta * S * (I / N) * delta_t)\n  # recoveries\n  dIR &lt;- rpois(n = sims, gamma * I * delta_t)\n  # note that this can be done with a binomial step as well\n  # dSI &lt;- rbinom(n=sims,size=S,prob=1-exp(-beta*(I/N)*delta_t))\n  # new incident infections\n  # dIR &lt;- rbinom(n=sims,size=I,prob=1-exp(-gamma*delta_t))\n  # recoveries\n\n  # since it is possible for the transitions to drive the states negative,\n  # we have to prevent that\n  # change in S\n  S &lt;- pmax(S - dSI, 0)\n  # change in I\n  I &lt;- pmax(I + dSI - dIR, 0)\n  # change in R\n  R &lt;- R + dIR\n  # note that dSI are the new incident infections\n  cbind(S, I, R, dSI)\n}\n\n\nNote in the code above that we’re taking Poisson random draws, but there is some code commented out that uses binomial draws. The former is exact for the theory, but it can give rise to settings where the transitions “get ahead of themselves” and you have more recoveries than infecteds, or more infections than susceptibles, which drives the states negative. We can fix this by checking if the states go negative and disallowing this … which is inelegant. We can also fix this by using binomial draws, which are naturally constrained not to go negative. The reason we don’t automatically start with binomial draws is that they are computationally slower than Poisson draws (this occurs because the binomial distribution includes some combinatorial terms that are slow to compute). As computers have gotten faster, this is less of an issue.\n\nCode####################################################################\n# set up and storage for states                                    #\n####################################################################\nmax_time &lt;- 100\n# time to simulate over. With Tau Leaping the random draws scale with time\n# not with population size, so this is much more efficient than Gillespie\n# for large populations\nsims &lt;- 1000\n# number of simulations: notice that we can do WAY more now\n\ns_mat &lt;- matrix(S, 1, sims) # storage item for S for all simulations\ni_mat &lt;- matrix(I, 1, sims) # storage item for I for all simulations\nr_mat &lt;- matrix(R, 1, sims) # storage item for R for all simulations\nnew_cases &lt;- matrix(0, 1, sims)\n# storage item for new cases (i.e. incidence) for all simulations\nn_mat &lt;- S + I + R # storage item for N for all simulations\n\n\n\nCode####################################################################\n# run over a time from 2 to T                                      #\n####################################################################\n#\nfor (time_step in 2:max_time) {\n  # loop over time, time_step is the index\n\n  out &lt;- tau_sir_step(\n    sims,\n    s_mat[time_step - 1, ],\n    i_mat[time_step - 1, ],\n    r_mat[time_step - 1, ],\n    beta,\n    gamma,\n    delta_t = 1\n  )\n  # call to SIR step function above\n\n  # update state\n  s_mat &lt;- rbind(s_mat, out[, 1])\n  # update state\n  i_mat &lt;- rbind(i_mat, out[, 2])\n  # update state\n  r_mat &lt;- rbind(r_mat, out[, 3])\n  # update state -- note population size isn't changing, but this could be\n  # updated with births/deaths\n  n_mat &lt;- rbind(n_mat, out[, 1] + out[, 2] + out[, 3])\n  # update state\n  new_cases &lt;- rbind(new_cases, out[, 4])\n}\n\n\nHopefully you can easily see that simulating 1000 iterations of Tau Leaping is faster than Gillespie.\n\nCode####################################################################\n# plotting                                                         #\n####################################################################\n# put output in a data frame\ntau_df &lt;- tibble(\n  S = array(s_mat),\n  I = array(i_mat),\n  R = array(r_mat),\n  N = array(n_mat),\n  cases = array(new_cases),\n  time = rep(1:max_time, sims),\n  iteration = rep(1:sims, each = max_time)\n)\n\ntau_df &lt;- tau_df %&gt;%\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -c(time, iteration),\n    names_to = \"state\",\n    values_to = \"number\"\n  ) %&gt;%\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\", \"N\", \"cases\")))\n\n# fix this color\nsir_colors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\", cases = \"#2ca02c\")\n\ntau_df %&gt;%\n  mutate(iteration = as.factor(iteration)) %&gt;%\n  filter(state %in% c(\"S\", \"I\", \"R\")) %&gt;%\n  ggplot(aes(\n    x = time,\n    y = number,\n    group = interaction(iteration, state),\n    color = state\n  )) +\n  geom_line(linewidth = 1.5, alpha = .1) +\n  scale_color_manual(values = sir_colors) +\n  labs(\n    x = \"Time\",\n    y = \"Number\",\n    color = \"State\"\n  ) +\n  guides(color = guide_legend(override.aes = list(alpha = 1))) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nAnd plotting the simulated trajectories should look pretty close to what we got with Gillespie. But, because we can simulate many more iterations, we can start to observe some of the rarer behavior; e.g. even for simulations with \\(R_0 = 3.5\\) there are some simulation runs for which the epidemic doesn’t take off (S stays at 1000).\nNote that because we have stored the newly infected individuals as “new cases” then we can plot both the incidence (new cases) and prevalence (I) each day.\n\nCode####################################################################\n# plotting                                                         #\n####################################################################\n# put output in a data frame\ntau_df &lt;- tibble(\n  S = array(s_mat),\n  I = array(i_mat),\n  R = array(r_mat),\n  N = array(n_mat),\n  cases = array(new_cases),\n  time = rep(1:max_time, sims),\n  iteration = rep(1:sims, each = max_time)\n)\n\ntau_df &lt;- tau_df %&gt;%\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -c(time, iteration),\n    names_to = \"state\",\n    values_to = \"number\"\n  ) %&gt;%\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\", \"N\", \"cases\")))\n\n# fix this color\nsir_colors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\", cases = \"#2ca02c\")\n\ntau_df %&gt;%\n  mutate(iteration = as.factor(iteration)) %&gt;%\n  filter(state %in% c(\"I\", \"cases\")) %&gt;%\n  ggplot(aes(\n    x = time,\n    y = number,\n    group = interaction(iteration, state),\n    color = state\n  )) +\n  geom_line(linewidth = 1.5, alpha = .1) +\n  scale_color_manual(values = sir_colors) +\n  labs(\n    x = \"Time\",\n    y = \"Number\",\n    color = \"State\"\n  ) +\n  guides(color = guide_legend(override.aes = list(alpha = 1))) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that the time series of new incident cases is very different than the time series of prevalent cases. Recall from our earlier discussion that the former are more likely to be what we would see in clinical surveillance. The latter are what we might see if we did random testing in the population. Which one would you expect to correspond best to environmental wastewater surveillance?",
    "crumbs": [
      "Day 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>R Session 04</span>"
    ]
  },
  {
    "objectID": "r-session-04.html#the-chain-binomial-algorithm",
    "href": "r-session-04.html#the-chain-binomial-algorithm",
    "title": "\n10  R Session 04\n",
    "section": "\n10.5 The Chain Binomial Algorithm",
    "text": "10.5 The Chain Binomial Algorithm\nIn the last section, we saw that by moving from continuous time to discrete time steps, we limit the complexity (the number of stochastic evaluations scales with time and the number of states, instead of population size and the number of states). You may have also noticed that the individual steps in the Tau Leaping algorithm can be modeled as Poisson or Binomial random variables. Combining these two gives us another common algorithm for stochastic simulation, the Chain Binomial model. Here, we simplify further and use a time step that is equal to the infectious generation period; e.g. 2 weeks for measles, or 1 week for flu. If we assume that time progresses in discrete, non-overlapping generations, then those that get infected at the start of one time step, recover at the end of that time step. This simplification means that we only have to model the infection process as stochastic, and the recovery process at the end of the time step is now deterministic. This is obviously unrealistic, but makes the model much simpler for formal statistical model fitting using likelihood and Bayesian methods. So while it is imperfect, it remains as a common method for “first-pass” analyses.\nAs before, we start with initial conditions. Here we initialize with the same population as before, but notice that the transmission rate, \\(\\beta\\) is different. Time is rescaled here to units of epidemic generation time, so \\(\\beta\\) is scaled correspondingly and now, for this formulation, \\(\\beta\\) is \\(R_0\\).\n\n\n\n\n\n\nNote\n\n\n\nRecall that the basic reproduction number, \\(R_0\\) is a function of the pathogen and the population of interest. In models, both the population (e.g. mixing, heterogeneity, etc) and the pathogen (e.g. time-scale of recovery) are represented using approximations to the biology. Here, the choice of model forces a change in time-scale which means that we have to change \\(\\beta\\) accordingly; thus, \\(\\beta\\), which is ostensibly a biological parameter is also a property of the model.\n\n\n\nCode####################################################################\n# Parameters and initial conditions                                #\n####################################################################\nS &lt;- 998 # number susceptible\nI &lt;- 1 # number infected\nR &lt;- 1 # number recovered\ntime &lt;- 0\n\nbeta &lt;- 3.5 # transmission rate -- note the change in value\n\n\nWhen we implement the forward simulation, we now only have to generate random draws for the infection process, which again simplifies the amount of stochastic simulation we have to do, and speeds up run times. While this might not be limiting in the scale of this activity, this DOES become an issue when we have to fit stochastic models. Recall from the lecture on parameter estimation that the basic recipe has us build a model and test out all, or many, values of the parameters (here \\(\\beta\\)) to find which one is closest to the data. Here, because each run of the stochastic simulation is different, we need to run simulations over many possible parameters, and for each parameter, we need to run many stochastic runs to characterize the average, or most likely, behavior. So, the number of simulations can rapidly increase into the millions (bigger for models with more parameters), so every bit of savings in computational time can be valuable.\n\nCode####################################################################\n# Single time step                                                 #\n####################################################################\nchain_binomial_step &lt;- function(sims, S, I, R, beta, ...) {\n  # S = initial susceptible population\n  # I = initial infected population\n  # R = initial recovered population\n  # beta is transmission rate\n\n  # total population size\n  N &lt;- S + I + R\n  # this is the only stochastic step; only do draw if I &gt;0\n  new_i &lt;- rbinom(n = sims, pmax(S, 0), 1 - exp(-beta * pmax(I, 0) / N))\n  # this step is now deterministic, because everyone from the past time\n  # step recovers\n  new_s &lt;- S - new_i\n  new_r &lt;- R + I\n\n  cbind(new_s, new_i, new_r)\n}\n\n\nThen we can run the simulation over a set of discrete time steps, T. Note again that T is now the number of infectious generation times, since time is rescaled relative to the models above.\n\nCode####################################################################\n# Run over many steps                                              #\n####################################################################\n\nmax_time &lt;- 20\n# note here that the time step is one infectious generation time,\n# so 7 days from gamma above\nsims &lt;- 1000\n\ns_mat &lt;- matrix(S, 1, sims) # storage item for S for all simulations\ni_mat &lt;- matrix(I, 1, sims) # storage item for I for all simulations\nr_mat &lt;- matrix(R, 1, sims) # storage item for R for all simulations\nn_mat &lt;- S + I + R\n\nfor (time_step in 2:max_time) {\n  out &lt;- chain_binomial_step(\n    sims,\n    s_mat[time_step - 1, ],\n    i_mat[time_step - 1, ],\n    r_mat[time_step - 1, ],\n    beta\n  )\n  # update state\n  s_mat &lt;- rbind(s_mat, out[, 1])\n  # update state\n  i_mat &lt;- rbind(i_mat, out[, 2])\n  # update state\n  r_mat &lt;- rbind(r_mat, out[, 3])\n  # update state -- note population size isn't changing, but this could be\n  # updated with births/deaths\n  n_mat &lt;- rbind(n_mat, out[, 1] + out[, 2] + out[, 3])\n}\n\n\nWe can plot, but note again, that this is plotted in terms of epidemic generations on the X-axis, rather than days (as in the previous sections).\n\nCode# put output in a data frame\nbin_df &lt;- tibble(\n  S = array(s_mat),\n  I = array(i_mat),\n  R = array(r_mat),\n  N = array(matrix(n_mat)),\n  time = rep(1:max_time, sims),\n  iteration = rep(1:sims, each = max_time)\n)\n\nbin_df &lt;- bin_df %&gt;%\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -c(time, iteration),\n    names_to = \"state\",\n    values_to = \"number\"\n  ) %&gt;%\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\", \"N\")))\n\n# fix this color\nsir_colors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\")\n\nbin_df %&gt;%\n  mutate(iteration = as.factor(iteration)) %&gt;%\n  filter(state %in% c(\"S\", \"I\", \"R\")) %&gt;%\n  ggplot(aes(\n    x = time,\n    y = number,\n    group = interaction(iteration, state),\n    color = state\n  )) +\n  geom_line(linewidth = 1.5, alpha = .1) +\n  scale_color_manual(values = sir_colors) +\n  labs(\n    x = \"Time\",\n    y = \"Number\",\n    color = \"State\"\n  ) +\n  guides(color = guide_legend(override.aes = list(alpha = 1))) +\n  theme(legend.position = \"top\")",
    "crumbs": [
      "Day 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>R Session 04</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bjørnstad, Ottar N. 2018. “Advanced: The Next-Generation\nMatrix.” In Epidemics: Models and\nData Using R, 51. Use R!\nCham: Springer International Publishing. https://doi.org/10.1007/978-3-319-97487-3.\n\n\nDiekmann, O., and J. A. P. Heesterbeek. 2000. Mathematical\nEpidemiology of Infectious Diseases: Model Building,\nAnalysis and Interpretation. Wiley Series in Mathematical &\nComputational Biology. Wiley. https://books.google.ca/books?id=5VjSaAf35pMC.\n\n\nDrake, John M, and Pejman Rohani. 2019. “Estimation.” In.\nSeattle, Washington. http://daphnia.ecology.uga.edu/drakelab/wp-content/uploads/2019/07/estimation.pdf.\n\n\nGrais, R. F., M. J. Ferrari, C. Dubray, O. N. Bjørnstad, B. T. Grenfell,\nA. Djibo, F. Fermon, and P. J. Guerin. 2006. “Estimating\nTransmission Intensity for a Measles Epidemic in Niamey,\nNiger: Lessons for Intervention.” Transactions\nof The Royal Society of Tropical Medicine and Hygiene 100 (9):\n867–73. https://doi.org/10.1016/j.trstmh.2005.10.014.\n\n\nHeesterbeek, J. A. P. 2002. “A Brief History of\nR0 and a Recipe for Its\nCalculation.” Acta Biotheoretica 50 (3):\n189–204. https://doi.org/10.1023/A:1016599411804.\n\n\nHeffernan, J. M, R. J Smith, and L. M Wahl. 2005. “Perspectives on\nthe Basic Reproductive Ratio.” J R Soc Interface 2 (4):\n281–93. https://doi.org/10.1098/rsif.2005.0042.\n\n\nHurford, Amy, Daniel Cownden, and Troy Day. 2009. “Next-Generation\nTools for Evolutionary Invasion Analyses.” Journal of The\nRoyal Society Interface 7 (45): 561–71. https://doi.org/10.1098/rsif.2009.0448.\n\n\nKeeling, Matthew James, and Pejman Rohani. 2008. “Introduction to\nSimple Epidemic Models.” In Modeling Infectious Diseases in\nHumans and Animals, 21–22. Princeton: Princeton\nUniversity Press.\n\n\nKing, Aaron A, and Helen J Wearing. 2011. “Age Structured\nModels.” In. https://ms.mcmaster.ca/~bolker/eeid/2011_eco/waifw.pdf.\n\n\nMossong, Joël, Niel Hens, Mark Jit, Philippe Beutels, Kari Auranen,\nRafael Mikolajczyk, Marco Massari, et al. 2008. “Social\nContacts and Mixing Patterns Relevant to the\nSpread of Infectious Diseases.”\nPLOS Medicine 5 (3): e74. https://doi.org/10.1371/journal.pmed.0050074.",
    "crumbs": [
      "**References**"
    ]
  }
]